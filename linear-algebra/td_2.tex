
% ==> preamble
\input{../preamble.tex}
\renewcommand{\thesection}{\arabic{section}.}
\renewcommand{\theexercise}{\arabic{section}.\arabic{exercise}}
\def\by{\times}
\def\theenumi{\alph{enumi})}
%\renewcommand{\thehomework}{\arabic{section}.\arabic{exercise}}
% <==

\setcounter{chapter}{1}

\begin{document}

\chapter{Systems of linear equations}

% ==> intro
\section{Different faces of linear transformation}
% <==
% ==> Echelon form
\section{Solution of a linear system. Echelon forms}

% ==> ex1
\begin{exercise}
  Write the systems of equations below in matrix form.
\end{exercise}
% <==
% ==> ex2
\begin{exercise}
  Find all solutions of the vector equation
  \[x_1\vec{v_1}+x_2\vec{v_2}+x_3\vec{v_3}=\vec{0}\]
  where $\vec{v_1}=(1,1,0)\tran,~\vec{x_2}=(0,1,1)\tran$ and 
  $\vec{v_3}=(1,0,1)\tran.$ What conclusion can you make about linear
  independence (dependence) of the system of vectors 
  $\vec{v_1},\vec{v_2},\vec{v_3}$.
\end{exercise}
\begin{proof}
  The echelon form of the system is
  \[
    \begin{pmatrix}
      1 &0 &1\\
      1 &1 &0\\
      0 &1 &1
    \end{pmatrix}
    \sim
    \begin{pmatrix}
      1 &0 &1\\
      0 &1 &-1\\
      0 &0 &2
    \end{pmatrix}
  \]
  so, clearly the solution to this equation is
  $x_1=x_2=x_3=0$.

  \textbf{Conclusion.} If the vectors $\vec{v_1},\vec{v_2},\vec{v_3}$
  are linearly independence, then the above equation has unique 
  solution, namely $x_1=x_2=x_3=0$.

  If the vectors $\vec{v_1},\vec{v_2},\vec{v_3}$ are linearly 
  dependence, then there exists $\alpha, \beta,\gamma$ 
  (some of them are non-zero) such that
  \[\alpha\vec{v_1}+\beta\vec{v_2}+\gamma\vec{v_3}=\vec{0}\]
  Therefore, the solution to the above equation is
  \[
    \begin{cases}
      x_1=\alpha t\\
      x_2=\beta t\\
      x_3=\gamma t
    \end{cases}
  \]
  for some $t\in\R$.
\end{proof}
% <==

% <==
% ==> Analyzing pivots
\section{Analyzing pivots}


\setcounter{exercise}{5}
% ==> ex6
\begin{exercise}
  Prove or disprove. If the columns of a square ($n\by n$)
  matrix are linearly independence, so are the columns of $A^2$.
\end{exercise}
\begin{proof}
  Since $A$ is a square matrix, and columns vectors are linearly 
  independence, so
  \begin{align*}
    &\text{has pivot in every column}\\\implies\quad
    &\text{has pivot in evry row}\\\implies\quad
    &A\text{ is invertable}\\\implies\quad
    &A^2\text{ also invertable}\\\implies\quad
    &A^2\text{ has pivot every column and row}
  \end{align*}
  Therefore, the column vectors of $A^2$ also independence.
\end{proof}
% <==
% ==> ex7
\begin{exercise}
  Prove or disprove. If the columns of a square ($n\by n$)
  matrix are linearly independence, so are the columns of $A^3$.
\end{exercise}
% <==
% ==> ex8
\begin{exercise}
  Show that if the equation $A\vec{x}=\vec{0}$ has unique solution,
  then $A$ is left invertable.
\end{exercise}
\begin{proof}
  Because $A$ has unique solution, then it has pivot in every
  column. Therfore, \#col $\leq$ \#row, so we let $m\by n$ be the size
  of $A$ where $n\leq m$
  Let $R$ be the reduced 
  echelon form of $A$, hence there exists $E=E_k\cdots E_2E_1$
  such that $R=EA$.
  Observe that, $R$ would look like
  \[
    R=
    \begin{pmatrix}
      \fbox{1} &0  &\cdots &0\\
      0 &\fbox{1}  &\cdots &0\\
      \vdots&\vdots&\ddots & \\
      0 &0  &\cdots &\fbox{1}\\
      0 &0  &\cdots &0\\
      0 &0  &\cdots &0\\
    \end{pmatrix}
  \]
  Using matrix muliplication gives us $R\tran R=I_n$. We obtain that
  \[
    R\tran EA=R\tran T=I_n
  \]
  Therfore, 
  \fbox{$A$ is left invertable, and $R\tran E$ is its left inverse}.



\end{proof}
% <==
% <==
% ==> find A-inverse
\section{Find $A\inv$ by row reduction}
% <==
% ==> dimension
\section{Dimension}

% ==> ex1
\begin{exercise}
  True or false.
  \begin{enumerate}
    \item Every vector space that is generated by a finite set has a basis;
    \item Every vector space has a (finite) basis;
    \item A vector space cannot have more that one basis;
    \item A vector space has a finite basis, then the number of vectors
      in every basis is the same;
    \item The dimension of $\P_n$ is $n$;
    \item The dimension of $M_{m\by n}$ is $m+n$;
    \item If vectors $\vec{v_1},\vec{v_2},\dots,\vec{v_n}$ generate (span)
      the vector space $V$, then every vector in $V$ can be written as
      a linear combination of vectors $\vec{v_1},\vec{v_2},\dots,\vec{v_n}$
      in only one way;
    \item Every subspace of a finite-dimensional space is finite-dimensional.
    \item If $V$ is a vector space having dimension $n$, then $V$ has exactly
      one subspace of dimension $0$, and exactly one subspace of dimension
      $n$.
  \end{enumerate}
\end{exercise}
\begin{proof}
  \begin{enumerate}
    \item \textbf{True.} That finite set which generated a vector space
      is the spanning set itself. Since it's finite, it contains a
      basis.
    \item \textbf{False.} Take $\R[x]$ for example.
    \item \textbf{False.} In $\R^2$, one can choose 
      \[
        \left\{\binom{1}{0}, \binom{0}{1}\right\}
        \quad\text{or}\quad
        \left\{\binom{1}{1}, \binom{0}{1}\right\}
      \]
      as a basis.
    \item \textbf{True.} As proved in the above theorems,
      \begin{align*}
        &\text{\# independence vectors}\leq \dim V\\
        &\text{\# generating vectors}\geq \dim V
      \end{align*}
      hence the number of any basis in $V$ must be exactly $\dim V$ vectors.
    \item \textbf{False.} In $\P_n$, the standard basis is
      \[
        1,~t,~t^2,\dots,t^n
      \]
      which has $n+1$ vectors. Hence $\boxed{\dim\P_n=n+1}$.
    \item \textbf{False.} The standard basis in $M_{m\by n}$ is
      \[
        \{\vec{e}_{11},\vec{e}_{12},\dots,\vec{e}_{mn}\}
      \]
      has $m\times n$ vectors. Hence $\boxed{\dim M_{m\by n}=mn}$.
    \item \textbf{False.} span doesn't guarantee uniqueness.
    \item \textbf{True.} Let $W$ be a subspace of $V$. Because $\dim V$
      finite, we can find 
      $$\mathcal{A}=\{\vec{v_1},\vec{v_2},\dots,\vec{v_n}\}\subset V$$
      that spans $V$. WLOG, we assume that none of vectors in $\mathcal{A}$
      belongs to $W$ (the unluckiest case.) We can choose 
      $\vec{w_1}\in W$ such that $\vec{w_1}\neq\vec{0}$. Then
      \[
        \vec{w_1}=\alpha_1\vec{v_1}+\cdots+\alpha_n\vec{v_n}
      \]
      Because $\vec{w_1}\neq 0$, we're sure some of the $\alpha_i$'s are
      non-zero, say $\alpha_1$. Then the new system
      \[
        \mathcal{A}_1=\{\vec{w_1},\vec{v_2},\dots,\vec{v_n}\}
      \]
      still spans the space $V$. Now, if $\mathcal{B}_1:=\{\vec{w_1}\}$
      doesn't span $W$, we can repeat the above procedure and find 
      $\vec{w_2}$. We can do this at most $n$ times, because once
      we reach the $n$th step, we have the new system 
      $\mathcal{A}_n\subset W$ that spans the whole space $V$.

      Therfore, after some finite $k\le n$ step, we have
      \[
        \mathcal{B}_k=\{\vec{w_1},\vec{w_2},\dots,\vec{w_k}\}\subset W
      \]
      spans $W$. Hence, $W$ is finite dimensional.
    \item \textbf{Not sure.}
  \end{enumerate}
\end{proof}
% <==
% ==> ex2
\begin{exercise}
  Prove that if $V$ is a vector space having dimension $n$, then
  a system of vectors $\vec{v_1},\vec{v_2},\dots,\vec{v_n}$ in $V$
  is linearly independent iff it spans $V$.
\end{exercise}
% <==
% <==
% ==> change of basis
\section{Change of basis}
\setcounter{exercise}{2}

% ==> ex3
\begin{exercise}
  Find the change of coordinates matrix that changes the 
  coordinates in basis $\{1,1+t\}$ in $\P_1$ to the 
  coordinates in the basis $\{1-t, 2t\}$.
\end{exercise}
\begin{proof}
  Let's denote $\mathcal{A}=\{1,1+t\}$ and 
  $\mathcal{B}=\{1-t, 2t\}$. Let $\mathcal{S}$
  be the standard basis in $\P_1$. Therefore, 
  the matrix that transforms from vector in basis
  $\mathcal{A}$ to basis $\mathcal{B}$ is 
  $[\mathcal{BA}]= [\mathcal{BS}][\mathcal{SA}]$.
  We have
  \[ [\mathcal{SA}] = \begin{pmatrix} 1 &1 \\ 0 &1 \end{pmatrix} \]
  and 
  \[
    [\mathcal{BS}]=[\mathcal{SB}]\inv=
    \begin{pmatrix} 1  & 0\\ -1 & 2 \end{pmatrix}\inv=\frac{1}{2}
    \begin{pmatrix} 2 &0\\ 1 &1 \end{pmatrix}
  \]
  Therefore, 
  \[
    [\mathcal{BA}]= \frac{1}{2}
    \begin{pmatrix} 2 &0\\ 1 &1 \end{pmatrix}
    \begin{pmatrix} 1 &1 \\ 0 &1 \end{pmatrix}.
  \]
\end{proof}
% <==
% ==> ex4
\begin{exercise}
  Let $T$ be the ...
\end{exercise}
\begin{proof}
  In standard basis, $T$ looks like
  \[ [T]= \begin{pmatrix} 3 &1\\ 1 &-2 \end{pmatrix}.\]
  Let $\mathcal{B}=\{(1,1)\tran, (1,2)\tran\}$. In basis $\mathcal{B}$,
  the transformation would look like
  \[
    [T]_{\mathcal{BB}}= [\mathcal{BS}] [T] [\mathcal{SB}]
  \]
  And we have
  \[ [\mathcal{SB}]= \begin{pmatrix} 1 &1\\ 1 &2 \end{pmatrix} \]
  so
  \[
    [\mathcal{BS}]= [\mathcal{SB}]\inv=
    \begin{pmatrix} 1 &1\\ 1 &2 \end{pmatrix}\inv=
    \begin{pmatrix} 2 &-1\\ -1&1 \end{pmatrix}
  \]
  Therefore, the transformation $[T]_{\mathcal{BB}}$ in basis
  $\mathcal{B}$ is
  \[
    [T]_{\mathcal{BB}}=
    \begin{pmatrix} 2 &-1\\ -1&1 \end{pmatrix}
    \begin{pmatrix} 3 &1\\ 1 &-2 \end{pmatrix}
    \begin{pmatrix} 1 &1\\ 1 &2 \end{pmatrix} 
  \]
\end{proof}
% <==
% ==> ex5
\begin{exercise}
  Prove that if $A$ and $B$ are similar matrices, then
  $\tr A=\tr B$.
\end{exercise}
\begin{proof}
  Because $A$ and $B$ are similar, then there exists 
  an invertable matrix $Q$ such that $A=Q\inv BQ$. Observe that
  \[
    A=Q\inv\cdot BQ
    \quad\text{and}\quad
    B=BQ\cdot Q\inv
  \]
  This implies that $\tr A=\tr B$.
\end{proof}
% <==

% <==



\end{document}
