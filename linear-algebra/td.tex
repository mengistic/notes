
% ==> preamble
\input{../preamble.tex}

\renewcommand{\thesection}{\arabic{section}.}
\renewcommand{\theexercise}{\arabic{section}.\arabic{exercise}}
%\renewcommand{\thehomework}{\arabic{section}.\arabic{exercise}}
\def\by{\times}
% <==

%\setlrmarginsandblock{2cm}{5cm}{*}
%\setulmarginsandblock{2.5cm}{*}{1}
%\checkandfixthelayout
\def\theenumi{\alph{enumi})}


\begin{document}
% ==> 1. basic
\chapter{Basic Notions}
% ==> Vector Spaces
\section{Vector Spaces}
% ==> ex1
\begin{exercise}
  Let $\vec{x}=(1,2,3)\tran, ~\vec{y}=(y_1,y_2,y_3)\tran$ and 
  $\vec{z}=(4,2,1)\tran$. Compute 
  $2\vec{x},~3\vec{y},~\vec{x}+2\vec{y}-3\vec{z}$.
\end{exercise}
\begin{proof}
  Little calculation reveals that
  \[
    2\vec{x}=
    \begin{pmatrix}
      2\\4\\6
    \end{pmatrix},\quad
    3\vec{y}=
    \begin{pmatrix}
      3y_1\\3y_2\\3y_3
    \end{pmatrix},\quad
    \vec{x}+2\vec{y}-3\vec{z}=
    \begin{pmatrix}
      2y_1 -11\\
      2y_2-4\\
      2y_3
    \end{pmatrix}
  \]
\end{proof}
% <==
% ==> ex2
\begin{exercise}
  Which of the following sets (with natural addition and multiplication
  by a scalar) are vector spaces? Justify your answers.
  \begin{enumerate}
    \item The set of all continuous functions on the interval $[0,1]$;
    \item The set of all non-negative functions on the interval $[0,1]$;
    \item The set of all polynomials of degree \emph{exactly} $n$;
    \item The set of all symmetric $n\times n$ matrices, i.e. 
      the set of matrices $A=\{a_{j,k}\}_{j,k=1}^{n}$ such 
      that $A\tran=A$.
  \end{enumerate}
\end{exercise}
\begin{proof}
  \text{}
  \begin{enumerate}
    \item Let $\mathcal{C}[0,1]$ be the set of all continuous functions
      on $[0,1]$. For any $f,g\in\mathcal{C}[0,1]$ and $\alpha\in\R$, we define
      \[
        (f+g)(x):=f(x)+g(x)
        \quad\text{and}\quad
        (\alpha f)(x):=\alpha\cdot f(x)
      \]
      for each $x\in[0,1]$. Therefore, $(\mathcal{C}[0,1], +,\cdot)$
      is closed under addition and scalar multiplication.
      It's immediate to see that all the eight properties of vector space
      are all satisfied, that is
      \begin{multicols}{2}
        \begin{itemize}
          \item $f+g=g+f$
          \item $f+(g+h)=(f+g)+h$
          %\item the function $0\in\mathcal{C}[0,1]$ such that 
          \item $f+0=f$
          \item $f+(-f)=0$
          %
          \item $1f=f$
          \item $\alpha(\beta f)=(\alpha\beta)f$
          \item $(\alpha+\beta)f=\alpha f+\beta f$
          \item $\alpha(f+g)=\alpha f+\beta g$
        \end{itemize}
      \end{multicols}
      Note that the function $0\in\mathcal{C}[0,1]$ such that
      $0(x)=0$ for each $x\in[0,1]$.
    \item Let $\mathcal{B}$ is the set of all non-negative functions on $[0,1]$.
      Then $(\mathcal{B},+\cdot)$ is not a vector space because it's not closed 
      under scalar multiplication, i.e. if $f\in\mathcal{B}$, hence $f>0$ yet
      \[-f=(-1)\cdot f<0.\]
      (Even if we restrict the scalar be to positive real numbers, this set 
      still won't be a vector space, because it fails to have an inverse.)
    \item Let $\mathcal{P}$ be the set of all polynomials of degree exactly $n$,
      then $(\mathcal{P}, +, \cdot)$ is \emph{not} a vector space, 
      because the addtive indentity is the polynomial 
      $0$. However, $0\notin\mathcal{P}$.
    \item Let $\sym(n)$ be the set of all symmetric $n\times n$ matrices. 
      The addition and scalar multiplication are defined as an
      \emph{entrywise} operations. Hence, $\sym(n)$ is closed under
      $(+)$ and $(\cdot)$. The additive indentity is the matrix 
      \[ \vec{0}=
        \begin{pmatrix}
          0&0&\cdots&0\\
          0&0&\cdots&0\\
          \vdots&&\ddots&\\
          0&0&\cdots&0
        \end{pmatrix}\in\sym(n).
      \]
      We could easily see that all the eight properties of vector spaces 
      are all satisfied.
  \end{enumerate}
\end{proof}
% <==
% ==> ex3
\begin{exercise}
  True or false:
  \begin{enumerate}
    \item Every vector space contains a zero vector;
      (\textbf{True.})
    \item A vector space can have more than one zero vector;
      (\textbf{False.} The zero vector is unique.)
    \item An $m\times n$ matrix has $m$ rows and $n$ columns;
      (\textbf{True.})
    \item If $f$ and $g$ are polynomials of degree $n$, then $f+g$
      is also a polynomial of degree $n$.
      (\textbf{False.} consider $t^n$ and $t-t^n$.)
    \item If $f$ and $g$ are polynomials of degree atmost $n$, the $f+g$
      is also a polynomial of degree atmost $n$.
      (\textbf{True.})
  \end{enumerate}
\end{exercise}
% <==
% ==> ex4
\begin{exercise}
  Prove that a zero vector $\vec{0}$ of a vector space $V$ is
  unique.
\end{exercise}
\begin{proof}
  Suppose that $\vec{a}$ and $\vec{b}$ are the zero vectors of $V$.
  From the \emph{Axioms of Vector Space}, we obtain that
  \begin{align*}
    \vec{a}
      &=\vec{a}+\vec{b} && \text{($\vec{b}$ is the zero vector)}\\
      &=\vec{b}+\vec{a} && \text{(commutitativity)}\\
      &=\vec{b}         && \text{($\vec{a}$ is the zero vector)}
  \end{align*}
  Hence, a zero vector of a vector space is unique,
  and we usually denote it by $\vec{0}$.
\end{proof}
% <==
% ==> ex5
\begin{exercise}
  What is the zero matrix of the space $M_{2\times 3}$?
\end{exercise}
\begin{proof}[Answer]
  In the space $M_{2\times 3}$, the zero matrix is 
  \[
    \vec{0}=
    \begin{pmatrix}
      0&0&0\\
      0&0&0
    \end{pmatrix}.
  \]
\end{proof}
% <==
% ==> ex6
\begin{exercise}
  Prove that the additive inverse of a vector space is unique.
\end{exercise}
\begin{proof}
  Let $\vec{a}$ be an arbitrary vector. Assume the $\vec{a}$ has 
  two inverses, namely $\vec{x}$ and $\vec{y}$. Hence
  \begin{align*}
    \vec{x}
      &=\vec{x}+\vec{0}   && \\
      &=\vec{x}+(\vec{a}+\vec{y}) && \text{($\vec{y}$ is an inverse)}\\
      &=(\vec{x}+\vec{a})+\vec{y} && \text{(associativity)}\\
      &=\vec{0}+\vec{y}           && \text{($\vec{x}$ is an inverse)}\\
      &=\vec{y}.
  \end{align*}
  Therefore, the inverse of any vector $\vec{a}\in V$ is unique, and 
  is usually denoted by $-\vec{a}$.
\end{proof}
% <==
% ==> ex7
\begin{exercise}
  Prove that $0\vec{v}=\vec{0}$ for any vector $\vec{v}\in V$.
\end{exercise}
\begin{proof}
  Let $\vec{v}\in V$ and  $\vec{b}$ is an inverse of $0\vec{v}$. Therefore, 
  \begin{align*}
    0
      &=0\vec{v}+b        \\
      &=(0+0)\vec{v}+b    \\
      &=(0\vec{v}+0\vec{v})+b && \text{(distributivity)}\\
      &=0\vec{v}+(0\vec{v}+b) && \text{(associativity)}\\
      &=0\vec{v}+\vec{0}      && \text{($\vec{b}$ is an inverse of $0\vec{v}$)}\\
      &=0\vec{v}
  \end{align*}
  for any $\vec{v}\in V$.
\end{proof}
% <==
% ==> ex8
\begin{exercise}
  Prove that for any vector $\vec{v}$ its additive inverse $-\vec{v}$
  is given by $(-1)\vec{v}$.
\end{exercise}
\begin{proof}
  As proved in the above exercise for any $\vec{v}\in V$, 
  \[
    \vec{0}=0\vec{v}=(1-1)\vec{v}=\vec{v}+(-1)\vec{v}
  \]
  where the last equallity derives from the distributive property.
  Because $-\vec{v}$ is the inverse of $\vec{v}$, then
  \begin{align*}
    -\vec{v}
      &=-\vec{v}+\vec{0}\\
      &=-\vec{v}+\big[\vec{v}+(-1)\vec{v}\big]\\
      &=(\underbrace{-\vec{v}+\vec{v}}_{\vec{0}})+(-1)\vec{v}\\
      &=(-1)\vec{v}
  \end{align*}
  as desired.
\end{proof}
% <==
% <==
% ==> Linear Combination
\section{Linear Combination, bases}
% ==> ex1
\begin{exercise}
  Find the basis in the space of $3\by 2$ matrices
  $M_{3\by 2}$.
\end{exercise}
\begin{proof}[Answer]
  Consider the vectors:
  \begin{align*}
    \vec{e_1}=\begin{bmatrix} 1&0\\0&0\\0&0 \end{bmatrix},\quad
    \vec{e_2}=\begin{bmatrix} 0&1\\0&0\\0&0 \end{bmatrix},\quad
    \vec{e_3}=\begin{bmatrix} 0&0\\1&0\\0&0 \end{bmatrix},\\[0.3cm]
    \vec{e_4}=\begin{bmatrix} 0&0\\0&1\\0&0 \end{bmatrix},\quad
    \vec{e_5}=\begin{bmatrix} 0&0\\0&0\\1&0 \end{bmatrix},\quad
    \vec{e_6}=\begin{bmatrix} 0&0\\0&0\\0&1 \end{bmatrix}
  \end{align*}
  and we're going to prove that the system of thses vectors are a basis.
  Any  matrix 
  \[
    \vec{v}=
    \begin{bmatrix}
      a&b\\ c&d\\ e&f
    \end{bmatrix}\in M_{3\by 2}
  \]
  can be represented as the combination
  $\vec{v}=a\vec{e_1}+b\vec{e_2}+c\vec{e_3}+d\vec{e_4}+e\vec{e_5}+f\vec{e_6}$
  thus this system is generating. Next we're going to prove the uniqueness.

  Suppose that there are $\hat{a},\hat{b},\dots,\hat{f}$ with
  \begin{align*}
    \vec{v}&=\hat{a}\vec{e_1}+\hat{b}\vec{e_2}+\hat{c}\vec{e_3}
              +\hat{d}\vec{e_4}+\hat{e}\vec{e_5}+\hat{f}\vec{e_6}\\
    \implies\quad 
    \begin{bmatrix} 
      a&b\\c&d\\e&f 
    \end{bmatrix}
    &=
    \begin{bmatrix} 
      \hat{a} & \hat{b}\\ \hat{c}&\hat{d} \\ \hat{e}&\hat{f}
    \end{bmatrix}
  \end{align*}
  This implies that each corresponding entry is equals. Hence the representation
  is unique. Therefore this system is a basis.




\end{proof}
% <==
% ==> ex2
\begin{exercise}
  True or false:
  \begin{enumerate}
    \item Any set containing a zero vector is linearly dependent;
    \item A basis must contain $\vec{0}$;
    \item subsets of linearly dependent sets are linearly dependent;
    \item subsets of linearly independent sets are linearly independent;
    \item if $\alpha_1\vec{v}_1+\alpha_2\vec{v_2}+\cdots+\alpha_n\vec{v_n}=0$
      then all scalars $\alpha_k$ are zero.
  \end{enumerate}
\end{exercise}
\begin{proof}[Answer]
  \text{}
  \begin{enumerate}
    \item \textbf{True.} because $\vec{0}$ can be represented as a linear 
      combination of the other vectors (simply put all the scalars to $0$).
    \item \textbf{No.} if so, they must be linearly dependent, which is not a base.
    \item \textbf{No.} Take for example the system of linearly dependent
      $\{\vec{e_1},\vec{e_2},\vec{e_3}\}$ where 
      $\vec{e_1}=(1,0),~\vec{e_2}=(0,1)$ and $\vec{e_3}=(1,1)$.
      The subset $\{\vec{e_1},\vec{e_2}\}$ is a basis, which is 
      clearly not linearly dependent.
    \item \textbf{True.} Supppose that the system 
      $\{\vec{v_1},\dots,\vec{v_p}\}$
      is a subset of the linearly independent system 
      $\{\vec{v_1},\dots,\vec{v_p},\dots,\vec{v_n}\}$. Let $\alpha_k$ the 
      real numbers such that
      $\alpha_{1}\vec{v_1}+\cdots+\alpha_{p}\vec{v_p}=\vec{0}$
      hence
      \[
        \alpha_{1}\vec{v_1}+\cdots+\alpha_{p}\vec{v_p}+
        0\vec{v_{p+1}}+\cdots+0\vec{v_n}=\vec{0}.
      \]
      Because the system $\{\vec{v_1},\dots,\vec{v_p},\dots,\vec{v_n}\}$
      is linearly independent, therefore all the scalars $\alpha_k=0$. 
      Thus, the system $\{\vec{v_1},\dots,\vec{v_p}\}$ is also linearly
      independent.
    \item \textbf{No.} Take, $\vec{e_1}=(2,2)$ and $\vec{e_2}=(1,1)$ for instance.
      We have $\vec{e_1}-2\vec{e_2}=\vec{0}$ yet the scalars are non-zero.
  \end{enumerate}
\end{proof}
% <==
% ==> ex3
\begin{exercise}
  Recall, that a matrix is called \emph{symmetric} if 
  $A\tran=A$. Write down a basis in the space of \emph{symmetric}
  $2\by 2$ matrices (there are many possible answers). How many
  elements are there in the basis.
\end{exercise}
\begin{proof}[Answer]
  We are going to prove that the system $\{\vec{d_1},\vec{d_2},\vec{e_1}\}$ where
  \[
    \vec{d_1}= \begin{bmatrix} 1&0\\0&0 \end{bmatrix},\quad
    \vec{d_2}= \begin{bmatrix} 0&0\\0&1 \end{bmatrix},\quad
    \vec{e_1}= \begin{bmatrix} 0&1\\1&0 \end{bmatrix},\quad
  \]
  is a basis. Observe that any symmetric matrix 
  \[
    \vec{v}=
    \begin{bmatrix}
      d_1 & e_1\\
      e_1 & d_2
    \end{bmatrix}
  \]
  can be represented as $\vec{v}=d_1\vec{d_1}+d_2\vec{d_2}+e_1\vec{e_1}$,
  hence it's generating. Note that the equation 
  \begin{align*}
    &d_1\vec{d_1}+d_2\vec{d_2}+e_1\vec{e_1}=\vec{0}\\
    &\begin{bmatrix}d_1 & e_1\\  e_1 &d_2\end{bmatrix}=\begin{bmatrix}0&0\\0&0\end{bmatrix}
  \end{align*}
  holds only when all the scalars are all zero. Hence the system is linearly 
  independent. Thus, it's a basis.

\end{proof}
% <==
% ==> ex4
\begin{exercise}
  Write down a basis for the space of
  \begin{enumerate}
    \item $3\by 3$ symmetric matrices;
    \item $n\by n$ symmetric matrices;
    \item $n\by n$ antisymmetric matrices.
  \end{enumerate}
\end{exercise}
\begin{proof}[Answer]
  \text{}
  \begin{enumerate}
    \item we are going to prove that the system of vectors
      \begin{align*}
        \vec{d_1}= \begin{bmatrix} 1&0&0 \\ 0&0&0 \\ 0&0&0 \end{bmatrix},\quad
        \vec{d_2}= \begin{bmatrix} 0&0&0 \\ 0&1&0 \\ 0&0&0 \end{bmatrix},\quad
        \vec{d_3}= \begin{bmatrix} 0&0&0 \\ 0&0&0 \\ 0&0&1 \end{bmatrix},\\[0.4cm]
        \vec{e_1}= \begin{bmatrix} 0&1&0 \\ 1&0&0 \\ 0&0&0 \end{bmatrix},\quad
        \vec{e_2}= \begin{bmatrix} 0&0&1 \\ 0&0&0 \\ 1&0&0 \end{bmatrix},\quad
        \vec{e_3}= \begin{bmatrix} 0&0&0 \\ 0&0&1 \\ 0&1&0 \end{bmatrix}.
      \end{align*}
      is the basis. First of, any symmetric matrix 
      \begin{align*}
        \vec{v}
        &=\begin{bmatrix}
          d_1 & e_1 & e_2 \\
          e_1 & d_2 & e_3 \\
          e_2 & e_3 & d_3
        \end{bmatrix}
      \intertext{can be represented as}
        \vec{v}&=d_1\vec{d_1}+d_2\vec{d_2}+d_3\vec{d_3}
          +e_1\vec{e_1}+e_2\vec{e_2}+e_3\vec{e_3}
      \end{align*}
      yeilds that the system is generating. Similar to the previous problem, 
      if the linear combination of these vectors equals $\vec{0}$, then all 
      the scalars must equals zero. Thus it's linearly independent. 
      Therefore it's a basis.
    \item Working on it.
    \item Working on it.
  \end{enumerate}
\end{proof}
% <==
% ==> ex5
\begin{exercise}
  Let a system of vectors $\vec{v}_1,\vec{v}_2,\dots, \vec{v}_r$
  be linearly independent but not generating. Show that it is possible 
  to find a vector $\vec{v}_{r+1}$ such that the system 
  $\vec{v}_1,\vec{v}_2, \dots, \vec{v}_r,\vec{v}_{r+1}$ is linearly 
  independent.
\end{exercise}
\begin{proof}
  Because the system $\vec{v}_1,\vec{v}_2,\dots, \vec{v}_r$ is not generating, 
  therefore there exists a vector $\vec{v}_{r+1}$ such that $\vec{v}_{r+1}$ 
  cannot be represented as a linear combination of $\vec{v}_1,\vec{v}_2,\dots, \vec{v}_r$.
  Let $\alpha_i$ be the scalars such that 
  \begin{equation}
    \label{eq:2:r_plus_1}
    \alpha_1\vec{v}_1+\alpha_2\vec{v}_2+\cdots+\alpha_r\vec{v}_r+\alpha_{r+1}\vec{v}_{r+1}=\vec{0}
  \end{equation}
  Now we have to prove that all the scalars are all zero.
  If $\alpha_{r+1}\neq 0$ then 
  \[
    \vec{v}_{r+1}=-\sum_{i=1}^{r}\frac{\alpha_i}{\alpha_{r+1}}\cdot\vec{v}_{i},
  \]
  meaning $\vec{v}_{r+1}$ is the linear combination of the other vectors, 
  a contradiction. Hence $\alpha_{r+1}$ must equals to zero. So
  the $r+1$ term in the equation \eqref{eq:2:r_plus_1} vanishes. And 
  because the system $\vec{v}_1,\vec{v}_2,\dots, \vec{v}_r$ is linearly independent, 
  all the scalars $\alpha_i=0$ for all $i=0,1,\dots,r$. Thus, the system
  \[
    \vec{v}_1,\vec{v}_2,\dots, \vec{v}_r, \vec{v}_{r+1}
  \]
  is also \emph{linearly independent}.
\end{proof}
% <==
% ==> ex6
\begin{exercise}
  Is it possible that vectors $\vec{v_1}, \vec{v_2}, \vec{v_3}$
  are linearly dependent, but the vectors $\vec{w_1}=\vec{v_1}+\vec{v_2}$,
  $\vec{w_2}=\vec{v_2}+\vec{v_3}$ and $\vec{w_3}=\vec{v_3}+\vec{v_1}$
  are linearly \emph{independent}.
\end{exercise}
\begin{proof}
  It's not possible, and we're going to prove this assertion via contradiction.
  Assume that there are such vectors $\vec{v}_1,\vec{v}_2,\vec{v}_3$
  satisfying the above conditions. Then there are numbers $x,y,z\in\R$
  such that
  \[
    \abs{x}+\abs{y}+\abs{z}>0
    \quad\text{and}\quad
    x\vec{v}_1+y\vec{v}_2+z\vec{v}_3=\vec{0}.
  \]
  By letting 
  \[a=x+y-z,\quad b=y+z-x,\quad c=z+x-y\]
  we obtain that
  \begin{align*}
    a\vec{w}_1+b\vec{w}_2+c\vec{w}_3
    &=(x\vec{w}_1+y\vec{w}_1-z\vec{w}_1)
     +(y\vec{w}_2+z\vec{w}_2-x\vec{w}_2)\\
    &\qquad\qquad\qquad\qquad +(x\vec{w}_3+z\vec{w}_3-y\vec{w}_3)\\
    &=2x\vec{v}_1+2y\vec{v}_2+2z\vec{v}_3\\
    &=\vec{0}.
  \end{align*}
  Since $\{\vec{w}_1,\vec{w}_2,\vec{w}_3\}$
  are linearly independent, we must have $a=b=c=0$. Hence
  \[
    \begin{cases}
      x+y-z=0\\
      y+z-x=0\\
      z+x-y=0
    \end{cases}
  \]
  adding all the 3 eqations, $x+y+z=0$. Substituting back to the 
  system of eqations above we get
  \[x=y=z=0\]
  which contradicts to the fact that $\abs{x}+\abs{y}+\abs{z}>0$.
\end{proof}
% <==
% ==> ex7
\begin{exercise}
  Any finite independent system is a subset of some basis.
\end{exercise}
\begin{proof}
  Let $\{\vec{v}_1,\vec{v}_2,\dots,\vec{v}_n\}$ is linearly independent.
  If this system is generating, then it's a base and we're done. If not,
  from exercise 2.5, there exists $\vec{v}_{n+1}$ such that
  \[\{\vec{v}_1,\vec{v}_2,\dots,\vec{v}_n,\vec{v}_{n+1}\}\]
  is still linearly independent. Now if this new system is generating, then 
  we're done. If not, we keep continue this process a finite steps, 
  adding vectors $\vec{v}_{n+1},\vec{v}_{n+2},\dots,\vec{v}_{n+r}$, and 
  eventually the new system
  \[\{\vec{v}_1,\dots,\vec{v}_{n},\vec{v}_{n+1},\dots,\vec{v}_{n+r}\}\]
  is now a basis.
\end{proof}
% <==
% <==
% ==> linear transformation
\section{Linear Transformation}
% ==> hw1
\begin{homework}
  Prove that the transformation $T:\F^n\to\F^m$ if and only if 
  $T(\alpha\vec{x}+\beta\vec{y})=\alpha T(\vec{x})+\beta T(\vec{y})$
  for any scalars $\alpha,\beta$ and vectors 
  $\vec{x},\vec{y}\in\F$.
\end{homework}
\begin{proof}
  We need to prove this in two directions.
  \begin{itemize}
    \item[($\Rightarrow$)] Suppose $T$ is a linear transformation, 
      then 
      %$T(\vec{x}+\vec{y})=T(\vec{x})+T(\vec{y})$ and 
      %$T(\alpha\vec{x})=\alpha T(\vec{\alpha})$. Thus
      \[
        T(\alpha\vec{x}+\beta\vec{y})
        =T(\alpha\vec{x})+T(\beta\vec{y})
        =\alpha T(\vec{x})+\beta T(\vec{y})
      \]
      as needed.
    \item[$(\Leftarrow)$] For this direction, we first assume that
      $T$ has the property that 
      $T(\alpha\vec{x}+\beta\vec{y}) =\alpha T(\vec{x})+\beta T(\vec{y})$
      for all $\alpha,\beta,\vec{x},\vec{y}$. We need to show that
      $T$ has the property listed in the definition of the linear 
      transformation. Observe that
      \begin{itemize}
        \item take $\alpha=\beta=1$ then,
          $T(\vec{x}+\vec{y})=T(\vec{x})+T(\vec{y})$
        \item take $\beta=0$ then,
          $T(\alpha\vec{x})=\alpha T(\vec{x})$
      \end{itemize}
      Hence $T$ is a linear transformation, 
      and the proof is completed.
  \end{itemize}
\end{proof}
% <==
% ==> hw2
\begin{homework}
  Let $T:V\to W$ be a linear transformation. Prove that
  $T(\vec{0})=\vec{0}$ and 
  \[TV=\{T\vec{v}~:~\vec{v}\in V\}\]
  is a vector space.
\end{homework}
\begin{proof}
  Since $T$ is linear, and as proved before $0\cdot\vec{0}=\vec{0}$, 
  it's easy to see that 
  $$T(\vec{0})=T(0\cdot\vec{0})=0\cdot T(\vec{0})=\vec{0}.$$
  To prove that $TV$ is a vector space, we need to check that $TV$ satisfies
  all the eight conditions listed in the definition of vector space.

  We first need to prove that $TV$ is closed.
  Because $TV\subset W$, hence $TV$ is closed under scalar multiplication and
  vector addition. Let $\vec{x},\vec{y},\vec{z}\in V$. Observe that
  \begin{itemize}
    \item $T\vec{x}+T\vec{y}=T\vec{y}+T\vec{x}$
      \quad (commutitativity of $W$)
    \item $(T\vec{x}+T\vec{y})+T\vec{z}=T\vec{x}+(T\vec{y}+T\vec{z})$
      \quad (associativity of $W$)
    \item The vector $\vec{0}\in W$ is the indentity of $TV$ because 
      \[
        T\vec{x}+\vec{0}=T\vec{x}+T\vec{0}=T(\vec{x}+\vec{0})=T(\vec{x}),\quad 
        \forall \vec{x}\in V
      \]
    \item The vector $T(-\vec{x})$ is the additive inverse of $T\vec{x}$ because
      \[T\vec{x}+T(-\vec{x})=T(\vec{x}-\vec{x})=\vec{0}\]
    \item $1\cdot T\vec{v}=T\vec{v}$
      \quad (multiplicative iden. in $W$)
  \end{itemize}
  Let $\alpha, \beta$ be scalars.
  \begin{itemize}
    \item multiplicative associativity
      \begin{align*}
        (\alpha\beta)T\vec{x} 
        &= T((\alpha\beta)\vec{x})
        && (\text{linearity of $T$})\\
        &= T(\alpha (\beta\vec{x})) 
        && (\text{mult. asso. of $V$})\\
        &= \alpha T(\beta\vec{x})
        && (\text{linearity of $T$})\\
        &= \alpha\cdot\beta T\vec{x}
      \end{align*}
    \item scalar multiplication
      \begin{align*}
        \alpha(T\vec{x}+T\vec{y})
        &=\alpha T(\vec{x}+\vec{y}) && (\text{linearity of $T$})\\
        &=T(\alpha(\vec{x}+\vec{y})) \\
        &=T(\alpha\vec{x}+\alpha\vec{y}) && (\text{scalar mult. in $V$})\\
        &=T(\alpha\vec{x})+T(\alpha\vec{x}) && (\text{linearity of $T$})\\
        &=\alpha T\vec{x}+\alpha T\vec{y}
      \end{align*}
    \item scalar multiplication
      \begin{align*}
        (\alpha+\beta)T\vec{x}
        &=T((\alpha+\beta)\vec{x}) && (\text{linearity of $T$})\\
        &=T(\alpha\vec{x}+\beta\vec{x}) && (\text{scalar mult. of $V$})\\
        &=T(\alpha\vec{x})+T(\beta\vec{x}) \\
        &=\alpha T\vec{x}+\beta T\vec{x}
      \end{align*}
  \end{itemize}
  We see that $TV$ has all eight properties to be a vector space, and the proof
  is completed.


\end{proof}
% <==
% ==> hw3
\begin{homework}
  Let $V,W$ be vector spaces. Prove that $\mathcal{L}(V,W)$, the set of all 
  linear transformations $T:V\to W$, is also a vector space.
\end{homework}
\begin{proof}
  We first need to show that $\mathcal{L}(V,W)$ is closed.
  Let $T_1, T_2\in\mathcal{L}(V,W)$ and $a$ be a scalar.
  So we need to show the transformation $T_1+T_2$ and $a T_1$ 
  are both linear. 
  \begin{itemize}
    \item Let $\vec{x}, \vec{y}$ be arbitrary vectors in $V$ 
      and $\alpha,\beta$ be scalar. 
      Denote $T:=T_1+T_2$. Observe that 
      \begin{align*}
        T(\alpha\vec{x}+\beta\vec{y})
        =&(T_1+T_2)(\alpha\vec{x}+\beta\vec{y})\\
        &=T_1(\alpha\vec{x}+\beta\vec{y})+T_2(\alpha\vec{x}+\beta\vec{y}) && (\text{by def. of $T_1+T_2$})\\
        &=\alpha T_1\vec{x}+\beta T_1\vec{y}+\alpha T_2\vec{x}+\beta T_2\vec{y} && (\text{by lin. of $T_1$ and $T_2$})\\
        &=(\alpha T_1\vec{x}+\alpha T_2\vec{x}) + (\beta T_1\vec{y}+\beta T_2\vec{y})\\
        &=\alpha (T_1\vec{x}+T_2\vec{x}) + \beta (T_1\vec{y}+T_2\vec{y}) && (\text{by scalar mult. in $W$})\\
        &=\alpha (T_1+T_2)\vec{x}+\beta (T_1+T_2)\vec{y}\\
        &=\alpha T\vec{x}+\beta T\vec{y}
      \end{align*}
      This shows that $T_1+T_2$ is also a linear transformation, hence 
      $\mathcal{L}(V,W)$ is closed under addition.
    \item Similarly, we let $\vec{x},\vec{y}\in V$. For simplicity, we again 
      denote $T:=a T_1$. Hence for any scalars $\alpha, \beta$
      \begin{align*}
        T(\alpha\vec{x}+\beta\vec{y})
        &=(aT_1)(\alpha\vec{x}+\beta\vec{y})\\
        &=a\cdot T_1(\alpha\vec{x}+\beta\vec{y}) && (\text{by def. of $aT_1$})\\
        &=a\cdot (\alpha T_1\vec{x}+\beta T_1\vec{y}) && (\text{by lin. of $T_1$})\\
        &=\alpha aT_1\vec{x}+\beta aT_1\vec{y}\\
        &=\alpha (aT_1)\vec{x}+\beta (aT_1){\vec{y}}\\
        &=\alpha T\vec{x}+\beta T\vec{y}
      \end{align*}
      This suggests that $aT_1$ is also linear, hence $\mathcal{L}(V,W)$
      is closed under scalar multiplication.
      Ultimately, we've proved that $\mathcal{L}(V,W)$ is closed as needed.
  \end{itemize}
  We are now ready to prove that $\mathcal{L}(V,W)$ is a vector space.
  Let $T_1,T_2,T_3\in\mathcal{L}(V,W)$ we have
  \begin{itemize}
    \item $T_1+T_2=T_2+T_1$, because for any $\vec{x}\in V$
      \[ (T_1+T_2)\vec{x}=T_1\vec{x}+T_2\vec{x}=T_2\vec{x}+T_1\vec{x}=(T_2+T_1)\vec{x}. \]
    \item $T_1+(T_2+T_3)=(T_1+T_2)+T_3$, because for any $\vec{x}\in V$
      \begin{align*}
        (T_1+(T_2+T_3))\vec{x}
        &=T_1\vec{x}+(T_2+T_3)\vec{x}\\
        &=T_1\vec{x}+(T_2\vec{x}+T_3\vec{x})\\
        &=(T_1\vec{x}+T_2\vec{x})+T_3\vec{x} && (\text{by asso. of $W$})\\
        &=(T_1+T_2)\vec{x}+T_3\vec{x}\\
        &=((T_1+T_2)+T_3)\vec{x}
      \end{align*}
    \item Consider the transformation $0:V\to W$ such that
      $0(\vec{v})=\vec{0}$ for all $\vec{v}\in V$. We're going to prove that
      this $0$ is the indentity of $\mathcal{L}(V,W)$. But first, we need to 
      know if $0$ is  linear or not. For any $\vec{v_1},\vec{v_2}\in V$, we have
      \[
        0(\alpha\vec{v_1}+\beta\vec{v_2})=\vec{0}
        \quad\text{and}\quad
        \alpha 0\vec{v_1}+\beta 0\vec{v_2}=\alpha\vec{0}+\beta\vec{0}=\vec{0}.
      \]
      Hence $0(\alpha\vec{v_1}+\beta\vec{v_2})=\alpha 0\vec{v_1}+\beta 0\vec{v_2}$,
      thus the transformation $0$ is linear, i.e. $0\in\mathcal{L}(V,W)$.

      Observe that for any $\vec{x}\in V$
      \[(T_1+0)\vec{x}=T_1\vec{x}+0\vec{x}=T_1\vec{x}\]
      This implies that $T_1+0=T_1$ for any $T_1\in\mathcal{L}(V,W)$.
      We conclude that $0$ is the indentity of $\mathcal{L}(V,W)$.
    \item The transformation $-T_1:=(-1)T_1$ is the additive inverse of $T_1$
      because for any $\vec{x}\in V$
      \[T_1\vec{x}+(-T_1\vec{x})=T_1\vec{x}+T_1(-\vec{x})=T_1(\vec{x}-\vec{x})=\vec{0}=0(\vec{x}).\]
    \item $1\cdot T_1=T_1$ because  $(1\cdot T_1)\vec{x}=1\cdot T_1\vec{x}=T_1\vec{x}$
      for any $\vec{x}\in V$.
    \item $(\alpha\beta)T_1=\alpha (\beta T_1)$, because 
      \[ 
        [(\alpha \beta)T_1]\vec{x}=(\alpha\beta)T_1\vec{x}=T_1(\alpha\beta\vec{x})
        =\alpha T_1(\beta\vec{x})=\alpha (\beta T_1)\vec{x}
      \]
    \item $\alpha (T_1+T_2)=\alpha T_1+\alpha T_2$ because
      \[
        [\alpha(T_1+T_2)](\vec{x})=\alpha T_1\vec{x}+\alpha T_2\vec{x}=(\alpha T_1+\alpha T_2)(\vec{x})
      \]
  \end{itemize}
\end{proof}
% <==

% ==> ex1
\begin{exercise}
  Multiply
  \begin{enumerate}
    \item 
      $
      \begin{pmatrix} 1&2&3\\4&54&6 \end{pmatrix}
      \begin{pmatrix} 1\\3\\2 \end{pmatrix} =
      \begin{pmatrix} 1+6+6\\ 4+15+12 \end{pmatrix}=
      \begin{pmatrix} 13\\31 \end{pmatrix}
      $
    \item 
      $
      \begin{pmatrix} 1&2\\0&1\\2&0 \end{pmatrix}
      \begin{pmatrix} 1\\3 \end{pmatrix}=
      \begin{pmatrix} 1+6\\ 0+3\\ 2+0 \end{pmatrix}=
      \begin{pmatrix} 7\\3\\2 \end{pmatrix}
      $
    \item 
      $
      \begin{pmatrix} 1&2&0&0\\ 0&1&2&0\\ 0&0&1&2\\ 0&0&0&1 \end{pmatrix}
      \begin{pmatrix} 1\\2\\3\\4 \end{pmatrix}=
      \begin{pmatrix} 1+4+0+0\\ 0+2+6+0\\ 0+0+3+8\\ 0+0+0+4 \end{pmatrix}=
      \begin{pmatrix} 5\\8\\11\\4 \end{pmatrix}
      $
    \item 
      $ \begin{pmatrix} 1&2&0\\0&1&2\\ 0&0&1\\0&0&0 \end{pmatrix}
      \begin{pmatrix} 1\\2\\3\\4 \end{pmatrix} $\\
      can't be multiplied because the number of columns of the first matrix
      doesn't equal to the number of rows of the second matrix.
  \end{enumerate}
\end{exercise}
% <==
% ==> ex2
\begin{exercise}
  Let a linear transformation in $\R^2$ be the reflection in the line 
  $x_1=x_2$. Find its matrix.
\end{exercise}
\begin{proof}[Solution]
  Let $T:\R^2\to\R^2$ be this transformation. The basis of the domain is
  $\{\vec{e_1}, \vec{e_2}\}$ where 
  $\vec{e_1}=(1,0)\tran$ and $\vec{e_2}=(0,1)\tran$. Because $T$ reflect
  the line $x_1=x_2$ then 
  \[
    T\vec{e_1}= \begin{pmatrix} 0\\1 \end{pmatrix}
    \quad\text{and}\quad
    T\vec{e_2}= \begin{pmatrix} 1\\0 \end{pmatrix}.
  \]
  Therefore, the matrix of this transformation is
  $[T]= \begin{bmatrix} 0&1\\1&0 \end{bmatrix}$.

\end{proof}
% <==
% ==> ex3
\begin{exercise}
  For each linear transformation below, find its matrix
  \begin{enumerate}
    \item $T:\R^2\to\R^3$ defined by $T(x,y)\tran=(x+2y,2x-5y,7y)\tran$
    \item $T:\R^4\to\R^3$ defined by 
      $T(x_1,x_2,x_3,x_4)\tran=(x_1+x_2+x_3+x_4, x_2-x_4, x_1+3x_2+6x_4)\tran$
    \item $T:\P_n\to\P_n$ st $Tf(t)=f'(t)$ 
      (find the matrix with respect to the standard basis 
      $1,t,t^2,\dots, t^n$)
    \item $T:\P_n\to\P_n$ st $Tf(t)=2f(t)+3f'(t)-4f''(t)$.
  \end{enumerate}
\end{exercise}
\begin{proof}
  Find the matrix.
  \begin{enumerate}
    \item The standard basis in $\R^2$ is $\{\vec{e_1},\vec{e_2}\}$ where
      $\vec{e_1}=(1,0)\tran$ and $\vec{e_2}=(0,1)\tran$. We have
      \[
        T\vec{e_1}= \begin{pmatrix*}[r] 1\\2\\0 \end{pmatrix*}
        \quad\text{and}\quad
        T\vec{e_2}= \begin{pmatrix*}[r] 2\\-5\\7 \end{pmatrix*}
      \]
      %Observe also that
      %\[
        %\begin{pmatrix*}[r]
          %1&2\\
          %2&-5\\
          %0&7
        %\end{pmatrix*}
        %\begin{pmatrix}
          %x\\y
        %\end{pmatrix}=
        %\begin{pmatrix}
          %x+2y\\2x-5y\\7y
        %\end{pmatrix}
      %\]
      Hence
      $
        \begin{pmatrix*}[r]
          1&2\\
          2&-5\\
          0&7
        \end{pmatrix*}
      $ is its matrix.
    \item Let $\{\vec{e_1},\vec{e_2},\vec{e_3},\vec{e_4}\}$ be the standard 
      basis in $\R^4$. Hence
      \begin{align*}
        &T\vec{e_1}=T(1,0,0,0)\tran = \begin{pmatrix} 1\\0\\1 \end{pmatrix},
        &&T\vec{e_2}=T(0,1,0,0)\tran= \begin{pmatrix} 1\\1\\3 \end{pmatrix}\\
        &T\vec{e_3}=T(0,0,1,0)\tran= \begin{pmatrix} 1 \\0\\0 \end{pmatrix},
        &&T\vec{e_4}=T(0,0,0,1)\tran= \begin{pmatrix*}[r] 1\\-1\\6 \end{pmatrix*}
      \end{align*}
      Therefore, 
      $ \begin{pmatrix*}[r]
        1&1&1&1\\
        0&1&0&-1\\
        1&3&0&6
      \end{pmatrix*} $
      is its matrix.
    \item Let $E=\{t^n,t^{n-1},\dots,t,1\}$ be the standard basis
      and $f(t)=a_nt^n+a_{n-1}t^{n-1}+\cdots+a_1t+a_0\in\P_n$. We write
      \[
        f(t)=(a_n, a_{n-1},\dots,a_1,a_0)\tran
      \]
      is base $E$. Since
      \[
        T(t^n)=nt^{n-1},\quad~T(t^{n-1})=(n-1)t^{n-2}\quad,\dots,\quad T(t)=1,\quad T(1)=0
      \]
      Therefore its matrix is 
      \[
        \begin{pmatrix*}[l]
          0&0&0&\cdots&0&0\\
          n&0&0&\cdots&0&0\\
          0&n-1&0&\cdots&0&0\\
          0&0&n-2&\cdots&0&0\\
          \vdots & \vdots & \vdots & \ddots &\vdots & \vdots\\
          0&0&0&\cdots&1&0\\
          0&0&0&\cdots&0&0
        \end{pmatrix*}
      \]
      %\begin{align*}
        %Tf(t)=f'(t)&=na_nt^{n-1}+(n-1)a_{n-1}t^{n-2}+\cdots+a_1
      %\end{align*}
      %\begin{align*}
        %&T(t^n)=nt^{n-1}= \begin{pmatrix} 0\\n\\0\\\vdots\\0\\0 \end{pmatrix},
        %&&T(t^{n-1})=(n-1)t^{n-2}= \begin{pmatrix*} 0\\0\\n-1\\\vdots\\0\\0 \end{pmatrix*},\quad\\
        %&\vdots&&\vdots\\
        %&T(t)=1= \begin{pmatrix} 0\\0\\0\\\vdots\\1\\0 \end{pmatrix},
        %&&T(1)=0= \begin{pmatrix} 0\\0\\0\\\vdots\\0\\0 \end{pmatrix}
      %\end{align*}
      \newpage
    \item $Tf(t)=2f(t)+3f'(t)-4f''(t)$\\
      Again, the standard basis is $\{t^n,t^{n-1},\dots,t,1\}$. For
      each $i\in [0,n]$ we have
      \[T(t^i)=2t^i+3it^{i-1}-4i(i-1)t^{i-2}\]
      Hence the matrix is achieved by stacking 
      $[T(t^n),\dots,T(t^i),\dots,T(t),T(1)]$, therefore the matrix is
      \[ [T]=
        \begin{bmatrix*}
          2        &0      &\cdots &0   &0\\
          3n       &2      &\cdots &0   &0\\
          -4n(n-1) &3(n-1) &\cdots &0   &0\\
          \vdots \\
          0        &0      &\cdots &2   &0\\
          0        &0      &\cdots &3   &2
        \end{bmatrix*}
      \]
  \end{enumerate}
\end{proof}
% <==
% ==> ex4
\begin{exercise}
  Find $3\by3$ matrices representing the transformations of $\R^3$ which
  \begin{enumerate}
    \item project every vector onto $x$-$y$ plane;
    \item reflect every vector through $x$-$y$ plane;
    \item rotate the $x$-$y$ plane through $30^\circ$, leaving the
      $z$-axis alone.
  \end{enumerate}
\end{exercise}
\begin{proof}
  In space $\R^3$, we shall use its standard basis 
  $\{\vec{e_1},\vec{e_2},\vec{e_3}\}$ where 
  $\vec{e_1}=(1,0,0)\tran$,
  $\vec{e_2}=(0,1,0)\tran$ and 
  $\vec{e_3}=(0,0,1)\tran$.
  \begin{enumerate}
    \item Let $T$ be this transformation. This means 
      $T(x,y,z)\tran=(x,y,0)\tran$. We get
      \[
        T\vec{e_1}= \begin{pmatrix} 1\\0\\0 \end{pmatrix},\quad
        T\vec{e_2}= \begin{pmatrix} 0\\1\\0 \end{pmatrix},\quad
        T\vec{e_3}= \begin{pmatrix} 0\\0\\0 \end{pmatrix}.
      \]
      Therefore is matrix is 
      $ \begin{pmatrix} 1&0&0\\0&1&0\\0&0&0 \end{pmatrix}. $
    \item Let $R$ be this transformation. Since $R$ project every vector
      through $x$-$y$ plane, hence $R(x,y,z)\tran=(x,y,-z)\tran$. We get
      \[
        R\vec{e_1}= \begin{pmatrix} 1\\0\\0 \end{pmatrix},\quad
        R\vec{e_2}= \begin{pmatrix} 0\\1\\0 \end{pmatrix},\quad
        R\vec{e_3}= \begin{pmatrix*}[r] 0\\0\\-1 \end{pmatrix*}.
      \]
      Thus the matrix of $R$ is 
      $
      \begin{pmatrix*}[r]
        1&0&0\\
        0&1&0\\
        0&0&-1
      \end{pmatrix*}
      $
    \item Let $S$ be this transformation. $S$ moves the vectors
      $\vec{e_1},\vec{e_2}$ to the point $x',y'$ respectively.
      %Then we have 
      %$T(x,y,z)\tran = T(x',y',z)$ where $x'=\cos 30^\circ=\frac{\sqrt{3}}{2}$
      %and $y=\sin 30^\circ=\frac{1}{2}$.
      \begin{center}
        \begin{tikzpicture}[scale=2.5, rotate=-10]
          \coordinate (O) at (0,0);
          \coordinate (x) at (0,-1);
          \coordinate (y) at (1,0);
          \coordinate (xp) at (-60:1);
          \coordinate (yp) at (30:1);


          \draw[-open triangle 60, thick] (0,0)--(0,-1) node[below]{$\vec{e_1}$};
          \draw[-open triangle 60, thick] (0,0)--(1,0) node[right]{$\vec{e_2}$};

          \draw[-open triangle 60, gray] (0,0)--(-60:1);
          \draw[-open triangle 60, gray] (0,0)--(30:1);

          \draw[dashed] ($(0,-0.86)$)--(xp) (0.5,0)--(xp);
          %\draw

          \node at ($(-60:1)+(0.1,-0.05)$) {$S\vec{e_1}$};
          \node at ($(30:1)+(0.1,0.05)$) {$S\vec{e_2}$};

          \draw pic[draw, angle eccentricity=2] {angle=x--O--xp};
          \draw pic[draw, angle eccentricity=2,"$30^\circ$"] {angle=y--O--yp};
        \end{tikzpicture}
      \end{center}
      Since $\cos30^\circ=\frac{\sqrt{3}}{2}$ and $\sin30^\circ=\frac{1}{2}$,
      we conclude that
      \[
        S\vec{e_1}= \begin{pmatrix} \frac{\sqrt{3}}{2}\\[0.2cm]\frac{1}{2}\\[0.2cm]0 \end{pmatrix},\quad
        S\vec{e_2}= \begin{pmatrix} -\frac{1}{2}\\[0.2cm]\frac{\sqrt{3}}{2}\\[0.2cm]0 \end{pmatrix},\quad
        S\vec{e_3}= \begin{pmatrix*}[r] 0\\0\\1 \end{pmatrix*}.
      \]
      Therefore the matrix is
      $ \begin{pmatrix}
        \frac{\sqrt{3}}{2} & -\frac{1}{2} & 0\\[0.2cm]
        \frac{1}{2} & \frac{\sqrt{3}}{2} & 0\\[0.2cm]
        0 & 0 &1
      \end{pmatrix}. $
  \end{enumerate}
\end{proof}
% <==
% ==> ex5
\begin{exercise}
  Let $A$ be a linear transformation. If $\vec{z}$ is the center of the
  staight interval $[\vec{x},\vec{y}]$, show that $A\vec{z}$ is the
  center of the interval $[A\vec{x}, A\vec{y}]$.
\end{exercise}
\begin{proof}
  $\vec{z}$ is the center of $[\vec{x}, \vec{y}]$ iff 
  $\vec{z}=\frac{1}{2}\vec{x}+\frac{1}{2}\vec{y}$. Therefore,
  \[
    A\vec{z}=A\left(\frac{1}{2}\vec{x}+\frac{1}{2}\vec{y}\right)
    =\frac{1}{2}A\vec{x}+\frac{1}{2}A\vec{y}
  \]
  Thus, $A\vec{z}$ is the center of the interval $[A\vec{x}, A\vec{y}]$.
\end{proof}
% <==
% ==> ex6
\begin{exercise}
  The set $\C$ of complex numbers can be canonically identified 
  with the space $\R^2$ by treating each $z=x+iy\in\C$ as a column
  $(x,y)\tran\in\R^2$.
  \begin{enumerate}
    \item Treating $\C$ as a complex vector space, show that the multiplication
      by $\alpha=a+ib\in\C$ is a linear transformation in $\C$.
      What is its matrix.
    \item Treating $\C$ as the real vector space $\R^2$ show that the multiplication 
      by $\alpha=a+ib\in\C$ is a linear transformation there.
    \item Define $T(x+iy)=2x-y+i(x-3y)$. Show that this tran is not a 
      linear transformation in the complex vector space $\C$, but
      if we treat $\C$ as the real vector space $\R^2$ then it is a linear
      transformation there, then find its matrix.
  \end{enumerate}
\end{exercise}
\begin{proof}
  \text{}
  \begin{enumerate}
    \item Let $T$ be this transformation. For any $\vec{x}\in\C$, we have
      $T\vec{x}=\alpha \vec{x}\in\C$.
      Thus $T:\C\to\C$, and we'll prove that $T$ is a linear 
      transformation. Let $\vec{x},\vec{y}\in\C$ be two vectors, 
      and $z\in\C$ be a scalar (complex). Observe that
      \begin{itemize}
        \item $T(\vec{x}+\vec{y})=\alpha(\vec{x}+\vec{y})
          =\alpha\vec{x}+\alpha\vec{y}=T\vec{x}+T\vec{y}$
          \quad (distributivity of complex numbers)
        \item $T(z\vec{x})=\alpha(z\vec{x})=z(\alpha\vec{x})=zT\vec{x}$
      \end{itemize}
      This shows that this transformation $T$ is a linear one.
      To find its matrix, we only need to know the basis of $\C$.
      Since any vector $\vec{x}\in\C$ we be written as 
      \[\vec{x}=1\cdot \underbrace{\vec{x}}_{\text{scalar}}\]
      and because this representation is unique, we obtain that 
      $\{1\}\subset\C$ is a basis of $\C$. Thus the matrix is
      \[
        [T]=[T(1)]=[\alpha\cdot 1]=[\alpha].
      \]
    \item Because we treat $\C$ as $\R^2$, then any complex number 
      $\vec{x}=x+iy$ can be represented as 
      $\begin{pmatrix} x\\y \end{pmatrix}\in\R^2$.
      Let $T$ be this transformation. Thus $T$ would look like
      \begin{align*}
        T\begin{pmatrix} x\\y \end{pmatrix}
        &=T(\vec{x})=\alpha\vec{x}\\
        &=(a+ib)(x+iy)\\
        &=(ax-by)+i(ay+bx)\\
        &=\begin{pmatrix} ax-by\\ay+bx \end{pmatrix}\in\R^2
      \end{align*}
      Thus $T:\R^2\to\R^2$.
      We need to show that $T$ is in fact linear. Let 
      $\vec{x_1}= \begin{pmatrix} x_1\\y_1 \end{pmatrix} $ and 
      $\vec{x_2}= \begin{pmatrix} x_2\\y_2 \end{pmatrix} $ and 
      be two arbitrary vectors. We have
      \[
        T\vec{x_1}+T\vec{x_2}=
        \begin{pmatrix} ax_1-by_1\\ay_1+bx_1 \end{pmatrix}+
        \begin{pmatrix} ax_2-by_2\\ay_2+bx_2 \end{pmatrix}=
        \begin{pmatrix} a(x_1+x_2)-b(y_1+y_2)\\a(y_1+y_2)+b(x_1+x_2) \end{pmatrix}=
        T(\vec{x_1}+\vec{x_2}),
      \]
      and for any scalar $r\in\R$,
      \[
        rT\vec{x}=r\begin{pmatrix} ax-by\\ay+bx \end{pmatrix}=
        \begin{pmatrix} rax-rby\\ray+rbx \end{pmatrix}=
        T(r\vec{x}).
      \]
      This shows that $T$ is a linear transformation.
      To find the matrix, we first need to find a bisis in $\R^2$.
      Luckily, as we've proved earlier we could choose $\{\vec{e_1},\vec{e_2}\}$
      to be a basis where
      \[
        \vec{e_1} \begin{pmatrix} 1\\0 \end{pmatrix}
        \quad\text{and}\quad
        \vec{e_2} \begin{pmatrix} 0\\1 \end{pmatrix}
      \]
      Therefore
      \[
        T\vec{e_1}= \begin{pmatrix} a\\b \end{pmatrix}
        \quad\text{and}\quad
        T\vec{e_2}= \begin{pmatrix} -b\\a \end{pmatrix}
      \]
      Thus the matrix of this transformation is 
      $ \begin{pmatrix} a&-b\\b&a \end{pmatrix} $.
    \item Define $T(x+iy)=2x-y+i(x-3y)$
      \begin{itemize}
        \item We'll prove that $T$ is not linear in complex vector space.
          Observe that
          \[
            T(i)=T(0+i)=-1-3i
            \quad\text{and}\quad
            T(1)=T(1+0i)=2+i
          \]
          cleary $T(i)\neq iT(1)$, this implies that $T$ is 
          not a linear transformation in $\C$.
        \item 
          In $\R^2$ the transformation would look like
          \[
            T\begin{pmatrix}x\\y\end{pmatrix}=
            \begin{pmatrix}2x-y\\x-3y\end{pmatrix}.
          \]
          For any vectors
          $\vec{x_1}=\begin{pmatrix}x_1\\y_1\end{pmatrix}$ and
          $\vec{x_2}=\begin{pmatrix}x_2\\y_2\end{pmatrix}$, we have
          \[
            T\vec{x_1}+T\vec{x_2}=
            \begin{pmatrix}2x_1-y_1\\x_1-3y_1\end{pmatrix}+
            \begin{pmatrix}2x_2-y_2\\x_2-3y_2\end{pmatrix}=
            \begin{pmatrix}2(x_1+x_2)-(y_1+y_2)\\(x_1+x_2)-3(y_1+y_2)\end{pmatrix}=
            T(\vec{x_1}+\vec{x_2})
          \]
          and for any scalar $r\in\R$,
          \[
            rT\vec{x}=r\begin{pmatrix}2x-y\\x-3y\end{pmatrix}=
            \begin{pmatrix}2rx-ry\\rx-3ry\end{pmatrix}=
            T(r\vec{x})
          \]
          this shows that $T:\R^2\to\R^2$ is linear.
          Because $\begin{pmatrix} 1\\0 \end{pmatrix}$ and 
          $\begin{pmatrix} 0\\1 \end{pmatrix}$ is a basis in $\R^2$ and
          \[
            T\begin{pmatrix} 1\\0 \end{pmatrix}=\begin{pmatrix} 2\\1 \end{pmatrix}
            \quad\text{and}\quad
            T\begin{pmatrix} 0\\1 \end{pmatrix}=\begin{pmatrix} -1\\-3 \end{pmatrix},
          \]
          thus the matrix of this transformation is 
          $ \begin{pmatrix*}[r] 2 & -1\\1&-3 \end{pmatrix*} $.
      \end{itemize}
  \end{enumerate}
\end{proof}
% <==
% ==> ex7
\begin{exercise}
  Show that any linear transformation in $\C$ 
  (treated as a complex vector space) is a multiplication by 
  $\alpha\in\C$.
\end{exercise}
\begin{proof}
  Let $T:\C\to\C$ be this transformation. For any $\vec{x}\in\C$
  \[
    T\vec{x}=T(\vec{x}\cdot \vec{1})=\vec{x}\cdot \underbrace{T(\vec{1})}_{\text{scalar}}
  \]
  and the proof is completed.
\end{proof}
% <==
% <==
% ==> linear transformation as a vector
\section{Linear transformation as a Vector}
Let set $\mathcal{L}(V,W)$ is a vector space with addition and scalar 
multiplication (as proved above).
% <==
% ==> composition
\section{Composition}

% ==> hw1
\begin{homework}\label{_trace}
  Let $A$ and $B$ be matrices of size $m\by n$ and $n\by m$ 
  respectively. Then \[\tr(AB)=\tr(BA).\]
\end{homework}
\begin{proof}
  % ==> intro
  We'd like to prove this theorem \emph{less} computationally.
  Let $X\in M_{n\by m}$. Consider the mapping
  $T,~T_1: M_{n\by m}\to\F$ defined by
  \[
    T(X)=\tr(AX)
    \quad\text{and}\quad
    T_1(X)=\tr(XA).
  \]
  To prove the theorem it is sufficient to show that 
  $T,T_1$ are linear and they are the same.
  so by substituting $X=B$ gives the theorem.
  % <==
  % ==> claim: linearity
  \begin{claim}
    The transformations $T,T_1$ defined above are linear.
  \end{claim}
  \begin{proof}
    For $X,Y\in M_{n\by m}$,
    \begin{itemize}
      \item From the properties of matrix, $A(X+Y)=AX+AY$. 
        Because $AX$ and $BX$ are both square matrices with 
        size $m\by m$, and since we add the matrices $AX+AY$
        entrywise, it follows that
        \begin{align*}
          T(X+Y)&=\tr(A(X+Y))=\tr(AX+AY)\\
                &=\tr(AX)+\tr(AY)\\
                &=T(X)+T(Y)
        \end{align*}
      \item Similarly for any scalar $\alpha\in\F$,
        \[
          T(\alpha X)=\tr(A\cdot\alpha X)
          =\tr(\alpha AX)=\alpha\tr(AX)=\alpha T(X)
        \]
    \end{itemize}
    This implies that $T$ is a linear transformation. With simply
    proof, we conclude that $T_1$ is also a linear transformation.
  \end{proof}
  % <==
  % ==> claim: T=T_1
  We choose $\vec{e_{11}},\vec{e_{21}},\dots,\vec{e_{nm}}$ to be 
  the standard basis of $M_{n\by m}$, meaning the vector
  \[
    \vec{e_{ij}}=
    \begin{pmatrix}
      0 & 0 & \cdots & 0 & \cdots & 0\\
      \vdots  & & \cdots & & \cdots & \vdots\\
      0 & 0 & \cdots & 1 & \cdots & 0\\
      0 & 0 & \cdots & 0 & \cdots & 0
    \end{pmatrix}
  \]
  is a matrix whose entries are zero, except at
  the entry at row $i$ and column $j$, which is $1$.
  Then we only need to show that $T\vec{e_{ij}}=T_1\vec{e_{ij}}$
  for all $i,j$. Let 
  \[
    A=
    \begin{pmatrix}
      a_{11} & a_{12} & \cdots & a_{1j} & \cdots  &a_{1m}\\
      \vdots &        & \vdots &        & \vdots  &\vdots\\
      a_{i1} & a_{i2} & \cdots & a_{ij} & \cdots  &a_{im}\\
      a_{nn} & a_{n2} & \cdots & a_{nj} & \cdots  &a_{nm}
    \end{pmatrix}
  \]
  Hence 
  \begin{align*}
    A\vec{e_{ij}}&=
    \begin{pmatrix}
      \cdot & \cdot & \cdot  & \cdot \\
      \cdot & \cdot & a_{ij} & \cdot \\
      \cdot & \cdot & \cdot  & \cdot
    \end{pmatrix}
    \begin{pmatrix}
      0 & 0 & 0 \\
      0 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 0 & 0
    \end{pmatrix}=
    \begin{pmatrix}
      0     &\cdot   &\cdot \\
      \cdot &a_{ij}  &\cdot \\
      \cdot &\cdot   &0
    \end{pmatrix}
  \intertext{and}
    \vec{e_{ij}}A&=
    \begin{pmatrix}
      0 & 0 & 0 \\
      0 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 0 & 0
    \end{pmatrix}
    \begin{pmatrix}
      \cdot & \cdot & \cdot  & \cdot \\
      \cdot & \cdot & a_{ij} & \cdot \\
      \cdot & \cdot & \cdot  & \cdot
    \end{pmatrix}=
    \begin{pmatrix}
      0      &  \cdot  &  \cdot  & \cdot \\
      \cdot  &  0      &  \cdot  & \cdot \\
      \cdot  &  \cdot  &  a_{ij} & \cdot \\
      \cdot  &  \cdot  &  \cdot  & 0
    \end{pmatrix}
  \end{align*}
  This implies that $T\vec{e_{ij}}=T_1\vec{e_{ij}}$ for all $i,j$,
  and hence $T=T_1$.
  % <==
\end{proof}
% <==

% ==> ex1
\begin{exercise}
  Working on it.
\end{exercise}
% <==
% ==> ex2
\begin{exercise}
  Let $T_{\gamma}$ be the rotation matrix by $\gamma$ in $\R^2$.
  Check by matrix multiplication that 
  $T_{\gamma}T_{-\gamma}=T_{-\gamma}T_{\gamma}=I$.
\end{exercise}
\begin{proof}
  Working on it.
\end{proof}
% <==
% ==> ex3
\begin{exercise}
  Multiply two rotation matrices $T_{\alpha}$ and $T_{\beta}$. Deduce
  formulas for $\sin(\alpha+\beta)$ and $\cos(\alpha+\beta)$
  from here.
\end{exercise}
\begin{proof}
  Working on it.
\end{proof}
% <==
% ==> ex4
\begin{exercise}
  Find the matrix of the orthogonal projection in $\R^2$ on to the line
  $x_1=-2x_2$.
\end{exercise}
\begin{proof}
  Let $T$ be this transformation. Let $R_{\gamma}$ and $P_x$ be the
  transformations of rotation by $\gamma$ and projection to $x$-axis,
  respectively. Therefore $T=R_{\gamma}P_xR_{-\gamma}$. Note that
  \[
    R_{\gamma}=
    \begin{pmatrix*}[r]
      \cos\gamma &-\sin\gamma\\
      \sin\gamma &\cos\gamma
    \end{pmatrix*},
    \quad
    R_{-\gamma}=
    \begin{pmatrix*}[r]
      \cos\gamma &\sin\gamma\\
      -\sin\gamma &\cos\gamma
    \end{pmatrix*},
    \quad\text{and}\quad
    P_x=
    \begin{pmatrix}
      1  &0\\
      0  &0
    \end{pmatrix}
  \]
  Thus, 
  \[
    \cos\gamma=\frac{\overline{OB}}{\overline{OA}}=\frac{2}{\sqrt{5}}
    \quad\text{and}\quad
    \sin\gamma=\frac{\overline{AB}}{\overline{OA}}=\frac{-1}{\sqrt{5}}.
  \]
  We get
  \marginpar{
  \begin{center}
    \begin{tikzpicture}[scale=0.9]
      \coordinate [label=-135:$O$] (O) at (0,0);
      \coordinate [label=-90:$A$]  (X) at (2,-1);
      \coordinate [label=90:$B$]   (Y) at (2,0) ;
      \draw[-open triangle 60] (-1,0) -- (3.5,0);
      \draw[-open triangle 60] (0,-2) -- (0,2);

      % draw the tick
      \foreach \x in {1,2,3}{
        \draw (\x,0.1)--(\x,-0.1);
      }
      \draw (-0.1,-1)--(0.1,-1);

      % draw the dashes
      \draw[dashed] (X)--(0,-1) (X)--(Y);

      \draw (-1,0.5)--(3.5,-1.75);
      %\draw (-1,0.5)--(3.5,-1.75)node[right]{$y=-\frac{1}{2}x$};
      \draw pic[draw, angle eccentricity=2, "$\gamma$"]
        {angle=X--O--Y};
    \end{tikzpicture}
  \end{center}
  }
  \begin{align*}
    T&=
    \begin{pmatrix*}[r]
      \frac{2}{\sqrt{5}}  &\frac{1}{\sqrt{5}}\\[0.3cm]
      -\frac{1}{\sqrt{5}} &\frac{2}{\sqrt{5}}
    \end{pmatrix*}
    \begin{pmatrix} 1&0\\0&0 \end{pmatrix}
    \begin{pmatrix*}[r]
      \frac{2}{\sqrt{5}}  &-\frac{1}{\sqrt{5}}\\[0.3cm]
      \frac{1}{\sqrt{5}}  &\frac{2}{\sqrt{5}}
    \end{pmatrix*}\\[0.3cm]
    &=
    \frac{1}{5}
    \begin{pmatrix*}[r] 2 &1\\-1&2 \end{pmatrix*}
    \begin{pmatrix} 1&0\\0&0 \end{pmatrix}
    \begin{pmatrix*}[r] 2 &-1\\1&2 \end{pmatrix*}\\[0.3cm]
    &=
    \frac{1}{5}
    \begin{pmatrix*}[r]
      4&-2\\-2&-2
    \end{pmatrix*}
  \end{align*}
\end{proof}
% <==
% ==> ex5
\begin{exercise}
  Find linear transformations $A,B:\R^2\to\R^2$ such that 
  $AB=\vec{0}$ but $BA\neq \vec{0}$.
\end{exercise}
\begin{proof}[Solution]
  Consider the following
  \[
    \begin{pmatrix}
      1&1\\2&2
    \end{pmatrix}
    \begin{pmatrix*}[r]
      -1 &1\\ 1&-1
    \end{pmatrix*}=
    \begin{pmatrix}
      0&0\\0&0
    \end{pmatrix}
  \]
  however
  \[
    \begin{pmatrix*}[r]
      -1 &1\\ 1&-1
    \end{pmatrix*}
    \begin{pmatrix}
      1&1\\2&2
    \end{pmatrix}=
    \begin{pmatrix*}[r]
      1&1\\-1&-1
    \end{pmatrix*}\neq \vec{0}
  \]
  Therefore, these two matrices are the ones we wish to find.
\end{proof}
% <==
% ==> ex6
\begin{exercise}
  Prove that $\tr(AB)=\tr(BA)$.
\end{exercise}
\begin{proof}
  See on page \pageref{_trace}.
\end{proof}
% <==
% ==> ex7
\begin{exercise}
  Construct a non-zero matrix $A$ such that $A^2=\vec{0}$
\end{exercise}
\begin{proof}
  Let $A=\begin{pmatrix} a&c\\b&d \end{pmatrix}$. Thus
  perform the multiplication, we get
  \[
    \begin{cases}
      a^2 + bc=0\\
      ac+cd=0\\
      ab+bd=0\\
      bc+d^2=0
    \end{cases}
  \]
  for simplicity, we'll choose $a=1$. Hence $bc=-1$ and
  \[
    \begin{cases}
      c(d+1)=0\\
      b(d+1)=1\\
      d^2=1
    \end{cases}
  \]
  this suggests that $d=-1$, and $bc=-1$. Here, we'll choose $b=1$
  and $c=-1$. Therefore, the matrix
  \[ A=\begin{pmatrix}1&-1\\1&-1\end{pmatrix}. \]
\end{proof}
% <==
% ==> ex8
\begin{exercise}
  Find the matrix of the reflection through the line $y=-2x/3$.
\end{exercise}
  \marginpar{
  \begin{center}
    \begin{tikzpicture}[scale=0.9]
      \coordinate [label=-135:$O$] (O) at (0,0);
      \coordinate [label=-95:$A$]  (X) at (3,-2);
      \coordinate [label=90:$B$]   (Y) at (3,0) ;
      \draw[-open triangle 60] (-1,0) -- (3.8,0);
      \draw[-open triangle 60] (0,-2.5) -- (0,1);

      % draw the tick
      \foreach \x in {1,2,3}{
        \draw (\x,0.1)--(\x,-0.1);
      }
      \draw (-0.1,-1)--(0.1,-1);
      \draw (-0.1,-2)--(0.1,-2);

      % draw the dashes
      \draw[dashed] (X)--(0,-2) (X)--(Y);

      \draw (-1,2/3)--(3.5,-2.4);
      %\draw (-1,0.5)--(3.5,-1.75)node[right]{$y=-\frac{1}{2}x$};
      \draw pic[draw, angle eccentricity=2, "$\gamma$"]
        {angle=X--O--Y};
    \end{tikzpicture}
  \end{center}
  }
\begin{proof}
  Let $T$ be this transformation and $\gamma$ be the angle between
  the $x$-axis and the line $y=-2x/3$. Hence $T=R_{\gamma}T_0R_{\gamma}$.
  We then have $\cos\gamma=OB/OA=3/\sqrt{13}$ and 
  $\sin\gamma=-AB/OA=-2/\sqrt{13}$. Thus
  \begin{align*}
    T&=
    \begin{pmatrix*}[r]
      \cos\gamma &-\sin\gamma\\
      \sin\gamma &\cos\gamma
    \end{pmatrix*}
    \begin{pmatrix*}[r]
      1&0\\0&-1
    \end{pmatrix*}
    \begin{pmatrix*}[r]
      \cos\gamma &\sin\gamma\\
      -\sin\gamma &\cos\gamma
    \end{pmatrix*}\\
     &=
     \frac{1}{13}
     \begin{pmatrix*}[r]
       3&2\\-2&3
     \end{pmatrix*}
    \begin{pmatrix*}[r]
      1&0\\0&-1
    \end{pmatrix*}
     \begin{pmatrix*}[r]
       3&-2\\2&3
     \end{pmatrix*}\\
     &=
     \frac{1}{13}
     \begin{pmatrix*}[r]
       3&2\\-2&3
     \end{pmatrix*}
     \begin{pmatrix*}[r]
       3&-2\\-2&-3
     \end{pmatrix*}\\
     &=
     \frac{1}{15}
     \begin{pmatrix*}[r]
       5&-12\\-12&-5
     \end{pmatrix*}.
  \end{align*}
\end{proof}
% <==
% <==
% ==> isomorphism
\section{Isomophism}
% ==> ex1
\begin{exercise}
  Prove that if $A:V\to W$ is an isomorphism and
  $\vec{v_1},\vec{v_2},\dots,\vec{v_n}$ is a basis in $V$,
  then $A\vec{v_1},A\vec{v_2},\dots,A\vec{v_n}$ is a basis in
  $W$.
\end{exercise}
\begin{proof}
  Since $A:V\to W$ is an isomorphism, hence it's invertable i.e. 
  there is a linear transformation $A\inv:W\to V$ such that
  $AA\inv=A\inv A=I$. Thus for any $\vec{w}\in W$, there is a
  $\vec{v}\in V$ such that $A\inv\vec{w}=\vec{v}$. Recall that
  $\{\vec{v_1},\vec{v_2},\dots,\vec{v_n}\}$ is a basis in $V$, then
  there are unique scalars $\alpha_1,\alpha_2,\dots,\alpha_n$ such that
  \[\vec{v}=\alpha_1\vec{v_1}+\alpha_2\vec{v_2}+\cdots+\alpha_n\vec{v_n}.\]
  This implies that
  \begin{align*}
    A\inv\vec{w}&=\alpha_1\vec{v_1}+\alpha_2\vec{v_2}+\cdots+\alpha_n\vec{v_n}\\
    AA\inv\vec{w}&=A(\alpha_1\vec{v_1}+\alpha_2\vec{v_2}+\cdots+\alpha_n\vec{v_n})\\
    \vec{w}&=\alpha_1A\vec{v_1}+\alpha_2A\vec{v_2}+\cdots+\alpha_nA\vec{v_n}\\
  \end{align*}
  Because $\alpha_1,\alpha_2,\cdots,\alpha_n$ are unique, we conclude
  that any $\vec{w}\in W$ can be represented as a unique linear combination
  of $A\vec{v_1},A\vec{v_2},\dots,A\vec{v_2}$. Thus the proof is completed.
\end{proof}
% <==
% ==> ex2
\begin{exercise}
  Find all right inverses of the $1\by 2$ matrix (row) $A=(1,1)$.
  Conclude from here thaat the row $A$ is not left invertable.
\end{exercise}
% <==
% ==> ex3
\begin{exercise}
  Find all the left inverses of the column $(1,2,3)\tran$.
\end{exercise}
\begin{proof}
  Let $A=(1,2,3)\tran$. Because $A$ is a $3\by 1$ matrix, then its
  inverse, say $B$ is a $1\by 3$ matrix. Let 
  $B= \begin{pmatrix} x&y&z \end{pmatrix} $. Hence
  \[AB=
    \begin{pmatrix}
      x&y&z
    \end{pmatrix}
    \begin{pmatrix}
      1\\2\\3
    \end{pmatrix}=
    \begin{pmatrix}
      1
    \end{pmatrix}
  \]
  This implies that $x+2y+3z=1$ or $x=1-2y-3z$. Thus all the left
  inverses of $A$ is in the form
  \[B=
    \begin{pmatrix}
      1-2y-3z & y&z
    \end{pmatrix}
  \]
  where $y,z$ are arbitrary real numbers.
\end{proof}
% <==
% ==> ex4
\begin{exercise}
  Is the column $(1,2,3)\tran$ right invertable?
\end{exercise}
\begin{proof}[Solution]
  The column $(1,2,3)\tran$ is not right invertable, because
  as proved in previous exercise the column $(1,2,3)\tran$ has
  more than one left inverses.
\end{proof}
% <==
% ==> ex5
\begin{exercise}
  Find two matrices $A$ and $B$ that $AB$ is invertable, but
  $A$ and $B$ are not.
\end{exercise}
\begin{proof}[Solution]
  Consider:
  $ A=
    \begin{pmatrix}
      2&1&-1
    \end{pmatrix}
    \quad\text{and}\quad
    B=
    \begin{pmatrix}
      1&2&3
    \end{pmatrix}\tran $
    Note that
    \[
      AB=
    \begin{pmatrix}
      2&1&-1
    \end{pmatrix}
    \begin{pmatrix}
      1\\2\\3
    \end{pmatrix}=
    \begin{pmatrix}
      2+2-3
    \end{pmatrix}=
    \begin{pmatrix}
      1
    \end{pmatrix}
    \]
    However, as proved in previous exercise, we know that
    the matrix $A$ is not invertable. And we wish to prove that
    $B$ is not invertable either. To achieved this, we have to
    find two matrices that are right invertable to $B$. Observe that
    \[
      \begin{pmatrix}
        2&1&-1
      \end{pmatrix}
      \begin{pmatrix}
        1\\0\\1
      \end{pmatrix}=
      \begin{pmatrix}
        1
      \end{pmatrix}
      \quad\text{and}\quad
      \begin{pmatrix}
        2&1&-1
      \end{pmatrix}
      \begin{pmatrix}
        2\\1\\6
      \end{pmatrix}=
      \begin{pmatrix}
        1
      \end{pmatrix}
    \]
    This suggests that $B$ is not invertable. Therefore, we've fond
    matrices $A$ and $B$ such that $AB$ is invertable, yet 
    $A$ and $B$ are not.
\end{proof}
% <==
% ==> ex6
\begin{exercise}
  Suppose the product $AB$ is invertable. Show that $A$ is right 
  invertable, and $B$ is left invertable.
\end{exercise}
\begin{proof}
  Because $AB$ is invertable, then matrix $(AB)\inv$ is defined.
  Observe that
  \[ A\cdot B(AB)\inv=AB\cdot(AB)\inv=I \] and 
  \[ (AB)\inv A\cdot B=(AB)\inv\cdot AB=I\]
  This shows that $A$ is right invertable, and $B$ is left invertable,
  as expected.
\end{proof}
% <==
% ==> ex7
\begin{exercise}
  Let $A$ and $AB$ be invertable. Prove that $B$ is also in invertable.
\end{exercise}
\begin{proof}
  We claim that $(AB)\inv A$ is the inverse of $B$. To prove this,
  observe that 
  \[
    (AB)\inv A\cdot B=(AB)\inv\cdot AB=I
  \] and
  \[
    B\cdot (AB)\inv A=A\inv AB\cdot (AB)\inv A=A\inv IA=I
  \]
  This shows that $(AB)\inv A$ is both the left and the right inverses
  of $B$. Thus, $B$ is invertable.
\end{proof}
% <==
% ==> ex8
\begin{exercise}
  Let $A$ be $n\by n$ matrix. Prove that if $A^2=\vec{0}$ then 
  $A$ is not invertable.
\end{exercise}
\begin{proof}
  Assume by contradiction that $A$ is invertable, meaning there's a
  matrix $A\inv$ such that $AA\inv=A\inv A=I$. Thus
  \[
    A=A^2A\inv=\vec{0}A\inv=\vec{0}
  \]
  Then $I=AA\inv=\vec{0}A\inv=\vec{0}$, a contradiction. Therefore,
  $A$ is not invertable.
\end{proof}
% <==
% ==> ex9
\begin{exercise}
  Suppose $AB=\vec{0}$ for some non-zero matrix $B$. Can $A$ invertable?
\end{exercise}
\begin{proof}
  We claim that $A$ is not invertable. To prove this, we assume by
  contradiction that $A$ is invertable. Hence $A\inv$ exists, and
  \[
    B=A\inv A\cdot B=A\inv\cdot AB=A\inv\vec{0}=\vec{0}
  \]
  which is a contradiction that $B$ is non-zero.
\end{proof}
% <==
% ==> ex10
\begin{exercise}
  Write matrices of the linear transformations $T_1$ and $T_2$ in $\F^5$,
  defined as follows: $T_1$ interchanges the coordinates $X_2$ and $x_2$
  of the vector $\vec{x}$, and $T_2$ just adds to the coordinates 
  $x_2\to$
  $a$ times the coordinate $x_4$. and does not change other coordinates, i.e.
  \[
    T_1
    \begin{pmatrix}
      x_1\\x_2\\x_3\\x_4\\x_5
    \end{pmatrix}=
    \begin{pmatrix}
      x_1\\x_4\\x_3\\x_2\\x_5
    \end{pmatrix}
    \quad\text{and}\quad
    T_2
    \begin{pmatrix}
      x_1\\x_2\\x_3\\x_4\\x_5
    \end{pmatrix}=
    \begin{pmatrix}
      x_1\\x_2+ax_4\\x_3\\x_4\\x_5
    \end{pmatrix}.
  \]
  where $a$ is a fixed number.
  Show that $T_1$ and $T_2$ are invertable transformations, and write
  the matrices of the inverses.
\end{exercise}
\begin{proof}
  The matrix of this 
\end{proof}
% <==



% <==
% ==> subspace
\section{Subspaces}


% ==> ex1
\begin{exercise}
  Let $X$ and $Y$ be subspaces of a vector space $V$. Prove that
  $X\cap Y$ is a subspace of $V$.
\end{exercise}
\begin{proof}
  Let $\vec{a}$ and $\vec{b}$ be arbitrary vectors of $X\cap Y$.
  Because $X$ and $Y$ are themselves  subspaces of $V$, hence 
  \[
    \begin{cases}
      \vec{a}\in X\\
      \vec{b}\in X
    \end{cases}\implies
    \begin{cases}
      \alpha\vec{a}\in X\\
      \beta\vec{b}\in X
    \end{cases}
  \]
  this implies that $\alpha\vec{a}+\beta\vec{b}\in X$ for
  any scalars $\alpha,\beta$. Similarly, $\alpha\vec{a}+\beta\vec{b}\in Y$.
  Thus $\alpha\vec{a}+\beta\vec{b}\in X\cap Y$. Therefore, 
  $X\cap Y$ is also a subspace of $V$.
\end{proof}
% <==
% ==> ex2
\begin{exercise}
  Let $V$ be a vector space. For $X,Y\subset V$ the sum $X+Y$
  is the collection of all vectors $\vec{v}$ which can be represented as 
  $\vec{v}=\vec{x}+\vec{y}$ where $\vec{x}\in X$ and $\vec{y}\in Y$.
  Show that if $X$ and $Y$ are subspaces of $V$, then $X+Y$ is also
  a subspace of $V$.
\end{exercise}
\begin{proof}
  Let $\vec{v_1},\vec{v_2}\in X+Y$ then there are $\vec{x_1},\vec{x_2}\in X$
  and $\vec{y_1},\vec{y_2}\in Y$ such that
  \[
    \vec{v_1}=\vec{x_1}+\vec{y_1}
    \quad\text{and}\quad
    \vec{v_2}=\vec{x_2}+\vec{y_2}
  \]
  Because $X,Y$ are subspaces of $V$, then
  \[
    \alpha\vec{v_1}+\beta\vec{v_2}
    =\underbrace{(\alpha\vec{x_1}+\beta\vec{x_2})}_{\text{vector of $X$}} + 
    \underbrace{(\alpha\vec{y_1}+\beta\vec{y_2})}_{\text{vector of $Y$}}
  \]
  Hence $\alpha\vec{v_1}+\beta\vec{v_2}$ is also a vector of $X+Y$.
  Thus $X+Y$ is a subspace of $V$.
\end{proof}
% <==
% ==> ex3
\begin{exercise}
  Let $X$ be a subspace of a vector space $V$, and let $\vec{v}\in V$
  and $\vec{v}\notin X$. Prove that if $\vec{x}\in X$ then
  $\vec{x}+\vec{v}\notin X$.
\end{exercise}
\begin{proof}
  We'll prove this by contradiction by assuming that 
  $\vec{x}+\vec{v}\in X$. Because $X$ is a subspace of $V$ and 
  $\vec{x}\in X$, hence $\vec{-x}\in X$. Thus
  \[
    (\vec{x}+\vec{v})+(\vec{-x})\in X
  \]
  and we conclude that $\vec{v}\in X$, which is a contradiction.
\end{proof}
% <==
% ==> ex4
\begin{exercise}
  Let $X$ and $Y$ be subspaces of a vector space $V$. Using the previous
  exercise, show that $X\cup Y$ is a subspace iff $X\subset Y$ or 
  $Y\subset X$.
\end{exercise}
\begin{proof}
  We need to prove this in two directions.
  \begin{itemize}
    \item If $X\subset Y$ or $Y\subset X$: Without loss of generality, 
      we may assume that $Y\subset X$. Therefore $X\cup Y=X$ is a 
      subspace of $V$.
    \item If $X\not\subset Y$ and $Y\not\subset Y$, we now wish to show that
      $X\cup Y$ is not a subspace. Observe that if
      $X\not\subset Y$ and $Y\not\subset X$ that means there are 
      $\vec{x_0}\in X$ and $\vec{y_0}\in Y$ such that $\vec{x_0}\notin Y$
      and $\vec{y_0}\notin X$. Hence $\vec{x_0},\vec{y_0}\in X\cup Y$ and
      follow from the previous exercise, we conclude that
      \[
        \vec{x_0}+\vec{y_0}\notin X
        \quad\text{and}\quad
        \vec{x_0}+\vec{y_0}\notin Y
      \]
      Therefore, $\vec{x_0}+\vec{y_0}\notin X\cup Y$. This suggests that
      $X\cup Y$ is not a vector space. Hence the proof is completed.
  \end{itemize}
\end{proof}
% <==
% <==
<==
% ==> 2. Matrix
\chapter{Systems of linear equations}

% ==> intro
\section{Different faces of linear transformation}
% <==
% ==> Echelon form
\section{Solution of a linear system. Echelon forms}

% ==> ex1
\begin{exercise}
  Write the systems of equations below in matrix form.
\end{exercise}
% <==
% ==> ex2
\begin{exercise}
  Find all solutions of the vector equation
  \[x_1\vec{v_1}+x_2\vec{v_2}+x_3\vec{v_3}=\vec{0}\]
  where $\vec{v_1}=(1,1,0)\tran,~\vec{x_2}=(0,1,1)\tran$ and 
  $\vec{v_3}=(1,0,1)\tran.$ What conclusion can you make about linear
  independence (dependence) of the system of vectors 
  $\vec{v_1},\vec{v_2},\vec{v_3}$.
\end{exercise}
\begin{proof}
  The echelon form of the system is
  \[
    \begin{pmatrix}
      1 &0 &1\\
      1 &1 &0\\
      0 &1 &1
    \end{pmatrix}
    \sim
    \begin{pmatrix}
      1 &0 &1\\
      0 &1 &-1\\
      0 &0 &2
    \end{pmatrix}
  \]
  so, clearly the solution to this equation is
  $x_1=x_2=x_3=0$.

  \textbf{Conclusion.} If the vectors $\vec{v_1},\vec{v_2},\vec{v_3}$
  are linearly independence, then the above equation has unique 
  solution, namely $x_1=x_2=x_3=0$.

  If the vectors $\vec{v_1},\vec{v_2},\vec{v_3}$ are linearly 
  dependence, then there exists $\alpha, \beta,\gamma$ 
  (some of them are non-zero) such that
  \[\alpha\vec{v_1}+\beta\vec{v_2}+\gamma\vec{v_3}=\vec{0}\]
  Therefore, the solution to the above equation is
  \[
    \begin{cases}
      x_1=\alpha t\\
      x_2=\beta t\\
      x_3=\gamma t
    \end{cases}
  \]
  for some $t\in\R$.
\end{proof}
% <==

% <==
% ==> Analyzing pivots
\section{Analyzing pivots}


\setcounter{exercise}{5}
% ==> ex6
\begin{exercise}
  Prove or disprove. If the columns of a square ($n\by n$)
  matrix are linearly independence, so are the columns of $A^2$.
\end{exercise}
\begin{proof}
  Since $A$ is a square matrix, and columns vectors are linearly 
  independence, so
  \begin{align*}
    &\text{has pivot in every column}\\\implies\quad
    &\text{has pivot in evry row}\\\implies\quad
    &A\text{ is invertable}\\\implies\quad
    &A^2\text{ also invertable}\\\implies\quad
    &A^2\text{ has pivot every column and row}
  \end{align*}
  Therefore, the column vectors of $A^2$ also independence.
\end{proof}
% <==
% ==> ex7
\begin{exercise}
  Prove or disprove. If the columns of a square ($n\by n$)
  matrix are linearly independence, so are the columns of $A^3$.
\end{exercise}
% <==
% ==> ex8
\begin{exercise}
  Show that if the equation $A\vec{x}=\vec{0}$ has unique solution,
  then $A$ is left invertable.
\end{exercise}
\begin{proof}
  Because $A$ has unique solution, then it has pivot in every
  column. Therfore, \#col $\leq$ \#row, so we let $m\by n$ be the size
  of $A$ where $n\leq m$
  Let $R$ be the reduced 
  echelon form of $A$, hence there exists $E=E_k\cdots E_2E_1$
  such that $R=EA$.
  Observe that, $R$ would look like
  \[
    R=
    \begin{pmatrix}
      \fbox{1} &0  &\cdots &0\\
      0 &\fbox{1}  &\cdots &0\\
      \vdots&\vdots&\ddots & \\
      0 &0  &\cdots &\fbox{1}\\
      0 &0  &\cdots &0\\
      0 &0  &\cdots &0\\
    \end{pmatrix}
  \]
  Using matrix muliplication gives us $R\tran R=I_n$. We obtain that
  \[
    R\tran EA=R\tran T=I_n
  \]
  Therfore, 
  \fbox{$A$ is left invertable, and $R\tran E$ is its left inverse}.



\end{proof}
% <==
% <==
% ==> find A-inverse
\section{Find $A\inv$ by row reduction}
% <==
% ==> dimension
\section{Dimension}

% ==> ex1
\begin{exercise}
  True or false.
  \begin{enumerate}
    \item Every vector space that is generated by a finite set has a basis;
    \item Every vector space has a (finite) basis;
    \item A vector space cannot have more that one basis;
    \item A vector space has a finite basis, then the number of vectors
      in every basis is the same;
    \item The dimension of $\P_n$ is $n$;
    \item The dimension of $M_{m\by n}$ is $m+n$;
    \item If vectors $\vec{v_1},\vec{v_2},\dots,\vec{v_n}$ generate (span)
      the vector space $V$, then every vector in $V$ can be written as
      a linear combination of vectors $\vec{v_1},\vec{v_2},\dots,\vec{v_n}$
      in only one way;
    \item Every subspace of a finite-dimensional space is finite-dimensional.
    \item If $V$ is a vector space having dimension $n$, then $V$ has exactly
      one subspace of dimension $0$, and exactly one subspace of dimension
      $n$.
  \end{enumerate}
\end{exercise}
\begin{proof}
  \begin{enumerate}
    \item \textbf{True.} That finite set which generated a vector space
      is the spanning set itself. Since it's finite, it contains a
      basis.
    \item \textbf{False.} Take $\R[x]$ for example.
    \item \textbf{False.} In $\R^2$, one can choose 
      \[
        \left\{\binom{1}{0}, \binom{0}{1}\right\}
        \quad\text{or}\quad
        \left\{\binom{1}{1}, \binom{0}{1}\right\}
      \]
      as a basis.
    \item \textbf{True.} As proved in the above theorems,
      \begin{align*}
        &\text{\# independence vectors}\leq \dim V\\
        &\text{\# generating vectors}\geq \dim V
      \end{align*}
      hence the number of any basis in $V$ must be exactly $\dim V$ vectors.
    \item \textbf{False.} In $\P_n$, the standard basis is
      \[
        1,~t,~t^2,\dots,t^n
      \]
      which has $n+1$ vectors. Hence $\boxed{\dim\P_n=n+1}$.
    \item \textbf{False.} The standard basis in $M_{m\by n}$ is
      \[
        \{\vec{e}_{11},\vec{e}_{12},\dots,\vec{e}_{mn}\}
      \]
      has $m\times n$ vectors. Hence $\boxed{\dim M_{m\by n}=mn}$.
    \item \textbf{False.} span doesn't guarantee uniqueness.
    \item \textbf{True.} Let $W$ be a subspace of $V$. Because $\dim V$
      finite, we can find 
      $$\mathcal{A}=\{\vec{v_1},\vec{v_2},\dots,\vec{v_n}\}\subset V$$
      that spans $V$. WLOG, we assume that none of vectors in $\mathcal{A}$
      belongs to $W$ (the unluckiest case.) We can choose 
      $\vec{w_1}\in W$ such that $\vec{w_1}\neq\vec{0}$. Then
      \[
        \vec{w_1}=\alpha_1\vec{v_1}+\cdots+\alpha_n\vec{v_n}
      \]
      Because $\vec{w_1}\neq 0$, we're sure some of the $\alpha_i$'s are
      non-zero, say $\alpha_1$. Then the new system
      \[
        \mathcal{A}_1=\{\vec{w_1},\vec{v_2},\dots,\vec{v_n}\}
      \]
      still spans the space $V$. Now, if $\mathcal{B}_1:=\{\vec{w_1}\}$
      doesn't span $W$, we can repeat the above procedure and find 
      $\vec{w_2}$. We can do this at most $n$ times, because once
      we reach the $n$th step, we have the new system 
      $\mathcal{A}_n\subset W$ that spans the whole space $V$.

      Therfore, after some finite $k\le n$ step, we have
      \[
        \mathcal{B}_k=\{\vec{w_1},\vec{w_2},\dots,\vec{w_k}\}\subset W
      \]
      spans $W$. Hence, $W$ is finite dimensional.
    \item \textbf{Not sure.}
  \end{enumerate}
\end{proof}
% <==
% ==> ex2
\begin{exercise}
  Prove that if $V$ is a vector space having dimension $n$, then
  a system of vectors $\vec{v_1},\vec{v_2},\dots,\vec{v_n}$ in $V$
  is linearly independent iff it spans $V$.
\end{exercise}
\begin{proof}
  \text{}
  \begin{itemize}
    \item[$(\Longrightarrow)$] Suppose that $v_1,\dots,v_n$ linearly
      independent in $V$. If it's not span, then we can complete
      this system to a basis, but then this new system will have 
      the number of vectors more than $n$, but this is a contradiction
      because a basis (linearly independent) cannot have vectors more 
      than $n$.
    \item[$(\Longleftarrow)$] Now suppose that $v_1,\dots,v_n$ spans
      $V$. If it's not linearly independence, we can throw away some
      vectors, and become the basis. However, this new basis system
      has less that $n$ vectors in it, which is a contradiction because
      a basis (span) must have more than $n$ vectors in it.
  \end{itemize}
\end{proof}
% <==
% ==> ex3
\begin{exercise}
  Prove that a linearly independent system of vectors
  $\vec{v_1},\vec{v_2},\dots,\vec{v_n}$ in a vector
  space $V$ is a basis iff $n=\dim V$.
\end{exercise}
\begin{proof}
  We have $\vec{v_1},\dots,\vec{v_n}$ linearly independent.
  If it's a basis then $\dim V=n$. Now suppose that $\dim V=n$.
  We need to show that the system is a basis. From the above
  exercise, this system has to span $V$. Thus a basis in $V$.
\end{proof}
% <==
% ==> ex4
\begin{exercise}
  (An old problem revisited.) Is it possible that 
  vectors $\vec{v_1}, \vec{v_2}, \vec{v_3}$ are linearly
  dependent, but the vectors 
  $\vec{w_1}=\vec{v_1}+\vec{v_2},~\vec{w_2}=\vec{v_2}+\vec{v_3}$
  and $\vec{w_3}=\vec{v_3}+\vec{v_1}$ are linearly independent.
\end{exercise}
\begin{proof}
  Because $\vec{w_1},\vec{w_2},\vec{w_3}$ linearly independent, then
  \[\dim\spans\{\vec{w_1},\vec{w_2},\vec{w_3}\}=3.\]
  Because $\vec{w_1}=\vec{v_1}+\vec{v_2},~\vec{w_2}=\vec{v_2}+\vec{v_3}$,
  it's easy to see that
  \[
    \spans\{\vec{w_1},\vec{w_2},\vec{w_3}\}\subseteq
    \spans\{\vec{v_1},\vec{v_2},\vec{v_3}\}
  \]
  %
  But since the vectors $\vec{v_1},\vec{v_2},\vec{v_3}$ linearly dependent, 
  then one vector, say $\vec{v_3}$, is the linear combination of 
  $\vec{v_1},\vec{v_2}$. This suggests that 
  $\dim\spans\{\vec{v_1},\vec{v_2},\vec{v_3}\}=
  \dim\spans\{\vec{v_1},\vec{v_2}\}\leq 2$, 
  thus
  \[
    \dim\spans\{\vec{w_1},\vec{w_2},\vec{w_3}\}\leq
    \dim\spans\{\vec{v_1},\vec{v_2},\vec{v_3}\}
    \leq 2
  \]
  But this contradicts to the result we found above. Hence it's 
  impossible that such $\vec{v_1},\vec{v_2},\vec{v_3}$ exists.
\end{proof}
% <==
% ==> ex5
\begin{exercise}
  Let vectors $\vec{u},\vec{v},\vec{w}$ be a basis in $V$. Show that
  $\vec{u}+\vec{v}+\vec{w},~\vec{v}+\vec{w},~\vec{w}$ is also
  a basis in $V$.
\end{exercise}
\begin{proof}
  It can be shown by using the fact from the exercise above
  i.e. show that the new system is either spans or LI 
  (because it already has 3 vectors in it).
  But here I'm gonna use matrix and pivot stuff. 
  Since $V$ has three vectors as a basis, then
  $\dim V=3$. Choose an isomorphism $A:V\to\R^3$ such that
  $A\vec{v}=(1,0,0),~A\vec{u}=(0,1,0)$ and $A\vec{w}=(0,0,1)$. 
  Letting
  \begin{align*}
    &\vec{a_1}:=A(\vec{u}+\vec{v}+\vec{w})=(1,1,1)\\
    &\vec{a_2}:=A(\vec{v}+\vec{w})=(0,1,1)\\
    &\vec{a_3}:=A(\vec{w})=(0,0,1)
  \end{align*}
  Observe that the Echelon form of the matrix formed by 
  $\vec{a_1},\vec{a_2},\vec{a_3}$
  \begin{align*}
    \begin{bmatrix} 1&0&0\\ 1&1&0\\ 1&1&1 \end{bmatrix}\sim
    \begin{bmatrix} 1&0&0\\ 0&1&0\\ 0&1&1 \end{bmatrix}\sim
    \begin{bmatrix} 1&0&0\\ 0&1&0\\ 0&0&1 \end{bmatrix}
  \end{align*}
  has pivots in every row and every column, hence the system
  $\{\vec{a_1},\vec{a_2},\vec{a_3}\}$ is basis in $\R^3$. Using
  isomorphism theorem, we conclude that
  \[\vec{u}+\vec{v}+\vec{w},~\vec{v}+\vec{w},~\vec{w}\]
  also a basis in $V$.
\end{proof}
% <==



% <==
% ==> rank
\newpage
\section{Rank}

% ==> ex4
\setcounter{exercise}{3}
\begin{exercise}
  Prove that if $A:X\to Y$ and $V$ is a subspace of $X$, then
  $\dim AV\leq\rank A$. Then deduce that $\rank AB\leq\rank A$.
\end{exercise}
\begin{proof}
  Observe that $AV\subseteq AX=\col A$, hence
  \[\dim AV\leq \dim\col A=\rank A.\]
  To prove the next statement, we may assume that $B:Z\to X$.
  This implies that $\col B=BZ\subseteq X$.
  Using the previous result we obtain that
  \begin{align*}
    &\dim A(\col B)\leq\rank A\\\implies\quad
    &\dim\col AB\leq\rank A\\\implies\quad
    &\rank AB\leq\rank A
  \end{align*}
\end{proof}
% <==

% <==
% ==> change of basis
\newpage
\section{Change of basis}
\setcounter{exercise}{2}

% ==> ex3
\begin{exercise}
  Find the change of coordinates matrix that changes the 
  coordinates in basis $\{1,1+t\}$ in $\P_1$ to the 
  coordinates in the basis $\{1-t, 2t\}$.
\end{exercise}
\begin{proof}
  Let's denote $\mathcal{A}=\{1,1+t\}$ and 
  $\mathcal{B}=\{1-t, 2t\}$. Let $\mathcal{S}$
  be the standard basis in $\P_1$. Therefore, 
  the matrix that transforms from vector in basis
  $\mathcal{A}$ to basis $\mathcal{B}$ is 
  $[\mathcal{BA}]= [\mathcal{BS}][\mathcal{SA}]$.
  We have
  \[ [\mathcal{SA}] = \begin{pmatrix} 1 &1 \\ 0 &1 \end{pmatrix} \]
  and 
  \[
    [\mathcal{BS}]=[\mathcal{SB}]\inv=
    \begin{pmatrix} 1  & 0\\ -1 & 2 \end{pmatrix}\inv=\frac{1}{2}
    \begin{pmatrix} 2 &0\\ 1 &1 \end{pmatrix}
  \]
  Therefore, 
  \[
    [\mathcal{BA}]= \frac{1}{2}
    \begin{pmatrix} 2 &0\\ 1 &1 \end{pmatrix}
    \begin{pmatrix} 1 &1 \\ 0 &1 \end{pmatrix}.
  \]
\end{proof}
% <==
% ==> ex4
\begin{exercise}
  Let $T$ be the ...
\end{exercise}
\begin{proof}
  In standard basis, $T$ looks like
  \[ [T]= \begin{pmatrix} 3 &1\\ 1 &-2 \end{pmatrix}.\]
  Let $\mathcal{B}=\{(1,1)\tran, (1,2)\tran\}$. In basis $\mathcal{B}$,
  the transformation would look like
  \[
    [T]_{\mathcal{BB}}= [\mathcal{BS}] [T] [\mathcal{SB}]
  \]
  And we have
  \[ [\mathcal{SB}]= \begin{pmatrix} 1 &1\\ 1 &2 \end{pmatrix} \]
  so
  \[
    [\mathcal{BS}]= [\mathcal{SB}]\inv=
    \begin{pmatrix} 1 &1\\ 1 &2 \end{pmatrix}\inv=
    \begin{pmatrix} 2 &-1\\ -1&1 \end{pmatrix}
  \]
  Therefore, the transformation $[T]_{\mathcal{BB}}$ in basis
  $\mathcal{B}$ is
  \[
    [T]_{\mathcal{BB}}=
    \begin{pmatrix} 2 &-1\\ -1&1 \end{pmatrix}
    \begin{pmatrix} 3 &1\\ 1 &-2 \end{pmatrix}
    \begin{pmatrix} 1 &1\\ 1 &2 \end{pmatrix} 
  \]
\end{proof}
% <==
% ==> ex5
\begin{exercise}
  Prove that if $A$ and $B$ are similar matrices, then
  $\tr A=\tr B$.
\end{exercise}
\begin{proof}
  Because $A$ and $B$ are similar, then there exists 
  an invertable matrix $Q$ such that $A=Q\inv BQ$. Observe that
  \[
    A=Q\inv\cdot BQ
    \quad\text{and}\quad
    B=BQ\cdot Q\inv
  \]
  This implies that $\tr A=\tr B$.
\end{proof}
% <==

% <==
% <==
% ==> 3. Determinant
\chapter{Determinant}
\setcounter{section}{2}
\section{Constructing the determinant}
% ==> ex4
\setcounter{exercise}{3}
\begin{exercise}
  A square $(n\times n)$ matrix is called skew-symmetric 
  (or antisymmetric) if $A\tran = -A$. Prove that if $A$ is 
  skew-symmetric and $n$ is odd, then $\det A=0$. Is it
  true for even $n$?
\end{exercise}
\begin{proof}
  When $n$ is odd, we have
  \begin{align*}
    \det A=\det A\tran=\det (-A)=(-1)^n\det A=-\det A
  \end{align*}
  This implies that $\det A=0$ whenever $n$ is odd. For even $n$,
  the determinant $\det A$ doestn't neccessarily zero, for instance
  \[ B=\begin{pmatrix} 0&-1\\1&0 \end{pmatrix} \]
  is a skew-symmetric matrix, yet $\det B=1$.
\end{proof}
% <==
% ==> ex5
\begin{exercise}
  A square matrix is called \emph{nilpoten} if $A^k=0$
  for some $k\in\N$. Show that if $A$ is nilpoten, then
  $\det A=0$.
\end{exercise}
\begin{proof}
  We use the property of determinant,
  \[ \det 0=\det A^k=(\det A)^k \]
  This shows that $\det A=0$.
\end{proof}
% <==
% ==> ex6
\begin{exercise}
  Prove that if the matrices $A$ and $B$ are similar,
  then $\det A=\det B$.
\end{exercise}
\begin{proof}
  Since $A\sim B$, hence there's invertable $Q$ such that
  $A=QBQ\inv$. Therefore
  \begin{align*}
    \det A=\det QBQ\inv
    &=(\det Q)(\det B)(\det Q\inv)\\
    &=\det(Q)(\det Q\inv)(\det B)=\det(QQ\inv)(\det B)\\
    &=\det B.
  \end{align*}
\end{proof}
% <==
% ==> ex7
\begin{exercise}
  A real square matrix $Q$ is called \emph{othogonal} if 
  $Q\tran Q=I$. Prove that if $Q$ is a n orthogonal matrix
  then $\det Q=\pm 1$.
\end{exercise}
\begin{proof}
  We have
  \[ 1=\det Q\tran Q=(\det Q\tran)(\det Q)=(\det Q)^2 \]
  this implies that $\det Q=\pm 1.$
\end{proof}
% <==

\section{Formal definition}
% ==> ex2
\setcounter{exercise}{1}
\begin{exercise}
  Let $P$ be a permutation matrix.
  \begin{itemize}
    \item Can you describe the corresponding linear
      transformation?
    \item Show that $P$ is invertable. Can you describe $P\inv$.
    \item Show that for some $N>0$, $P^N=I$.
  \end{itemize}
\end{exercise}
\begin{proof}
  Suppose that $P$ is an $n\times n$ matrix.
  \begin{itemize}
    \item The linear transformation looks like it's swapping 
      the axis in $\R^n$.
    \item If we interchange columns of $P$, we'll get the indentity
      matrix hence $\det P=\pm 1$. The direct computation shows that
      $P\inv =P\tran$.
    \item Because there are finitely many permutations, the sequence
      $\{P^n\}$ will eventually have repetitions. Hence we're sure 
      there is $i,j$ with $i>j$ such that $P^i=P^j$. Since $P$ is 
      invertable, we can multiply both sides by $P^{-j}$. Therefore
      \[P^{i-j}=I,\]
      now choose $N:=i-j$ and this completes the proof.
  \end{itemize}
\end{proof}
% <==
% ==> ex3
\begin{exercise}
  Why is there an even number of permutations of $(1,2,\dots,9)$
  and why are exactly half of them odd permutations?
\end{exercise}
\begin{proof}
  There are $9!$ permutations of $(1,2,\dots,9)$, which is an even 
  number. To prove that there are exactly half of them odd 
  (and other half even) we consider a $9\times 9$ matrix $A$ with all 
  the entries are $1$. The determinant is $\det A=0$. However,
  \[
    \det A=\sum_{\sigma\in\perm(9)}
    a_{\sigma(1),1}\cdots a_{\sigma(n),n}\sign(\sigma)
  \]
  Since all the $a_{jk}=1$, we get 
  \[\sum_{\sigma}\sign\sigma=\det A=0\]
  Because there are even number of permutations, and
  $\sign\sigma=\pm 1$ we must have half of them has $\sign=1$ and
  the other half has $\sign=-1$.
\end{proof}
% <==

\section{Cofactor}
\section{Minor and rank}

% ==> ex2
\setcounter{exercise}{1}
\begin{exercise}
  Let $A$ be an $n\times n$ matrix. How are $\det(3A),~\det(-A)$
  and $\det(A^2)$ related to $\det A$ ?
\end{exercise}
\begin{proof}
  It follows immediately that 
  \begin{align*}
    &\det(-A)=(-1)^n\det A,\\
    &\det(3A)=3^n\det A,\\
    &\det(A^2)=(\det A)^2.
  \end{align*}
\end{proof}
% <==
% ==> ex3
\begin{exercise}
  If the entries of both $A$ and $A\inv$ are integers,
  is it possible that $\det A=3$ ?
\end{exercise}
\begin{proof}
  It's impossible. If it was such a case, we'll get 
  $\det A\inv=1/\det A=1/3$. Since all the entries of $A\inv$ are
  integers, so
  \[
    \det A\inv=\sum_{\sigma}a_{\sigma(1),1}\cdots 
    a_{\sigma(n),n}\sign\sigma\in\Z
  \]
  which is a contradiction that $\det A\inv=1/3$.
\end{proof}
% <==
% ==> ex4
\begin{exercise}
  Let $\vec{v_1},\vec{v_2}$ be vectors in $\R^2$ and let $A$ be the
  $2\times 2$ matrix with columns $\vec{v_1},\vec{v_2}$. Prove that
  $\abs{\det A}$ is the area of parallelogram with two sides given by
  vectors $\vec{v_1}$ and $\vec{v_2}$.
\end{exercise}
\begin{proof}
  Let $\alpha$ be angle between $\vec{v_1}$ and the $x$-axis, and
  let $R$ be the matrix of rotation by $-\alpha$ angle. Denote
  \[
    \vec{\tilde{v}_1}:=R\vec{v_1}=(a,0)\quad\text{and}\quad
    \vec{\tilde{v}_2}:=R\vec{v_2}=(b,c).
  \]
  for some reals $a,b,c$. 
  Note that after the transformation, the area stays the same, i.e.
  $\mathrm{area}(\vec{v_1},\vec{v_2})
  =\mathrm{area}(\vec{\tilde{v}_1},\vec{\tilde{v}_2})$. It's easy 
  to see that the area of the new parallelogram is 
  \[\abs{ac}=\abs{\det (\vec{\tilde{v}_1},\vec{\tilde{v}_2})}\]
  (it has  base $\abs{a}$, and height $\abs{c}$). But
  \begin{align*}
    \det(\vec{\tilde{v}_1},\vec{\tilde{v}_2})
    &=\det(R\vec{v_1}, R\vec{v_2})=\det(RA)=\det A
  \end{align*}
  because $\det R=1$ (rotation matrix). This implies that the area of
  the parallelogram is $\abs{\det A}$.
\end{proof}
% <==
% ==> ex5
\begin{exercise}
  Let $\vec{v_1},\vec{v_2}$ be vectors in $\R^2$. Show that 
  $\det(\vec{v_1},\vec{v_2})>0$ if and only if there's a rotation
  $T_{\alpha}$ such that $T_{\alpha}\vec{v_1}$ parallel to $\vec{e_1}$
  and $T_{\alpha}\vec{v_2}$ is in the upper half-plane.
\end{exercise}
\begin{proof}
  \text{}
  \begin{itemize}
    \item[$(\Rightarrow)$] For this direction, the proof is almost 
      identical to the previous exercise. We claim that 
      $T_{\alpha}=R$ ($R$ defined in the previous exercise).
      We now wanna show that $\vec{\tilde{v}_2}=(b,c)$ is in the 
      upper-half plane, meaning $c>0$. But this immediately true 
      from the fact that $a>0$ and
      \[
        0<\det(\vec{v_1},\vec{v_2})=
        \det(\vec{\tilde{v}_1},\vec{\tilde{v}_2})=ac
      \]
    \item[$(\Leftarrow)$]
  \end{itemize}
\end{proof}
% <==
<==
% ==> 4. spectral theory
\chapter{Intro to Spectral Theory}

\section{Main definition}
% ==> ex1
\begin{exercise}
  True or false.
  \begin{enumerate}[label={(\alph*)}]
    \item Every linea operator in an $n$-dimensional vector space
      has $n$ distinct eigenvalues;
    \item If a matrix has on eigenvalue, it has infinitely many
      eigenvectors;
    \item There exists a square real matrix with no real eigenvalues;
    \item There exists a square matrix with no (complex) eigenvectors;
    \item Similar matrices always have the same eigenvalues;
    \item Similar matrices always have the same eigenvectors;
    \item The sum of two eigenvectors of a matrix $A$ is always an
      eigenvector;
    \item The sum of two eigenvectors of a matrix $A$ corresponding
      to the same eigenvalue $\lambda$ is always an eigenvector.
  \end{enumerate}
\end{exercise}
\begin{proof}[Solution]
  \begin{enumerate}[label={(\alph*)}]
    \item \textbf{false}
    \item \textbf{true}. Suppose $\vec{v}$ be an eigenvector corresponding to 
      the eigenvalue $\lambda$ of a matrix $A$, then 
      $A\vec{v}=\lambda\vec{v}$. For any $\alpha\in\F$, we have
      \[
        A(\alpha\vec{v})=\alpha A\vec{v}=\alpha\lambda\vec{v}
        =\lambda (\alpha\vec{v})
      \]
      hence $\alpha\vec{v}$ is also an eigenvector of $\lambda$.
      Because of the choise of $\alpha$ is abitrary, we conclude that
      it has infinitely many eigenvectors.
    \item \textbf{true}. It's easy to see that the rotation matrix
      $R_\gamma$ where $\gamma\neq n\pi$ has no real eigenvalue. 
      Below, we present the special case when $\gamma=\pi/2$.
      Namely, the rotation matrix about $90^\circ$ 
      \[
        A:=
        \begin{pmatrix*}[r]
          0 &-1\\ 1&0
        \end{pmatrix*}.
      \]
      The characteristic polynomial 
      \[\det(A-\lambda I)=(-\lambda)(-\lambda)+1=\lambda^2+1\]
      has roots $\lambda=\pm i$, both roots are complex numbers.
    \item \textbf{false.} From fundamental theorem of Algebra,
      any polynomial with degrees greater than 2 always has
      complex roots. So, there's always an eigenvalue in complex
      space.
    \item \textbf{true.} As proved in the text book, if matrices 
      $A$ and $B$ are similar, then the determinant
      \[\det(A-\lambda I)=\det(B-\lambda I)\]
      therefore, $A$ and $B$ have the same eigenvalue.
    \item \textbf{true and false.} Since $A\sim B$, they're basically the same
      transformation but in different basis. We proved earlier that they have
      the same eigenvalues, therefore the asbtract eigenvector corresponding to 
      this eigenvalue is the same, but numerically different because of the 
      basis we denote them.
    \item \textbf{false.} There are plenty of counterexamples, the simplest one is
      \[ A= \begin{pmatrix} 1&0\\ 0&2 \end{pmatrix} \]
      having eigenvalues $\lambda_1=1$ and $\lambda_2=2$ with the corresponding
      eigenvector 
      \[
        \begin{pmatrix} 1\\0 \end{pmatrix}
        \quad\text{and}\quad
        \begin{pmatrix} 0\\1 \end{pmatrix}
      \]
      however their sum 
      $
        \begin{pmatrix} 1\\0 \end{pmatrix}+
        \begin{pmatrix} 0\\1 \end{pmatrix}=
        \begin{pmatrix} 1\\1 \end{pmatrix}
      $
      is not an eigenvector.
    \item \textbf{true.} Suppose that $\vec{v},\vec{w}$ are eigenvectors 
      corresponding to an eigenvector $\lambda$. Then
      \begin{align*}
        &A\vec{v}=\lambda\vec{v}
        \quad\text{and}\quad 
        A\vec{w}=\lambda\vec{w}\\\implies\quad
        &A(\vec{v}+\vec{w})=\lambda(\vec{v}+\vec{w})
      \end{align*}
      hence $\vec{v}+\vec{w}$ is also an eigenvector of $\lambda$.
  \end{enumerate}
\end{proof}
% <==
% ==> ex5
\setcounter{exercise}{4}
\begin{exercise}
  Prove that eigenvalues (counting multiplicities) of a
  triangular matrix coincide with its diagonal entries.
\end{exercise}
\begin{proof}
  Let $\lambda_1,\lambda_2,\dots,\lambda_n$ be the diagonal entries of 
  the triangular matrix $A$. Suppose that $A$ is upper triangular 
  matrix,
  \[
    A=
    \begin{pmatrix}
      \lambda_1 & & & \vec{*}\\
                & \lambda_2 & & &\\
                & & \ddots  & & \\
      \vec{0}   & & &  \lambda_4
    \end{pmatrix}
  \]
  The characteristic polynomial of $A$ is
  \[
    \det(A-\lambda I)=\prod_{i=1}^{n}(\lambda_i-\lambda)
  \]
  and the roots of this polynomial are exactly 
  $\lambda_1,\lambda_2,\dots,\lambda_n$. For lower triangular 
  matrix, we carry out the same proof.
\end{proof}
% <==
% ==> ex6
\begin{exercise}
  An operator $A$ is called \emph{nilpoten} if $A^k=\vec{0}$ for 
  some $k$. Prove that if $A$ is nilpoten, then 
  $\sigma(A)=\{0\}$.
\end{exercise}
\begin{proof}
  First, we prove that $0$ is an eigenvalue of $A$, that is 
  $\sigma(A)\neq\varnothing$. Then we show that if 
  $\lambda\in\sigma(A)$, then $\lambda=0$.
  \begin{itemize}
    \item Using the property of determinant, we have
      \[0=\det(A^k)=(\det A)^k\]
      hence $\det A=\det(A-0I)=0$. We conclude that 
      $0\in\sigma(A)$.
    \item Lastly, let $\lambda\in\sigma(A)$ be an abitrary eigenvalue
      of $A$. Then there exists $\vec{v}\neq\vec{0}$ such that
      \begin{align*}
        &A\vec{v}=\lambda\vec{v}\\\implies\quad
        &A^2\vec{v}=A\lambda\vec{v}=\lambda A\vec{v}=\lambda^2\vec{v}
      \end{align*}
      keep appying this for the total of $k$ times, we'd get
      $\lambda^k\vec{v}=A^k\vec{v}=\vec{0}$. Since $\vec{v}$ is a non
      zero vector, we must have $\lambda=0$.
  \end{itemize}
\end{proof}
% <==
% ==> ex7
\begin{exercise}
  Show that the characteristic polynomial of a block triangular
  matrix \[M:=\begin{pmatrix} A&*\\ \vec{0}&B \end{pmatrix}\]
  where $A$ and $B$ are square matrices, coincides with 
  $\det(A-\lambda I)\det(B-\lambda I)$.
\end{exercise}
\begin{proof}
  Let $A':=A-\lambda I$ and $B':=B-\lambda I$. Therefore, the 
  characteristic polynomial of $M$ is
  \begin{align*}
    \det(M-\lambda I)
    &=\det \begin{pmatrix} A'&*\\\vec{0}&B' \end{pmatrix}\\
    &=\det(A')\det(B')\\
    &=\det(A-\lambda I)\det(B-\lambda I)
  \end{align*}
  where the second equality comes from the determinant of block 
  the matrix.
\end{proof}
% <==
% ==> ex8
\begin{exercise}
  Let $\vec{v_1},\vec{v_2},\dots,\vec{v_n}$ be a basis in a vector 
  space $V$. Assume also that the first $k$ vectors 
  $\vec{v_1},\vec{v_2},\dots,\vec{v_k}$ of the basis are eigenvectors
  of an operator $A$, corresponding to an eigenvalue $\lambda$. Show
  that in this basis the matrix of the operator $A$ has block 
  triangular form 
  \[\begin{pmatrix}\lambda I_k&*\\\vec{0}&B\end{pmatrix}\]
  where $I_k$ is $k\times k$ identity matrix and $B$ is some 
  $(n-k)\times (n-k)$ matrix.
\end{exercise}
\begin{proof}
  Let $\mathcal B=\{\vec{v_1},\dots,\vec{v_n}\}$. To write $A$ in this
  basis, we need to know what happens to each basis vector. Now, 
  for the first $k$ vectors, for each $j=1,2,\dots,k$ we have
  \[
    A\vec{v_j}=\lambda\vec{v_j}=
    \begin{pmatrix}0\\\vdots\\\lambda\\\vdots\\0\end{pmatrix}
    _{\mathcal B}
  \]
  where $\lambda$ is at the $j$-th position. And for the others
  $(n-k)$ vectors, nah we don't really care about them. The matrix 
  $A$ written in basis $\mathcal B$ would look like
  \[
    A=
    \begin{pmatrix}
      \lambda_1 & 0 & \cdots & 0 & \\
      0 & \lambda_2 & \cdots & 0 & \\
      \vdots & \vdots & \ddots & \vdots & \text{\fbox{what ev}}\\
      0 & 0 & \cdots & \lambda_n & \\
        &&&&\\
        & \text{\fbox{zeros}} &&&\text{\fbox{ $(n-1)$ block}}
    \end{pmatrix}=
    \begin{pmatrix}
      \lambda I_k&*\\\vec{0}&B
    \end{pmatrix}
  \]
\end{proof}
% <==
% ==> ex9
\begin{exercise}
  Use the two previous exercises to prove that geometric multiplicity
  of an eigenvalue cannot exceed its algebraic multiplicity.
\end{exercise}
\begin{proof}
  Let $\lambda\in\sigma(A)$ be an arbitrary basis in operator 
  $A:V\to V$. Suppose that $\vec{v_1},\dots,\vec{v_k}$ be basis 
  in subspace $\ker(A-\lambda I)$. Then we can complete this 
  system to a basis in $V$, hence we can let 
  \[
    \mathcal B=\{\vec{v_1},\dots,\vec{v_k},\dots, \vec{v_n}\}
  \]
  be a basis in $V$ (we implicitely define $\dim V=n$). From the previous
  exercise, we can write $A$ in that basis $\mathcal B$ as follows
  \[
    [A]_{\mathcal B}=
    \begin{pmatrix}
      \lambda I_k&*\\\vec{0}&B
    \end{pmatrix}
  \]
  because eigenvalues don't rely on the choise of basis, $\lambda$ is
  also the root of characteristic polynomial (with variable $x$)
  \begin{align*}
    \det(A-xI)
    &=\det(\lambda I_k-xI_k)\cdot\det(B-xI_{n-k})\\
    &=(\lambda-x)^k \cdot\det(B-xI)
  \end{align*}
  therefore
  \[
    \boxed{\text{algebraic mult.}\geq k=\text{geometric mult.}}
  \]
\end{proof}
% <==
% ==> ex10
\newpage
\begin{exercise}
  Prove that determinant of a matrix $A$ is equals to the
  product of its eigenvalues (counting multiplicities).
\end{exercise}
\begin{proof}
  Let this square matrix $A=[a]_{jk}$ and  
  $A':=A-\lambda I=[b]_{jk}$ where $b_{jj}=a_{jj}-\lambda$.
  As in previous chapter, we defined $\perm(n)$ be to the set of
  all permutations of $n$ letters. Let $e$ be the indentity
  permutation in $\perm{n}$, that is $e(j)=j$ for all j, and 
  $\sign e=1$. Therefore,
  \begin{align}
    \label{eq_ch4:decompose_det}
    \det(A-\lambda I)
    &=\sum_{\sigma\in\perm n}\sign\sigma\cdot
    \prod_{j=1}^{n}b_{\sigma(j)j}\notag\\
    &=(a_{11}-\lambda)(a_{22}-\lambda)\cdots(a_{nn}-\lambda)+q(\lambda)
  \end{align}
  where $q(\lambda):=\sum_{\sigma\neq e}\sign\sigma
  \cdot\prod_{i=1}^{n}a_{\sigma(j)j}$. For any $\sigma\neq e$,
  there must be at least two letters were swapped. Since all the 
  variables $\lambda$'s are all on the main diagonal, the polynomial
  $\prod_{i=1}^{n}a_{\sigma(j)j}$ must have at most $n-2$ degrees.
  Note also that the coefficient of $\lambda^n$ in this polynomial
  is $(-1)^n$.

  Let $\lambda_1,\lambda_2,\cdots,\lambda_n$ be all the eigenvalues of
  $A$, hence they are the roots of $\det(A-\lambda I)$. So
  \begin{equation}
    \label{eq_ch4:factor_det}
    \det(A-\lambda I)=\coef(\lambda^n)\cdot 
    (\lambda-\lambda_1)(\lambda-\lambda_2)\cdots(\lambda-\lambda_n)
  \end{equation}
  Because $\coef(\lambda^n)=(-1)^n$, plugging $\lambda=0$ in equation
  \eqref{eq_ch4:decompose_det} we'd get
  \[
    \boxed{\det A=\lambda_1\lambda_2\cdots\lambda_n}
  \]
\end{proof}
% <==
% ==> ex11
\begin{exercise}
  Prove that trace of a matrix equals to the sum of its
  eigenvalue.
\end{exercise}
\begin{proof}
  From equation \eqref{eq_ch4:factor_det} in above exercise, 
  we have
  \[
    \det(A-\lambda I)
    =(-1)^n (\lambda-\lambda_1)(\lambda-\lambda_2)\cdots(\lambda-\lambda_n)
  \]
  has $\lambda_1,\dots,\lambda_n$ as root. So by Vietta theorem, 
  we obtain that the coefficient of $\lambda^{n-1}$ is 
  \[
    \coef(\lambda^{n-1})=-\frac{\lambda_1+\lambda_2+\cdots+\lambda_n}{(-1)^n}.
  \]
  Now, finding the coefficient of $\lambda^{n-1}$ in equation 
  \eqref{eq_ch4:decompose_det} is similar to find the coefficient of 
  $\lambda^{n-1}$ in the polynomial
  \[p(\lambda):=(a_{11}-\lambda)(a_{22}-\lambda)\cdots(a_{nn}-\lambda),\]
  and since this polynomial has roots $a_{11},a_{22},\dots,a_{nn}$, using
  Vietta theorem we'd get
  \[
    \coef{\lambda^{n-1}}=-\frac{a_{11}+a_{22}+\cdots+a_{nn}}{(-1)^n}.
  \]
  Therefore, we conclude that 
  \[
    \boxed{\tr(A)=\sum a_{jj}=\sum \lambda_j}
  \]
\end{proof}
% <==

\section{Diagonalization}
% ==> ex1
\begin{exercise}
  Let $A$ be $n\times n$ matrix. True or false.
  \begin{enumerate}[label={(\alph*)}]
    \item $A\tran$ has the same eigenvalues as $A$;
    \item $A\tran$ has the same eigenvectors as $A$;
    \item If $A$ is diagonalizable, the so is $A\tran$
  \end{enumerate}
\end{exercise}
\begin{proof}
  \begin{enumerate}[label={(\alph*)}]
    \item \textbf{true.} The characteristic polynomial of $A\tran$ is
      \begin{align*}
        \det(A\tran-\lambda I)
        =\det(A-\lambda I)\tran
        =\det(A-\lambda I)
      \end{align*}
      hene $A$ and $A\tran$ have the same eigenvalues.
    \item \textbf{false.} Not generally true. In fact, we can check this
      using the example in the text book (page 112). 
      However, if $A$ is a symmetric matrix ($A\tran=A$), then $A$ and 
      $A\tran$ have the same eigenvectors.
    \item \textbf{true.} 
      Since $A$ and $A\tran$ have the same characteristic polynomial, hence
      if $\lambda$ is a root, then
      $\textrm{alg}_A(\lambda)=\textrm{alg}_{A\tran}(\lambda)$, i.e. they have
      the same number of repeated roots. Because $A$ is diagonalizable, it's only
      the case that 
      \[\textrm{alg}_{A}(\lambda)=\textrm{geo}_{A}(\lambda)\] 
      now it only remains to show that 
      $\textrm{geo}_{A}(\lambda)=\textrm{geo}_{A\tran}(\lambda)$. Observe that
      \[ \dim\ker(A-\lambda I) =n-\rank(A-\lambda I) \]
      and
      \[ \dim\ker(A\tran-\lambda I) =n-\rank(A\tran -\lambda I) \] 
      note also that
      \[
        \rank(A\tran-\lambda I)=
        \rank(A-\lambda I)\tran=
        \rank(A-\lambda I)
      \]
      this shows that $\textrm{geo}_{A}(\lambda)=\textrm{geo}_{A\tran}(\lambda)$.
      Therefore, $A\tran$ is also diagonalizable.
  \end{enumerate}
\end{proof}
% <==
% ==> ex2
\begin{exercise}
  Let $A$ be a square matrix with real entries, and let $\lambda$
  be its complex eigenvalue. Suppose that $\vec{v}=(v_1,v_2,\dots,v_n)\tran$
  is a corresponding eigenvector. Prove that the $\overline{\lambda}$ is an
  eigenvalue of $A$ and that 
  $A\overline{\vec{v}}=\overline{\lambda}\overline{\vec{v}}$.
\end{exercise}
\begin{proof}
  Let us separate the real and imaginary part of $\vec{v}$ as
  $\vec{v}=\vec{a}+i\vec{b}$, where 
  $\vec{a}=(a_1,\dots,a_n)\tran$ and $\vec{b}=(b_1,\dots,b_n)\tran$
  are real-valued vectors in $\R^n$. Similarly, we also write $\lambda$ as
  $\lambda=\alpha+i\beta$ where $\alpha,\beta\in\R$.
  Because $A$ is real and $\lambda$
  is its eigenvalue, we obtain that
  \begin{align*}
    A\vec{v}
      &=\lambda\vec{v}\\\implies\quad
    A\vec{a}+iA\vec{b}
      &=(\alpha+i\beta)(\vec{a}+i\vec{b})\\
      &=(\alpha\vec{a}-\beta\vec{b})+i(\beta\vec{a}+\alpha\vec{b})
  \end{align*}
  Applying complex conjugate on both sides, we get
  \begin{align*}
    A\vec{a}-iA\vec{b}
    &=(\alpha\vec{a}-\beta\vec{b})-i(\beta\vec{a}+\alpha\vec{b})\\
    \implies\quad A\overline{\vec{v}}
    &=(\alpha-i\beta)(\vec{a}-i\vec{b})\\
    &=\overline{\lambda}\overline{\vec{v}}
  \end{align*}
\end{proof}
% <==
% <==
% ==> 5. inner product
\chapter{Inner Product}

\section{Inner product}
\section{Orthogonality}
\section{Orthogonal projection}
% ==> ex12
\setcounter{exercise}{11}
\begin{exercise}
  Show that for a subspace $E$ we have $(E^\perp)^\perp=E$.
\end{exercise}
\begin{proof}
  First note that for any $\vec{e}\in E$, $\vec{e}\perp E^\perp$, hence
  $E\perp E\perc$. To prove this assertion, we'll prove that each set is
  a subset of one another.
  \begin{itemize}
    \item Let $\vec{x}\in E$. From above reasoning we have 
      $\vec{x}\perp E\perc$, but this immediately implies that 
      $\vec{x}\in(E\perc)\perc$ because this set contains all of those 
      vectors that orthogonal to $E\perc$. Thus, 
      $E\subseteq (E\perc)\perc$.
    \item Now suppose that $\vec{y}\in (E\perc)\perc$. We can decompose 
      $\vec{y}$ as the sum
      \[\vec{y}=\proj{\vec{y}}{E}+\vec{y_0}\]
      where $\vec{y_0}\perp E$. Note the inner product
      $
        \ang{\vec{y_0,~\vec{y_0}}}=
        \ang{\vec{y},~\vec{y_0}}-\ang{\proj{\vec{y}}{E},~ \vec{y_0}}.
      $
      Since $\vec{y}\perp E\perc,~\vec{y_0}\in E\perc$ their inner product
      $\ang{\vec{y},~\vec{y_0}}=0$. Similarly since 
      $\proj{\vec{y}}{E}\in E,~ \vec{y_0}\perp E$, their inner product
      is also $0$. Hence we obtain that
      \[\ang{\vec{y_0},~\vec{y_0}}=0\iff \vec{y_0}=\vec{0}.\]
      This implies that $\vec{y}=\proj{\vec{y}}{E}\in E$ for all 
      $\vec{y}$. Hence $(E\perc)\perc\subseteq E$.
  \end{itemize}
\end{proof}
% <==
\newpage
\section{Least square}
% ==> ex5
\setcounter{exercise}{4}
\begin{exercise}[Minimal norm solution]
  Let an equation $A\vec{x}=\vec{b}$ has a solution, 
  and let $A$ has non-trivial kernel (so the solution is
  not unique.) Prove that
  \begin{enumerate}[label={(\alph*)}]
    \item There exist a unique solution $\vec{x_0}$ of $A\vec{x}=\vec{b}$
      minimizing the norm $\norm{\vec{x}}$, i.e. that there exists 
      unique $\vec{x_0}$ such that $A\vec{x_0}=\vec{b}$ and 
      $\norm{\vec{x_0}}\leq\norm{\vec{x}}$ for any $\vec{x}$
      satisfying $A\vec{x}=\vec{b}$.
    \item $\vec{x_0}=\proj{\vec{x}}{(\ker A)\perc}$ for any $\vec{x}$
      satisfying $A\vec{x}=\vec{b}$.
  \end{enumerate}
\end{exercise}
\begin{proof}
  \text{}
  \begin{enumerate}[label={(\alph*)}]
    \item 
      Let $V$ be the domain space of $A$, and let $\mathcal S\subset V$
      be the set of all solution of $A\vec{x}=\vec{b}$. 
      Let arbitrary $\vec{w}\in\mathcal S$, and denote 
      $\vec{x_0}\in(\ker A)\perc$ be its projection onto subspace 
      $(\ker A)\perc$. By decomposition, we obtain that there is 
      $\vec{x_h}\in ((\ker A)\perc)\perc=\ker A$, such that
      \begin{align*}
        \vec{w}&=\vec{x_0}+\vec{x_h} \\ \implies\quad 
        A\vec{x_0}&=A\vec{w}-A\vec{x_h}=\vec{b}-\vec{0}=\vec{b}
      \end{align*}
      So $\vec{x_0}\in(\ker A)\perc$ is also a solution of the above 
      equation ($\vec{x_0}$ is a more special root than our 
      regular $\vec{w}$). 

      \hspace{1em}
      In words, there exists a solution $\vec{x_0}$ such that 
      $\vec{x_0}\in(\ker A)\perc$. If $\vec{x_0}=\vec{0}$, 
      the above inequality is true. So from now on, we only 
      consider when $\vec{x_0}\neq \vec{0}$.
      Note that, for any general solution $\vec{x}\in\mathcal S$ 
      can be written as $\vec{x}=\vec{x_0}+\vec{e}$ for some vector 
      $\vec{e}\in\ker A$ (proved in chapter 2). Then
      \begin{align*}
        \norm{\vec{x_0}}^2
        &=\ang{\vec{x_0},\vec{x_0}}
        =\ang{\vec{x}-\vec{e},~ \vec{x_0}}\\
        &=\ang{\vec{x},\vec{x_0}}-\ang{\vec{e},\vec{x_0}}\\
        &=\ang{\vec{x},\vec{x_0}}
        &&\text{(because $\vec{x_0}\perp\vec{e}$)}\\
        &\leq \norm{\vec{x}}\cdot\norm{\vec{x_0}}
        &&\text{(using Cauchy--Schwarz)}
      \end{align*}
      divided both sides by $\norm{\vec{x_0}}\neq 0$, we get the desired 
      inequality $\norm{\vec{x_0}}\leq\norm{\vec{x}}$ for any 
      $\vec{x}\in\mathcal S$.
    \item Let $\vec{w_1}, \vec{w_2}\in\mathcal S$ be solutions of 
      $A\vec{x}=\vec{b}$. We wish to show that their projection 
      $P\vec{w_1}$ and $P\vec{w_2}$ onto $(\ker A)\perc$ are the same.
      Because $(\ker A)\perc$ is itself a subspace, then 
      \begin{equation}
        P\vec{w_1}-P\vec{w_2}\in(\ker A)\perc.
      \end{equation}
      Because $P\vec{w_1},P\vec{w_2}$ are projections, we obtain that 
      there exists $\vec{e_1},\vec{e_2}\in((\ker A)\perc)\perc=\ker A$,
      such that
      \[ \vec{w_1}=P\vec{w_1}+\vec{e_1} \] and
      \[ \vec{w_2}=P\vec{w_2}+\vec{e_2}. \]
      Substracting these two equations, and applying $A$ from both sides,
      we get
      \begin{align*}
        A(P\vec{w_1}-P\vec{w_2})
        &= A\vec{w_1}-A\vec{w_2}-A\vec{e_1}+A\vec{e_2}\\
        &= \vec{b}-\vec{b}-\vec{0}+\vec{0}=\vec{0}.
      \end{align*}
      This implies that 
      \begin{equation}
        P\vec{w_1}-P\vec{w_2}\in \ker A.
      \end{equation}
      But $\ker A \oplus(\ker A)\perc=V$, hence 
      $(\ker A)\cap(\ker A)\perc=\{\vec{0}\}$ therefore
      \[ P\vec{w_1}-P\vec{w_2}\in(\ker A)\cap(\ker A)\perc=\{0\} \]
      and thus $P\vec{w_1}=P\vec{w_2}$ as expected.
  \end{enumerate}
\end{proof}
% <==
% <==





\end{document}
