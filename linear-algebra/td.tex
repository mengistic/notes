
% ==> preamble
\input{../preamble.tex}

\renewcommand{\thesection}{\arabic{section}.}
\renewcommand{\theexercise}{\arabic{section}.\arabic{exercise}}
% <==
\def\by{\times}


\begin{document}
\chapter{Basic Notions}
% ==> Vector Spaces
\section{Vector Spaces}
% ==> ex1
\begin{exercise}
  Let $\vec{x}=(1,2,3)\tran, ~\vec{y}=(y_1,y_2,y_3)\tran$ and 
  $\vec{z}=(4,2,1)\tran$. Compute 
  $2\vec{x},~3\vec{y},~\vec{x}+2\vec{y}-3\vec{z}$.
\end{exercise}
\begin{proof}
  Little calculation reveals that
  \[
    2\vec{x}=
    \begin{pmatrix}
      2\\4\\6
    \end{pmatrix},\quad
    3\vec{y}=
    \begin{pmatrix}
      3y_1\\3y_2\\3y_3
    \end{pmatrix},\quad
    \vec{x}+2\vec{y}-3\vec{z}=
    \begin{pmatrix}
      2y_1 -11\\
      2y_2-4\\
      2y_3
    \end{pmatrix}
  \]
\end{proof}
% <==
% ==> ex2
\begin{exercise}
  Which of the following sets (with natural addition and multiplication
  by a scalar) are vector spaces? Justify your answers.
  \begin{enumerate}
    \item The set of all continuous functions on the interval $[0,1]$;
    \item The set of all non-negative functions on the interval $[0,1]$;
    \item The set of all polynomials of degree \emph{exactly} $n$;
    \item The set of all symmetric $n\times n$ matrices, i.e. 
      the set of matrices $A=\{a_{j,k}\}_{j,k=1}^{n}$ such 
      that $A\tran=A$.
  \end{enumerate}
\end{exercise}
\begin{proof}
  \text{}
  \begin{enumerate}
    \item Let $\mathcal{C}[0,1]$ be the set of all continuous functions
      on $[0,1]$. For any $f,g\in\mathcal{C}[0,1]$ and $\alpha\in\R$, we define
      \[
        (f+g)(x):=f(x)+g(x)
        \quad\text{and}\quad
        (\alpha f)(x):=\alpha\cdot f(x)
      \]
      for each $x\in[0,1]$. Therefore, $(\mathcal{C}[0,1], +,\cdot)$
      is closed under addition and scalar multiplication.
      It's immediate to see that all the eight properties of vector space
      are all satisfied, that is
      \begin{multicols}{2}
        \begin{itemize}
          \item $f+g=g+f$
          \item $f+(g+h)=(f+g)+h$
          %\item the function $0\in\mathcal{C}[0,1]$ such that 
          \item $f+0=f$
          \item $f+(-f)=0$
          %
          \item $1f=f$
          \item $\alpha(\beta f)=(\alpha\beta)f$
          \item $(\alpha+\beta)f=\alpha f+\beta f$
          \item $\alpha(f+g)=\alpha f+\beta g$
        \end{itemize}
      \end{multicols}
      Note that the function $0\in\mathcal{C}[0,1]$ such that
      $0(x)=0$ for each $x\in[0,1]$.
    \item Let $\mathcal{B}$ is the set of all non-negative functions on $[0,1]$.
      Then $(\mathcal{B},+\cdot)$ is not a vector space because it's not closed 
      under scalar multiplication, i.e. if $f\in\mathcal{B}$, hence $f>0$ yet
      \[-f=(-1)\cdot f<0.\]
      (Even if we restrict the scalar be to positive real numbers, this set 
      still won't be a vector space, because it fails to have an inverse.)
    \item Let $\mathcal{P}$ be the set of all polynomials of degree exactly $n$,
      then $(\mathcal{P}, +, \cdot)$ is \emph{not} a vector space, 
      because the addtive indentity is the polynomial 
      $0$. However, $0\notin\mathcal{P}$.
    \item Let $\sym(n)$ be the set of all symmetric $n\times n$ matrices. 
      The addition and scalar multiplication are defined as an
      \emph{entrywise} operations. Hence, $\sym(n)$ is closed under
      $(+)$ and $(\cdot)$. The additive indentity is the matrix 
      \[ \vec{0}=
        \begin{pmatrix}
          0&0&\cdots&0\\
          0&0&\cdots&0\\
          \vdots&&\ddots&\\
          0&0&\cdots&0
        \end{pmatrix}\in\sym(n).
      \]
      We could easily see that all the eight properties of vector spaces 
      are all satisfied.
  \end{enumerate}
\end{proof}
% <==
% ==> ex3
\begin{exercise}
  True or false:
  \begin{enumerate}
    \item Every vector space contains a zero vector;
      (\textbf{True.})
    \item A vector space can have more than one zero vector;
      (\textbf{False.} The zero vector is unique.)
    \item An $m\times n$ matrix has $m$ rows and $n$ columns;
      (\textbf{True.})
    \item If $f$ and $g$ are polynomials of degree $n$, then $f+g$
      is also a polynomial of degree $n$.
      (\textbf{False.} consider $t^n$ and $t-t^n$.)
    \item If $f$ and $g$ are polynomials of degree atmost $n$, the $f+g$
      is also a polynomial of degree atmost $n$.
      (\textbf{True.})
  \end{enumerate}
\end{exercise}
% <==
% ==> ex4
\begin{exercise}
  Prove that a zero vector $\vec{0}$ of a vector space $V$ is
  unique.
\end{exercise}
\begin{proof}
  Suppose that $\vec{a}$ and $\vec{b}$ are the zero vectors of $V$.
  From the \emph{Axioms of Vector Space}, we obtain that
  \begin{align*}
    \vec{a}
      &=\vec{a}+\vec{b} && \text{($\vec{b}$ is the zero vector)}\\
      &=\vec{b}+\vec{a} && \text{(commutitativity)}\\
      &=\vec{b}         && \text{($\vec{a}$ is the zero vector)}
  \end{align*}
  Hence, a zero vector of a vector space is unique,
  and we usually denote it by $\vec{0}$.
\end{proof}
% <==
% ==> ex5
\begin{exercise}
  What is the zero matrix of the space $M_{2\times 3}$?
\end{exercise}
\begin{proof}[Answer]
  In the space $M_{2\times 3}$, the zero matrix is 
  \[
    \vec{0}=
    \begin{pmatrix}
      0&0&0\\
      0&0&0
    \end{pmatrix}.
  \]
\end{proof}
% <==
% ==> ex6
\begin{exercise}
  Prove that the additive inverse of a vector space is unique.
\end{exercise}
\begin{proof}
  Let $\vec{a}$ be an arbitrary vector. Assume the $\vec{a}$ has 
  two inverses, namely $\vec{x}$ and $\vec{y}$. Hence
  \begin{align*}
    \vec{x}
      &=\vec{x}+\vec{0}   && \\
      &=\vec{x}+(\vec{a}+\vec{y}) && \text{($\vec{y}$ is an inverse)}\\
      &=(\vec{x}+\vec{a})+\vec{y} && \text{(associativity)}\\
      &=\vec{0}+\vec{y}           && \text{($\vec{x}$ is an inverse)}\\
      &=\vec{y}.
  \end{align*}
  Therefore, the inverse of any vector $\vec{a}\in V$ is unique, and 
  is usually denoted by $-\vec{a}$.
\end{proof}
% <==
% ==> ex7
\begin{exercise}
  Prove that $0\vec{v}=\vec{0}$ for any vector $\vec{v}\in V$.
\end{exercise}
\begin{proof}
  Let $\vec{v}\in V$ and  $\vec{b}$ is an inverse of $0\vec{v}$. Therefore, 
  \begin{align*}
    0
      &=0\vec{v}+b        \\
      &=(0+0)\vec{v}+b    \\
      &=(0\vec{v}+0\vec{v})+b && \text{(distributivity)}\\
      &=0\vec{v}+(0\vec{v}+b) && \text{(associativity)}\\
      &=0\vec{v}+\vec{0}      && \text{($\vec{b}$ is an inverse of $0\vec{v}$)}\\
      &=0\vec{v}
  \end{align*}
  for any $\vec{v}\in V$.
\end{proof}
% <==
% ==> ex8
\begin{exercise}
  Prove that for any vector $\vec{v}$ its additive inverse $-\vec{v}$
  is given by $(-1)\vec{v}$.
\end{exercise}
\begin{proof}
  As proved in the above exercise for any $\vec{v}\in V$, 
  \[
    \vec{0}=0\vec{v}=(1-1)\vec{v}=\vec{v}+(-1)\vec{v}
  \]
  where the last equallity derives from the distributive property.
  Because $-\vec{v}$ is the inverse of $\vec{v}$, then
  \begin{align*}
    -\vec{v}
      &=-\vec{v}+\vec{0}\\
      &=-\vec{v}+\big[\vec{v}+(-1)\vec{v}\big]\\
      &=(\underbrace{-\vec{v}+\vec{v}}_{\vec{0}})+(-1)\vec{v}\\
      &=(-1)\vec{v}
  \end{align*}
  as desired.
\end{proof}
% <==
% <==
% ==> Linear Combination
\section{Linear Combination, bases}
% ==> ex1
\begin{exercise}
  Find the basis in the space of $3\by 2$ matrices
  $M_{3\by 2}$.
\end{exercise}
\begin{proof}[Answer]
  The basis of $M_{3\by 2}$ has six vetors,
  \begin{align*}
    \vec{e_1}=\begin{bmatrix} 1&0\\0&0\\0&0 \end{bmatrix},\quad
    \vec{e_2}=\begin{bmatrix} 0&1\\0&0\\0&0 \end{bmatrix},\quad
    \vec{e_3}=\begin{bmatrix} 0&0\\1&0\\0&0 \end{bmatrix},\\[0.3cm]
    \vec{e_4}=\begin{bmatrix} 0&0\\0&1\\0&0 \end{bmatrix},\quad
    \vec{e_5}=\begin{bmatrix} 0&0\\0&0\\1&0 \end{bmatrix},\quad
    \vec{e_6}=\begin{bmatrix} 0&0\\0&0\\0&1 \end{bmatrix}.
  \end{align*}
\end{proof}
% <==
% ==> ex2
\begin{exercise}
  True or false:
  \begin{enumerate}[label={\alph*)}]
    \item Any set containing a zero vector is linearly dependent;
    \item A basis must contain $\vec{0}$;
    \item subsets of linearly dependent sets are linearly dependent;
    \item subsets of linearly independent sets are linearly independent;
    \item if $\alpha_1\vec{v}_1+\alpha_2\vec{v_2}+\cdots+\alpha_n\vec{v_n}=0$
      then all scalars $\alpha_k$ are zero.
  \end{enumerate}
\end{exercise}
\begin{proof}[Answer]
  \text{}
  \begin{enumerate}[label={\alph*)}]
    \item \textbf{True.} because $\vec{0}$ can be represented as a linear 
      combination of the other vectors (simply put all the scalars to $0$).
    \item \textbf{No.} if so, they must be linearly dependent, which is not a base.
    \item \textbf{No.} Take for example the system of linearly dependent
      $\{\vec{e_1},\vec{e_2},\vec{e_3}\}$ where 
      $\vec{e_1}=(1,0),~\vec{e_2}=(0,1)$ and $\vec{e_3}=(1,1)$.
      The subset $\{\vec{e_1},\vec{e_2}\}$ is a basis, which is 
      clearly not linearly dependent.
    \item \textbf{True.} Supppose that the system 
      $\{\vec{v_1},\dots,\vec{v_p}\}$
      is a subset of the linearly independent system 
      $\{\vec{v_1},\dots,\vec{v_p},\dots,\vec{v_n}\}$. Let $\alpha_k$ the 
      real numbers such that
      $\alpha_{1}\vec{v_1}+\cdots+\alpha_{p}\vec{v_p}=\vec{0}$
      hence
      \[
        \alpha_{1}\vec{v_1}+\cdots+\alpha_{p}\vec{v_p}+
        0\vec{v_{p+1}}+\cdots+0\vec{v_n}=\vec{0}.
      \]
      Because the system $\{\vec{v_1},\dots,\vec{v_p},\dots,\vec{v_n}\}$
      is linearly independent, therefore all the scalars $\alpha_k=0$. 
      Thus, the system $\{\vec{v_1},\dots,\vec{v_p}\}$ is also linearly
      independent.
    \item \textbf{No.} It's true only when $\{\vec{v_1},\dots,\vec{v_n}\}$
      are linearly independent.
  \end{enumerate}
\end{proof}
% <==
% ==> ex3
\begin{exercise}
  Recall, that a matrix is called \emph{symmetric} if 
  $A\tran=A$. Write down a basis in the space of \emph{symmetric}
  $2\by 2$ matrices (there are many possible answers). How many
  elements are there in the basis.
\end{exercise}
\begin{proof}[Answer]
  A basis of of this space is $\{\vec{e_1},\vec{e_2},\vec{e_3}\}$ where
  \[
    \vec{e_1}= \begin{bmatrix} 1&0\\0&0 \end{bmatrix},\quad
    \vec{e_2}= \begin{bmatrix} 0&1\\1&0 \end{bmatrix},\quad
    \vec{e_3}= \begin{bmatrix} 0&0\\0&1 \end{bmatrix}.
  \]
  There are 3 vectors in this basis.
\end{proof}
% <==
% ==> ex4
\begin{exercise}
  Write down a basis for the space of
  \begin{enumerate}[label={\alph*).}]
    \item $3\by 3$ symmetric matrices;
    \item $n\by n$ symmetric matrices;
    \item $n\by n$ antisymmetric matrices.
  \end{enumerate}
\end{exercise}
\begin{proof}[Answer]
  \text{}
  \begin{enumerate}[label={\alph*).}]
    \item the basis of $3\by 3$ matrices contains 6 vectors, namely
      \begin{align*}
        \vec{e_1}= \begin{bmatrix} 1&0&0 \\ 0&0&0 \\ 0&0&0 \end{bmatrix},\quad
        \vec{e_2}= \begin{bmatrix} 0&0&0 \\ 0&1&0 \\ 0&0&0 \end{bmatrix},\quad
        \vec{e_3}= \begin{bmatrix} 0&0&0 \\ 0&0&0 \\ 0&0&1 \end{bmatrix},\\[0.4cm]
        \vec{e_4}= \begin{bmatrix} 0&1&0 \\ 1&0&0 \\ 0&0&0 \end{bmatrix},\quad
        \vec{e_5}= \begin{bmatrix} 0&0&1 \\ 0&0&0 \\ 1&0&0 \end{bmatrix},\quad
        \vec{e_6}= \begin{bmatrix} 0&0&0 \\ 0&0&1 \\ 0&1&0 \end{bmatrix}.
      \end{align*}
    \item For the $n\by n$ symmetric matrices, we are going to prove that
      the basis of this space contains $\frac{n(n+1)}{2}$ vectors.
      It's easy to see that this is true for $n=2$ and $n=3$.
      Let's assume this is true for some integer $n$. Therefore, the 
      numbers of vectors in a basis of an $(n+1)\by (n+1)$ symmetric matrices is
  \end{enumerate}
\end{proof}
% <==
% ==> ex5
\begin{exercise}
  Let a system of vectors $\vec{v}_1,\vec{v}_2,\dots, \vec{v}_r$
  be linearly independent but not generating. Show that it is possible 
  to find a vector $\vec{v}_{r+1}$ such that the system 
  $\vec{v}_1,\vec{v}_2, \dots, \vec{v}_r,\vec{v}_{r+1}$ is linearly 
  independent.
\end{exercise}
\begin{proof}
  Because the system $\vec{v}_1,\vec{v}_2,\dots, \vec{v}_r$ is not generating, 
  therefore there exists a vector $\vec{v}_{r+1}$ such that $\vec{v}_{r+1}$ 
  cannot be represented as a linear combination of $\vec{v}_1,\vec{v}_2,\dots, \vec{v}_r$.
  Let $\alpha_i$ be the scalars such that 
  \begin{equation}
    \label{eq:2:r_plus_1}
    \alpha_1\vec{v}_1+\alpha_2\vec{v}_2+\cdots+\alpha_r\vec{v}_r+\alpha_{r+1}\vec{v}_{r+1}=\vec{0}
  \end{equation}
  Now we have to prove that all the scalars are all zero.
  If $\alpha_{r+1}\neq 0$ then 
  \[
    \vec{v}_{r+1}=-\sum_{i=1}^{r}\frac{\alpha_i}{\alpha_{r+1}}\cdot\vec{v}_{i},
  \]
  meaning $\vec{v}_{r+1}$ is the linear combination of the other vectors, 
  a contradiction. Hence $\alpha_{r+1}$ must equals to zero. So
  the $r+1$ term in the equation \eqref{eq:2:r_plus_1} vanishes. And 
  because the system $\vec{v}_1,\vec{v}_2,\dots, \vec{v}_r$ is linearly independent, 
  all the scalars $\alpha_i=0$ for all $i=0,1,\dots,r$. Thus, the system
  \[
    \vec{v}_1,\vec{v}_2,\dots, \vec{v}_r, \vec{v}_{r+1}
  \]
  is also \emph{linearly independent}.
\end{proof}
% <==
% ==> ex6
\begin{exercise}
  Is it possible that vectors $\vec{v_1}, \vec{v_2}, \vec{v_3}$
  are linearly dependent, but the vectors $\vec{w_1}=\vec{v_1}+\vec{v_2}$,
  $\vec{w_2}=\vec{v_2}+\vec{v_3}$ and $\vec{w_3}=\vec{v_3}+\vec{v_1}$
  are linearly \emph{independent}.
\end{exercise}
\begin{proof}
  It's not possible, and we're going to prove this assertion via contradiction.
  Assume that there are such vectors $\vec{v}_1,\vec{v}_2,\vec{v}_3$
  satisfying the above conditions. Then there are numbers $x,y,z\in\R$
  such that
  \[
    \abs{x}+\abs{y}+\abs{z}>0
    \quad\text{and}\quad
    x\vec{v}_1+y\vec{v}_2+z\vec{v}_3=\vec{0}.
  \]
  By letting 
  \[a=x+y-z,\quad b=y+z-x,\quad c=z+x-y\]
  we obtain that
  \begin{align*}
    a\vec{w}_1+b\vec{w}_2+c\vec{w}_3
    &=(x\vec{w}_1+y\vec{w}_1-z\vec{w}_1)
     +(y\vec{w}_2+z\vec{w}_2-x\vec{w}_2)\\
    &\qquad\qquad\qquad\qquad +(x\vec{w}_3+z\vec{w}_3-y\vec{w}_3)\\
    &=2x\vec{v}_1+2y\vec{v}_2+2z\vec{v}_3\\
    &=\vec{0}.
  \end{align*}
  Since $\{\vec{w}_1,\vec{w}_2,\vec{w}_3\}$
  are linearly independent, we must have $a=b=c=0$. Hence
  \[
    \begin{cases}
      x+y-z=0\\
      y+z-x=0\\
      z+x-y=0
    \end{cases}
  \]
  adding all the 3 eqations, $x+y+z=0$. Substituting back to the 
  system of eqations above we get
  \[x=y=z=0\]
  which contradicts to the fact that $\abs{x}+\abs{y}+\abs{z}>0$.
\end{proof}
% <==
% <==

\end{document}
