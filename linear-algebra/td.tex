
% ==> preamble
\input{../preamble.tex}

\renewcommand{\thesection}{\arabic{section}.}
\renewcommand{\theexercise}{\arabic{section}.\arabic{exercise}}
%\renewcommand{\thehomework}{\arabic{section}.\arabic{exercise}}
\def\by{\times}
% <==

%\setlrmarginsandblock{2cm}{5cm}{*}
%\setulmarginsandblock{2.5cm}{*}{1}
%\checkandfixthelayout
\def\theenumi{\alph{enumi})}


\begin{document}
\chapter{Basic Notions}
% ==> Vector Spaces
\section{Vector Spaces}
% ==> ex1
\begin{exercise}
  Let $\vec{x}=(1,2,3)\tran, ~\vec{y}=(y_1,y_2,y_3)\tran$ and 
  $\vec{z}=(4,2,1)\tran$. Compute 
  $2\vec{x},~3\vec{y},~\vec{x}+2\vec{y}-3\vec{z}$.
\end{exercise}
\begin{proof}
  Little calculation reveals that
  \[
    2\vec{x}=
    \begin{pmatrix}
      2\\4\\6
    \end{pmatrix},\quad
    3\vec{y}=
    \begin{pmatrix}
      3y_1\\3y_2\\3y_3
    \end{pmatrix},\quad
    \vec{x}+2\vec{y}-3\vec{z}=
    \begin{pmatrix}
      2y_1 -11\\
      2y_2-4\\
      2y_3
    \end{pmatrix}
  \]
\end{proof}
% <==
% ==> ex2
\begin{exercise}
  Which of the following sets (with natural addition and multiplication
  by a scalar) are vector spaces? Justify your answers.
  \begin{enumerate}
    \item The set of all continuous functions on the interval $[0,1]$;
    \item The set of all non-negative functions on the interval $[0,1]$;
    \item The set of all polynomials of degree \emph{exactly} $n$;
    \item The set of all symmetric $n\times n$ matrices, i.e. 
      the set of matrices $A=\{a_{j,k}\}_{j,k=1}^{n}$ such 
      that $A\tran=A$.
  \end{enumerate}
\end{exercise}
\begin{proof}
  \text{}
  \begin{enumerate}
    \item Let $\mathcal{C}[0,1]$ be the set of all continuous functions
      on $[0,1]$. For any $f,g\in\mathcal{C}[0,1]$ and $\alpha\in\R$, we define
      \[
        (f+g)(x):=f(x)+g(x)
        \quad\text{and}\quad
        (\alpha f)(x):=\alpha\cdot f(x)
      \]
      for each $x\in[0,1]$. Therefore, $(\mathcal{C}[0,1], +,\cdot)$
      is closed under addition and scalar multiplication.
      It's immediate to see that all the eight properties of vector space
      are all satisfied, that is
      \begin{multicols}{2}
        \begin{itemize}
          \item $f+g=g+f$
          \item $f+(g+h)=(f+g)+h$
          %\item the function $0\in\mathcal{C}[0,1]$ such that 
          \item $f+0=f$
          \item $f+(-f)=0$
          %
          \item $1f=f$
          \item $\alpha(\beta f)=(\alpha\beta)f$
          \item $(\alpha+\beta)f=\alpha f+\beta f$
          \item $\alpha(f+g)=\alpha f+\beta g$
        \end{itemize}
      \end{multicols}
      Note that the function $0\in\mathcal{C}[0,1]$ such that
      $0(x)=0$ for each $x\in[0,1]$.
    \item Let $\mathcal{B}$ is the set of all non-negative functions on $[0,1]$.
      Then $(\mathcal{B},+\cdot)$ is not a vector space because it's not closed 
      under scalar multiplication, i.e. if $f\in\mathcal{B}$, hence $f>0$ yet
      \[-f=(-1)\cdot f<0.\]
      (Even if we restrict the scalar be to positive real numbers, this set 
      still won't be a vector space, because it fails to have an inverse.)
    \item Let $\mathcal{P}$ be the set of all polynomials of degree exactly $n$,
      then $(\mathcal{P}, +, \cdot)$ is \emph{not} a vector space, 
      because the addtive indentity is the polynomial 
      $0$. However, $0\notin\mathcal{P}$.
    \item Let $\sym(n)$ be the set of all symmetric $n\times n$ matrices. 
      The addition and scalar multiplication are defined as an
      \emph{entrywise} operations. Hence, $\sym(n)$ is closed under
      $(+)$ and $(\cdot)$. The additive indentity is the matrix 
      \[ \vec{0}=
        \begin{pmatrix}
          0&0&\cdots&0\\
          0&0&\cdots&0\\
          \vdots&&\ddots&\\
          0&0&\cdots&0
        \end{pmatrix}\in\sym(n).
      \]
      We could easily see that all the eight properties of vector spaces 
      are all satisfied.
  \end{enumerate}
\end{proof}
% <==
% ==> ex3
\begin{exercise}
  True or false:
  \begin{enumerate}
    \item Every vector space contains a zero vector;
      (\textbf{True.})
    \item A vector space can have more than one zero vector;
      (\textbf{False.} The zero vector is unique.)
    \item An $m\times n$ matrix has $m$ rows and $n$ columns;
      (\textbf{True.})
    \item If $f$ and $g$ are polynomials of degree $n$, then $f+g$
      is also a polynomial of degree $n$.
      (\textbf{False.} consider $t^n$ and $t-t^n$.)
    \item If $f$ and $g$ are polynomials of degree atmost $n$, the $f+g$
      is also a polynomial of degree atmost $n$.
      (\textbf{True.})
  \end{enumerate}
\end{exercise}
% <==
% ==> ex4
\begin{exercise}
  Prove that a zero vector $\vec{0}$ of a vector space $V$ is
  unique.
\end{exercise}
\begin{proof}
  Suppose that $\vec{a}$ and $\vec{b}$ are the zero vectors of $V$.
  From the \emph{Axioms of Vector Space}, we obtain that
  \begin{align*}
    \vec{a}
      &=\vec{a}+\vec{b} && \text{($\vec{b}$ is the zero vector)}\\
      &=\vec{b}+\vec{a} && \text{(commutitativity)}\\
      &=\vec{b}         && \text{($\vec{a}$ is the zero vector)}
  \end{align*}
  Hence, a zero vector of a vector space is unique,
  and we usually denote it by $\vec{0}$.
\end{proof}
% <==
% ==> ex5
\begin{exercise}
  What is the zero matrix of the space $M_{2\times 3}$?
\end{exercise}
\begin{proof}[Answer]
  In the space $M_{2\times 3}$, the zero matrix is 
  \[
    \vec{0}=
    \begin{pmatrix}
      0&0&0\\
      0&0&0
    \end{pmatrix}.
  \]
\end{proof}
% <==
% ==> ex6
\begin{exercise}
  Prove that the additive inverse of a vector space is unique.
\end{exercise}
\begin{proof}
  Let $\vec{a}$ be an arbitrary vector. Assume the $\vec{a}$ has 
  two inverses, namely $\vec{x}$ and $\vec{y}$. Hence
  \begin{align*}
    \vec{x}
      &=\vec{x}+\vec{0}   && \\
      &=\vec{x}+(\vec{a}+\vec{y}) && \text{($\vec{y}$ is an inverse)}\\
      &=(\vec{x}+\vec{a})+\vec{y} && \text{(associativity)}\\
      &=\vec{0}+\vec{y}           && \text{($\vec{x}$ is an inverse)}\\
      &=\vec{y}.
  \end{align*}
  Therefore, the inverse of any vector $\vec{a}\in V$ is unique, and 
  is usually denoted by $-\vec{a}$.
\end{proof}
% <==
% ==> ex7
\begin{exercise}
  Prove that $0\vec{v}=\vec{0}$ for any vector $\vec{v}\in V$.
\end{exercise}
\begin{proof}
  Let $\vec{v}\in V$ and  $\vec{b}$ is an inverse of $0\vec{v}$. Therefore, 
  \begin{align*}
    0
      &=0\vec{v}+b        \\
      &=(0+0)\vec{v}+b    \\
      &=(0\vec{v}+0\vec{v})+b && \text{(distributivity)}\\
      &=0\vec{v}+(0\vec{v}+b) && \text{(associativity)}\\
      &=0\vec{v}+\vec{0}      && \text{($\vec{b}$ is an inverse of $0\vec{v}$)}\\
      &=0\vec{v}
  \end{align*}
  for any $\vec{v}\in V$.
\end{proof}
% <==
% ==> ex8
\begin{exercise}
  Prove that for any vector $\vec{v}$ its additive inverse $-\vec{v}$
  is given by $(-1)\vec{v}$.
\end{exercise}
\begin{proof}
  As proved in the above exercise for any $\vec{v}\in V$, 
  \[
    \vec{0}=0\vec{v}=(1-1)\vec{v}=\vec{v}+(-1)\vec{v}
  \]
  where the last equallity derives from the distributive property.
  Because $-\vec{v}$ is the inverse of $\vec{v}$, then
  \begin{align*}
    -\vec{v}
      &=-\vec{v}+\vec{0}\\
      &=-\vec{v}+\big[\vec{v}+(-1)\vec{v}\big]\\
      &=(\underbrace{-\vec{v}+\vec{v}}_{\vec{0}})+(-1)\vec{v}\\
      &=(-1)\vec{v}
  \end{align*}
  as desired.
\end{proof}
% <==
% <==
% ==> Linear Combination
\section{Linear Combination, bases}
% ==> ex1
\begin{exercise}
  Find the basis in the space of $3\by 2$ matrices
  $M_{3\by 2}$.
\end{exercise}
\begin{proof}[Answer]
  Consider the vectors:
  \begin{align*}
    \vec{e_1}=\begin{bmatrix} 1&0\\0&0\\0&0 \end{bmatrix},\quad
    \vec{e_2}=\begin{bmatrix} 0&1\\0&0\\0&0 \end{bmatrix},\quad
    \vec{e_3}=\begin{bmatrix} 0&0\\1&0\\0&0 \end{bmatrix},\\[0.3cm]
    \vec{e_4}=\begin{bmatrix} 0&0\\0&1\\0&0 \end{bmatrix},\quad
    \vec{e_5}=\begin{bmatrix} 0&0\\0&0\\1&0 \end{bmatrix},\quad
    \vec{e_6}=\begin{bmatrix} 0&0\\0&0\\0&1 \end{bmatrix}
  \end{align*}
  and we're going to prove that the system of thses vectors are a basis.
  Any  matrix 
  \[
    \vec{v}=
    \begin{bmatrix}
      a&b\\ c&d\\ e&f
    \end{bmatrix}\in M_{3\by 2}
  \]
  can be represented as the combination
  $\vec{v}=a\vec{e_1}+b\vec{e_2}+c\vec{e_3}+d\vec{e_4}+e\vec{e_5}+f\vec{e_6}$
  thus this system is generating. Next we're going to prove the uniqueness.

  Suppose that there are $\hat{a},\hat{b},\dots,\hat{f}$ with
  \begin{align*}
    \vec{v}&=\hat{a}\vec{e_1}+\hat{b}\vec{e_2}+\hat{c}\vec{e_3}
              +\hat{d}\vec{e_4}+\hat{e}\vec{e_5}+\hat{f}\vec{e_6}\\
    \implies\quad 
    \begin{bmatrix} 
      a&b\\c&d\\e&f 
    \end{bmatrix}
    &=
    \begin{bmatrix} 
      \hat{a} & \hat{b}\\ \hat{c}&\hat{d} \\ \hat{e}&\hat{f}
    \end{bmatrix}
  \end{align*}
  This implies that each corresponding entry is equals. Hence the representation
  is unique. Therefore this system is a basis.




\end{proof}
% <==
% ==> ex2
\begin{exercise}
  True or false:
  \begin{enumerate}
    \item Any set containing a zero vector is linearly dependent;
    \item A basis must contain $\vec{0}$;
    \item subsets of linearly dependent sets are linearly dependent;
    \item subsets of linearly independent sets are linearly independent;
    \item if $\alpha_1\vec{v}_1+\alpha_2\vec{v_2}+\cdots+\alpha_n\vec{v_n}=0$
      then all scalars $\alpha_k$ are zero.
  \end{enumerate}
\end{exercise}
\begin{proof}[Answer]
  \text{}
  \begin{enumerate}
    \item \textbf{True.} because $\vec{0}$ can be represented as a linear 
      combination of the other vectors (simply put all the scalars to $0$).
    \item \textbf{No.} if so, they must be linearly dependent, which is not a base.
    \item \textbf{No.} Take for example the system of linearly dependent
      $\{\vec{e_1},\vec{e_2},\vec{e_3}\}$ where 
      $\vec{e_1}=(1,0),~\vec{e_2}=(0,1)$ and $\vec{e_3}=(1,1)$.
      The subset $\{\vec{e_1},\vec{e_2}\}$ is a basis, which is 
      clearly not linearly dependent.
    \item \textbf{True.} Supppose that the system 
      $\{\vec{v_1},\dots,\vec{v_p}\}$
      is a subset of the linearly independent system 
      $\{\vec{v_1},\dots,\vec{v_p},\dots,\vec{v_n}\}$. Let $\alpha_k$ the 
      real numbers such that
      $\alpha_{1}\vec{v_1}+\cdots+\alpha_{p}\vec{v_p}=\vec{0}$
      hence
      \[
        \alpha_{1}\vec{v_1}+\cdots+\alpha_{p}\vec{v_p}+
        0\vec{v_{p+1}}+\cdots+0\vec{v_n}=\vec{0}.
      \]
      Because the system $\{\vec{v_1},\dots,\vec{v_p},\dots,\vec{v_n}\}$
      is linearly independent, therefore all the scalars $\alpha_k=0$. 
      Thus, the system $\{\vec{v_1},\dots,\vec{v_p}\}$ is also linearly
      independent.
    \item \textbf{No.} Take, $\vec{e_1}=(2,2)$ and $\vec{e_2}=(1,1)$ for instance.
      We have $\vec{e_1}-2\vec{e_2}=\vec{0}$ yet the scalars are non-zero.
  \end{enumerate}
\end{proof}
% <==
% ==> ex3
\begin{exercise}
  Recall, that a matrix is called \emph{symmetric} if 
  $A\tran=A$. Write down a basis in the space of \emph{symmetric}
  $2\by 2$ matrices (there are many possible answers). How many
  elements are there in the basis.
\end{exercise}
\begin{proof}[Answer]
  We are going to prove that the system $\{\vec{d_1},\vec{d_2},\vec{e_1}\}$ where
  \[
    \vec{d_1}= \begin{bmatrix} 1&0\\0&0 \end{bmatrix},\quad
    \vec{d_2}= \begin{bmatrix} 0&0\\0&1 \end{bmatrix},\quad
    \vec{e_1}= \begin{bmatrix} 0&1\\1&0 \end{bmatrix},\quad
  \]
  is a basis. Observe that any symmetric matrix 
  \[
    \vec{v}=
    \begin{bmatrix}
      d_1 & e_1\\
      e_1 & d_2
    \end{bmatrix}
  \]
  can be represented as $\vec{v}=d_1\vec{d_1}+d_2\vec{d_2}+e_1\vec{e_1}$,
  hence it's generating. Note that the equation 
  \begin{align*}
    &d_1\vec{d_1}+d_2\vec{d_2}+e_1\vec{e_1}=\vec{0}\\
    &\begin{bmatrix}d_1 & e_1\\  e_1 &d_2\end{bmatrix}=\begin{bmatrix}0&0\\0&0\end{bmatrix}
  \end{align*}
  holds only when all the scalars are all zero. Hence the system is linearly 
  independent. Thus, it's a basis.

\end{proof}
% <==
% ==> ex4
\begin{exercise}
  Write down a basis for the space of
  \begin{enumerate}
    \item $3\by 3$ symmetric matrices;
    \item $n\by n$ symmetric matrices;
    \item $n\by n$ antisymmetric matrices.
  \end{enumerate}
\end{exercise}
\begin{proof}[Answer]
  \text{}
  \begin{enumerate}
    \item we are going to prove that the system of vectors
      \begin{align*}
        \vec{d_1}= \begin{bmatrix} 1&0&0 \\ 0&0&0 \\ 0&0&0 \end{bmatrix},\quad
        \vec{d_2}= \begin{bmatrix} 0&0&0 \\ 0&1&0 \\ 0&0&0 \end{bmatrix},\quad
        \vec{d_3}= \begin{bmatrix} 0&0&0 \\ 0&0&0 \\ 0&0&1 \end{bmatrix},\\[0.4cm]
        \vec{e_1}= \begin{bmatrix} 0&1&0 \\ 1&0&0 \\ 0&0&0 \end{bmatrix},\quad
        \vec{e_2}= \begin{bmatrix} 0&0&1 \\ 0&0&0 \\ 1&0&0 \end{bmatrix},\quad
        \vec{e_3}= \begin{bmatrix} 0&0&0 \\ 0&0&1 \\ 0&1&0 \end{bmatrix}.
      \end{align*}
      is the basis. First of, any symmetric matrix 
      \begin{align*}
        \vec{v}
        &=\begin{bmatrix}
          d_1 & e_1 & e_2 \\
          e_1 & d_2 & e_3 \\
          e_2 & e_3 & d_3
        \end{bmatrix}
      \intertext{can be represented as}
        \vec{v}&=d_1\vec{d_1}+d_2\vec{d_2}+d_3\vec{d_3}
          +e_1\vec{e_1}+e_2\vec{e_2}+e_3\vec{e_3}
      \end{align*}
      yeilds that the system is generating. Similar to the previous problem, 
      if the linear combination of these vectors equals $\vec{0}$, then all 
      the scalars must equals zero. Thus it's linearly independent. 
      Therefore it's a basis.
    \item Working on it.
    \item Working on it.
  \end{enumerate}
\end{proof}
% <==
% ==> ex5
\begin{exercise}
  Let a system of vectors $\vec{v}_1,\vec{v}_2,\dots, \vec{v}_r$
  be linearly independent but not generating. Show that it is possible 
  to find a vector $\vec{v}_{r+1}$ such that the system 
  $\vec{v}_1,\vec{v}_2, \dots, \vec{v}_r,\vec{v}_{r+1}$ is linearly 
  independent.
\end{exercise}
\begin{proof}
  Because the system $\vec{v}_1,\vec{v}_2,\dots, \vec{v}_r$ is not generating, 
  therefore there exists a vector $\vec{v}_{r+1}$ such that $\vec{v}_{r+1}$ 
  cannot be represented as a linear combination of $\vec{v}_1,\vec{v}_2,\dots, \vec{v}_r$.
  Let $\alpha_i$ be the scalars such that 
  \begin{equation}
    \label{eq:2:r_plus_1}
    \alpha_1\vec{v}_1+\alpha_2\vec{v}_2+\cdots+\alpha_r\vec{v}_r+\alpha_{r+1}\vec{v}_{r+1}=\vec{0}
  \end{equation}
  Now we have to prove that all the scalars are all zero.
  If $\alpha_{r+1}\neq 0$ then 
  \[
    \vec{v}_{r+1}=-\sum_{i=1}^{r}\frac{\alpha_i}{\alpha_{r+1}}\cdot\vec{v}_{i},
  \]
  meaning $\vec{v}_{r+1}$ is the linear combination of the other vectors, 
  a contradiction. Hence $\alpha_{r+1}$ must equals to zero. So
  the $r+1$ term in the equation \eqref{eq:2:r_plus_1} vanishes. And 
  because the system $\vec{v}_1,\vec{v}_2,\dots, \vec{v}_r$ is linearly independent, 
  all the scalars $\alpha_i=0$ for all $i=0,1,\dots,r$. Thus, the system
  \[
    \vec{v}_1,\vec{v}_2,\dots, \vec{v}_r, \vec{v}_{r+1}
  \]
  is also \emph{linearly independent}.
\end{proof}
% <==
% ==> ex6
\begin{exercise}
  Is it possible that vectors $\vec{v_1}, \vec{v_2}, \vec{v_3}$
  are linearly dependent, but the vectors $\vec{w_1}=\vec{v_1}+\vec{v_2}$,
  $\vec{w_2}=\vec{v_2}+\vec{v_3}$ and $\vec{w_3}=\vec{v_3}+\vec{v_1}$
  are linearly \emph{independent}.
\end{exercise}
\begin{proof}
  It's not possible, and we're going to prove this assertion via contradiction.
  Assume that there are such vectors $\vec{v}_1,\vec{v}_2,\vec{v}_3$
  satisfying the above conditions. Then there are numbers $x,y,z\in\R$
  such that
  \[
    \abs{x}+\abs{y}+\abs{z}>0
    \quad\text{and}\quad
    x\vec{v}_1+y\vec{v}_2+z\vec{v}_3=\vec{0}.
  \]
  By letting 
  \[a=x+y-z,\quad b=y+z-x,\quad c=z+x-y\]
  we obtain that
  \begin{align*}
    a\vec{w}_1+b\vec{w}_2+c\vec{w}_3
    &=(x\vec{w}_1+y\vec{w}_1-z\vec{w}_1)
     +(y\vec{w}_2+z\vec{w}_2-x\vec{w}_2)\\
    &\qquad\qquad\qquad\qquad +(x\vec{w}_3+z\vec{w}_3-y\vec{w}_3)\\
    &=2x\vec{v}_1+2y\vec{v}_2+2z\vec{v}_3\\
    &=\vec{0}.
  \end{align*}
  Since $\{\vec{w}_1,\vec{w}_2,\vec{w}_3\}$
  are linearly independent, we must have $a=b=c=0$. Hence
  \[
    \begin{cases}
      x+y-z=0\\
      y+z-x=0\\
      z+x-y=0
    \end{cases}
  \]
  adding all the 3 eqations, $x+y+z=0$. Substituting back to the 
  system of eqations above we get
  \[x=y=z=0\]
  which contradicts to the fact that $\abs{x}+\abs{y}+\abs{z}>0$.
\end{proof}
% <==
% ==> ex7
\begin{exercise}
  Any finite independent system is a subset of some basis.
\end{exercise}
\begin{proof}
  Let $\{\vec{v}_1,\vec{v}_2,\dots,\vec{v}_n\}$ is linearly independent.
  If this system is generating, then it's a base and we're done. If not,
  from exercise 2.5, there exists $\vec{v}_{n+1}$ such that
  \[\{\vec{v}_1,\vec{v}_2,\dots,\vec{v}_n,\vec{v}_{n+1}\}\]
  is still linearly independent. Now if this new system is generating, then 
  we're done. If not, we keep continue this process a finite steps, 
  adding vectors $\vec{v}_{n+1},\vec{v}_{n+2},\dots,\vec{v}_{n+r}$, and 
  eventually the new system
  \[\{\vec{v}_1,\dots,\vec{v}_{n},\vec{v}_{n+1},\dots,\vec{v}_{n+r}\}\]
  is now a basis.
\end{proof}
% <==
% <==
% ==> linear transformation
\section{Linear Transformation}
% ==> hw1
\begin{homework}
  Prove that the transformation $T:\F^n\to\F^m$ if and only if 
  $T(\alpha\vec{x}+\beta\vec{y})=\alpha T(\vec{x})+\beta T(\vec{y})$
  for any scalars $\alpha,\beta$ and vectors 
  $\vec{x},\vec{y}\in\F$.
\end{homework}
\begin{proof}
  We need to prove this in two directions.
  \begin{itemize}
    \item[($\Rightarrow$)] Suppose $T$ is a linear transformation, 
      then 
      %$T(\vec{x}+\vec{y})=T(\vec{x})+T(\vec{y})$ and 
      %$T(\alpha\vec{x})=\alpha T(\vec{\alpha})$. Thus
      \[
        T(\alpha\vec{x}+\beta\vec{y})
        =T(\alpha\vec{x})+T(\beta\vec{y})
        =\alpha T(\vec{x})+\beta T(\vec{y})
      \]
      as needed.
    \item[$(\Leftarrow)$] For this direction, we first assume that
      $T$ has the property that 
      $T(\alpha\vec{x}+\beta\vec{y}) =\alpha T(\vec{x})+\beta T(\vec{y})$
      for all $\alpha,\beta,\vec{x},\vec{y}$. We need to show that
      $T$ has the property listed in the definition of the linear 
      transformation. Observe that
      \begin{itemize}
        \item take $\alpha=\beta=1$ then,
          $T(\vec{x}+\vec{y})=T(\vec{x})+T(\vec{y})$
        \item take $\beta=0$ then,
          $T(\alpha\vec{x})=\alpha T(\vec{x})$
      \end{itemize}
      Hence $T$ is a linear transformation, 
      and the proof is completed.
  \end{itemize}
\end{proof}
% <==
% ==> hw2
\begin{homework}
  Let $T:V\to W$ be a linear transformation. Prove that
  $T(\vec{0})=\vec{0}$ and 
  \[TV=\{T\vec{v}~:~\vec{v}\in V\}\]
  is a vector space.
\end{homework}
\begin{proof}
  Since $T$ is linear, and as proved before $0\cdot\vec{0}=\vec{0}$, 
  it's easy to see that 
  $$T(\vec{0})=T(0\cdot\vec{0})=0\cdot T(\vec{0})=\vec{0}.$$
  To prove that $TV$ is a vector space, we need to check that $TV$ satisfies
  all the eight conditions listed in the definition of vector space.

  We first need to prove that $TV$ is closed.
  Because $TV\subset W$, hence $TV$ is closed under scalar multiplication and
  vector addition. Let $\vec{x},\vec{y},\vec{z}\in V$. Observe that
  \begin{itemize}
    \item $T\vec{x}+T\vec{y}=T\vec{y}+T\vec{x}$
      \quad (commutitativity of $W$)
    \item $(T\vec{x}+T\vec{y})+T\vec{z}=T\vec{x}+(T\vec{y}+T\vec{z})$
      \quad (associativity of $W$)
    \item The vector $\vec{0}\in W$ is the indentity of $TV$ because 
      \[
        T\vec{x}+\vec{0}=T\vec{x}+T\vec{0}=T(\vec{x}+\vec{0})=T(\vec{x}),\quad 
        \forall \vec{x}\in V
      \]
    \item The vector $T(-\vec{x})$ is the additive inverse of $T\vec{x}$ because
      \[T\vec{x}+T(-\vec{x})=T(\vec{x}-\vec{x})=\vec{0}\]
    \item $1\cdot T\vec{v}=T\vec{v}$
      \quad (multiplicative iden. in $W$)
  \end{itemize}
  Let $\alpha, \beta$ be scalars.
  \begin{itemize}
    \item multiplicative associativity
      \begin{align*}
        (\alpha\beta)T\vec{x} 
        &= T((\alpha\beta)\vec{x})
        && (\text{linearity of $T$})\\
        &= T(\alpha (\beta\vec{x})) 
        && (\text{mult. asso. of $V$})\\
        &= \alpha T(\beta\vec{x})
        && (\text{linearity of $T$})\\
        &= \alpha\cdot\beta T\vec{x}
      \end{align*}
    \item scalar multiplication
      \begin{align*}
        \alpha(T\vec{x}+T\vec{y})
        &=\alpha T(\vec{x}+\vec{y}) && (\text{linearity of $T$})\\
        &=T(\alpha(\vec{x}+\vec{y})) \\
        &=T(\alpha\vec{x}+\alpha\vec{y}) && (\text{scalar mult. in $V$})\\
        &=T(\alpha\vec{x})+T(\alpha\vec{x}) && (\text{linearity of $T$})\\
        &=\alpha T\vec{x}+\alpha T\vec{y}
      \end{align*}
    \item scalar multiplication
      \begin{align*}
        (\alpha+\beta)T\vec{x}
        &=T((\alpha+\beta)\vec{x}) && (\text{linearity of $T$})\\
        &=T(\alpha\vec{x}+\beta\vec{x}) && (\text{scalar mult. of $V$})\\
        &=T(\alpha\vec{x})+T(\beta\vec{x}) \\
        &=\alpha T\vec{x}+\beta T\vec{x}
      \end{align*}
  \end{itemize}
  We see that $TV$ has all eight properties to be a vector space, and the proof
  is completed.


\end{proof}
% <==
% ==> hw3
\begin{homework}
  Let $V,W$ be vector spaces. Prove that $\mathcal{L}(V,W)$, the set of all 
  linear transformations $T:V\to W$, is also a vector space.
\end{homework}
\begin{proof}
  We first need to show that $\mathcal{L}(V,W)$ is closed.
  Let $T_1, T_2\in\mathcal{L}(V,W)$ and $a$ be a scalar.
  So we need to show the transformation $T_1+T_2$ and $a T_1$ 
  are both linear. 
  \begin{itemize}
    \item Let $\vec{x}, \vec{y}$ be arbitrary vectors in $V$ 
      and $\alpha,\beta$ be scalar. 
      Denote $T:=T_1+T_2$. Observe that 
      \begin{align*}
        T(\alpha\vec{x}+\beta\vec{y})
        =&(T_1+T_2)(\alpha\vec{x}+\beta\vec{y})\\
        &=T_1(\alpha\vec{x}+\beta\vec{y})+T_2(\alpha\vec{x}+\beta\vec{y}) && (\text{by def. of $T_1+T_2$})\\
        &=\alpha T_1\vec{x}+\beta T_1\vec{y}+\alpha T_2\vec{x}+\beta T_2\vec{y} && (\text{by lin. of $T_1$ and $T_2$})\\
        &=(\alpha T_1\vec{x}+\alpha T_2\vec{x}) + (\beta T_1\vec{y}+\beta T_2\vec{y})\\
        &=\alpha (T_1\vec{x}+T_2\vec{x}) + \beta (T_1\vec{y}+T_2\vec{y}) && (\text{by scalar mult. in $W$})\\
        &=\alpha (T_1+T_2)\vec{x}+\beta (T_1+T_2)\vec{y}\\
        &=\alpha T\vec{x}+\beta T\vec{y}
      \end{align*}
      This shows that $T_1+T_2$ is also a linear transformation, hence 
      $\mathcal{L}(V,W)$ is closed under addition.
    \item Similarly, we let $\vec{x},\vec{y}\in V$. For simplicity, we again 
      denote $T:=a T_1$. Hence for any scalars $\alpha, \beta$
      \begin{align*}
        T(\alpha\vec{x}+\beta\vec{y})
        &=(aT_1)(\alpha\vec{x}+\beta\vec{y})\\
        &=a\cdot T_1(\alpha\vec{x}+\beta\vec{y}) && (\text{by def. of $aT_1$})\\
        &=a\cdot (\alpha T_1\vec{x}+\beta T_1\vec{y}) && (\text{by lin. of $T_1$})\\
        &=\alpha aT_1\vec{x}+\beta aT_1\vec{y}\\
        &=\alpha (aT_1)\vec{x}+\beta (aT_1){\vec{y}}\\
        &=\alpha T\vec{x}+\beta T\vec{y}
      \end{align*}
      This suggests that $aT_1$ is also linear, hence $\mathcal{L}(V,W)$
      is closed under scalar multiplication.
      Ultimately, we've proved that $\mathcal{L}(V,W)$ is closed as needed.
  \end{itemize}
  We are now ready to prove that $\mathcal{L}(V,W)$ is a vector space.
  Let $T_1,T_2,T_3\in\mathcal{L}(V,W)$ we have
  \begin{itemize}
    \item $T_1+T_2=T_2+T_1$, because for any $\vec{x}\in V$
      \[ (T_1+T_2)\vec{x}=T_1\vec{x}+T_2\vec{x}=T_2\vec{x}+T_1\vec{x}=(T_2+T_1)\vec{x}. \]
    \item $T_1+(T_2+T_3)=(T_1+T_2)+T_3$, because for any $\vec{x}\in V$
      \begin{align*}
        (T_1+(T_2+T_3))\vec{x}
        &=T_1\vec{x}+(T_2+T_3)\vec{x}\\
        &=T_1\vec{x}+(T_2\vec{x}+T_3\vec{x})\\
        &=(T_1\vec{x}+T_2\vec{x})+T_3\vec{x} && (\text{by asso. of $W$})\\
        &=(T_1+T_2)\vec{x}+T_3\vec{x}\\
        &=((T_1+T_2)+T_3)\vec{x}
      \end{align*}
    \item Consider the transformation $0:V\to W$ such that
      $0(\vec{v})=\vec{0}$ for all $\vec{v}\in V$. We're going to prove that
      this $0$ is the indentity of $\mathcal{L}(V,W)$. But first, we need to 
      know if $0$ is  linear or not. For any $\vec{v_1},\vec{v_2}\in V$, we have
      \[
        0(\alpha\vec{v_1}+\beta\vec{v_2})=\vec{0}
        \quad\text{and}\quad
        \alpha 0\vec{v_1}+\beta 0\vec{v_2}=\alpha\vec{0}+\beta\vec{0}=\vec{0}.
      \]
      Hence $0(\alpha\vec{v_1}+\beta\vec{v_2})=\alpha 0\vec{v_1}+\beta 0\vec{v_2}$,
      thus the transformation $0$ is linear, i.e. $0\in\mathcal{L}(V,W)$.

      Observe that for any $\vec{x}\in V$
      \[(T_1+0)\vec{x}=T_1\vec{x}+0\vec{x}=T_1\vec{x}\]
      This implies that $T_1+0=T_1$ for any $T_1\in\mathcal{L}(V,W)$.
      We conclude that $0$ is the indentity of $\mathcal{L}(V,W)$.
    \item The transformation $-T_1:=(-1)T_1$ is the additive inverse of $T_1$
      because for any $\vec{x}\in V$
      \[T_1\vec{x}+(-T_1\vec{x})=T_1\vec{x}+T_1(-\vec{x})=T_1(\vec{x}-\vec{x})=\vec{0}=0(\vec{x}).\]
    \item $1\cdot T_1=T_1$ because  $(1\cdot T_1)\vec{x}=1\cdot T_1\vec{x}=T_1\vec{x}$
      for any $\vec{x}\in V$.
    \item $(\alpha\beta)T_1=\alpha (\beta T_1)$, because 
      \[ 
        [(\alpha \beta)T_1]\vec{x}=(\alpha\beta)T_1\vec{x}=T_1(\alpha\beta\vec{x})
        =\alpha T_1(\beta\vec{x})=\alpha (\beta T_1)\vec{x}
      \]
    \item $\alpha (T_1+T_2)=\alpha T_1+\alpha T_2$ because
      \[
        [\alpha(T_1+T_2)](\vec{x})=\alpha T_1\vec{x}+\alpha T_2\vec{x}=(\alpha T_1+\alpha T_2)(\vec{x})
      \]
  \end{itemize}
\end{proof}
% <==

% ==> ex1
\begin{exercise}
  Multiply
  \begin{enumerate}
    \item 
      $
      \begin{pmatrix} 1&2&3\\4&54&6 \end{pmatrix}
      \begin{pmatrix} 1\\3\\2 \end{pmatrix} =
      \begin{pmatrix} 1+6+6\\ 4+15+12 \end{pmatrix}=
      \begin{pmatrix} 13\\31 \end{pmatrix}
      $
    \item 
      $
      \begin{pmatrix} 1&2\\0&1\\2&0 \end{pmatrix}
      \begin{pmatrix} 1\\3 \end{pmatrix}=
      \begin{pmatrix} 1+6\\ 0+3\\ 2+0 \end{pmatrix}=
      \begin{pmatrix} 7\\3\\2 \end{pmatrix}
      $
    \item 
      $
      \begin{pmatrix} 1&2&0&0\\ 0&1&2&0\\ 0&0&1&2\\ 0&0&0&1 \end{pmatrix}
      \begin{pmatrix} 1\\2\\3\\4 \end{pmatrix}=
      \begin{pmatrix} 1+4+0+0\\ 0+2+6+0\\ 0+0+3+8\\ 0+0+0+4 \end{pmatrix}=
      \begin{pmatrix} 5\\8\\11\\4 \end{pmatrix}
      $
    \item 
      $ \begin{pmatrix} 1&2&0\\0&1&2\\ 0&0&1\\0&0&0 \end{pmatrix}
      \begin{pmatrix} 1\\2\\3\\4 \end{pmatrix} $\\
      can't be multiplied because the number of columns of the first matrix
      doesn't equal to the number of rows of the second matrix.
  \end{enumerate}
\end{exercise}
% <==
% ==> ex2
\begin{exercise}
  Let a linear transformation in $\R^2$ be the reflection in the line 
  $x_1=x_2$. Find its matrix.
\end{exercise}
\begin{proof}[Solution]
  Let $T:\R^2\to\R^2$ be this transformation. The basis of the domain is
  $\{\vec{e_1}, \vec{e_2}\}$ where 
  $\vec{e_1}=(1,0)\tran$ and $\vec{e_2}=(0,1)\tran$. Because $T$ reflect
  the line $x_1=x_2$ then 
  \[
    T\vec{e_1}= \begin{pmatrix} 0\\1 \end{pmatrix}
    \quad\text{and}\quad
    T\vec{e_2}= \begin{pmatrix} 1\\0 \end{pmatrix}.
  \]
  Therefore, the matrix of this transformation is
  $[T]= \begin{bmatrix} 0&1\\1&0 \end{bmatrix}$.

\end{proof}
% <==
% ==> ex3
\begin{exercise}
  For each linear transformation below, find its matrix
  \begin{enumerate}
    \item $T:\R^2\to\R^3$ defined by $T(x,y)\tran=(x+2y,2x-5y,7y)\tran$
    \item $T:\R^4\to\R^3$ defined by 
      $T(x_1,x_2,x_3,x_4)\tran=(x_1+x_2+x_3+x_4, x_2-x_4, x_1+3x_2+6x_4)\tran$
    \item $T:\P_n\to\P_n$ st $Tf(t)=f'(t)$ 
      (find the matrix with respect to the standard basis 
      $1,t,t^2,\dots, t^n$)
    \item $T:\P_n\to\P_n$ st $Tf(t)=2f(t)+3f'(t)-4f''(t)$.
  \end{enumerate}
\end{exercise}
\begin{proof}
  Find the matrix.
  \begin{enumerate}
    \item The standard basis in $\R^2$ is $\{\vec{e_1},\vec{e_2}\}$ where
      $\vec{e_1}=(1,0)\tran$ and $\vec{e_2}=(0,1)\tran$. We have
      \[
        T\vec{e_1}= \begin{pmatrix*}[r] 1\\2\\0 \end{pmatrix*}
        \quad\text{and}\quad
        T\vec{e_2}= \begin{pmatrix*}[r] 2\\-5\\7 \end{pmatrix*}
      \]
      %Observe also that
      %\[
        %\begin{pmatrix*}[r]
          %1&2\\
          %2&-5\\
          %0&7
        %\end{pmatrix*}
        %\begin{pmatrix}
          %x\\y
        %\end{pmatrix}=
        %\begin{pmatrix}
          %x+2y\\2x-5y\\7y
        %\end{pmatrix}
      %\]
      Hence
      $
        \begin{pmatrix*}[r]
          1&2\\
          2&-5\\
          0&7
        \end{pmatrix*}
      $ is its matrix.
    \item Let $\{\vec{e_1},\vec{e_2},\vec{e_3},\vec{e_4}\}$ be the standard 
      basis in $\R^4$. Hence
      \begin{align*}
        &T\vec{e_1}=T(1,0,0,0)\tran = \begin{pmatrix} 1\\0\\1 \end{pmatrix},
        &&T\vec{e_2}=T(0,1,0,0)\tran= \begin{pmatrix} 1\\1\\3 \end{pmatrix}\\
        &T\vec{e_3}=T(0,0,1,0)\tran= \begin{pmatrix} 1 \\0\\0 \end{pmatrix},
        &&T\vec{e_4}=T(0,0,0,1)\tran= \begin{pmatrix*}[r] 1\\-1\\6 \end{pmatrix*}
      \end{align*}
      Therefore, 
      $ \begin{pmatrix*}[r]
        1&1&1&1\\
        0&1&0&-1\\
        1&3&0&6
      \end{pmatrix*} $
      is its matrix.
    \item Let $E=\{t^n,t^{n-1},\dots,t,1\}$ be the standard basis
      and $f(t)=a_nt^n+a_{n-1}t^{n-1}+\cdots+a_1t+a_0\in\P_n$. We write
      \[
        f(t)=(a_n, a_{n-1},\dots,a_1,a_0)\tran
      \]
      is base $E$. Since
      \[
        T(t^n)=nt^{n-1},\quad~T(t^{n-1})=(n-1)t^{n-2}\quad,\dots,\quad T(t)=1,\quad T(1)=0
      \]
      Therefore its matrix is 
      \[
        \begin{pmatrix*}[l]
          0&0&0&\cdots&0&0\\
          n&0&0&\cdots&0&0\\
          0&n-1&0&\cdots&0&0\\
          0&0&n-2&\cdots&0&0\\
          \vdots & \vdots & \vdots & \ddots &\vdots & \vdots\\
          0&0&0&\cdots&1&0\\
          0&0&0&\cdots&0&0
        \end{pmatrix*}
      \]
      %\begin{align*}
        %Tf(t)=f'(t)&=na_nt^{n-1}+(n-1)a_{n-1}t^{n-2}+\cdots+a_1
      %\end{align*}
      %\begin{align*}
        %&T(t^n)=nt^{n-1}= \begin{pmatrix} 0\\n\\0\\\vdots\\0\\0 \end{pmatrix},
        %&&T(t^{n-1})=(n-1)t^{n-2}= \begin{pmatrix*} 0\\0\\n-1\\\vdots\\0\\0 \end{pmatrix*},\quad\\
        %&\vdots&&\vdots\\
        %&T(t)=1= \begin{pmatrix} 0\\0\\0\\\vdots\\1\\0 \end{pmatrix},
        %&&T(1)=0= \begin{pmatrix} 0\\0\\0\\\vdots\\0\\0 \end{pmatrix}
      %\end{align*}
      \newpage
    \item $Tf(t)=2f(t)+3f'(t)-4f''(t)$\\
      Again, the standard basis is $\{t^n,t^{n-1},\dots,t,1\}$. For
      each $i\in [0,n]$ we have
      \[T(t^i)=2t^i+3it^{i-1}-4i(i-1)t^{i-2}\]
      Hence the matrix is achieved by stacking 
      $[T(t^n),\dots,T(t^i),\dots,T(t),T(1)]$, therefore the matrix is
      \[ [T]=
        \begin{bmatrix*}
          2        &0      &\cdots &0   &0\\
          3n       &2      &\cdots &0   &0\\
          -4n(n-1) &3(n-1) &\cdots &0   &0\\
          \vdots \\
          0        &0      &\cdots &2   &0\\
          0        &0      &\cdots &3   &2
        \end{bmatrix*}
      \]
  \end{enumerate}
\end{proof}
% <==
% ==> ex4
\begin{exercise}
  Find $3\by3$ matrices representing the transformations of $\R^3$ which
  \begin{enumerate}
    \item project every vector onto $x$-$y$ plane;
    \item reflect every vector through $x$-$y$ plane;
    \item rotate the $x$-$y$ plane through $30^\circ$, leaving the
      $z$-axis alone.
  \end{enumerate}
\end{exercise}
\begin{proof}
  In space $\R^3$, we shall use its standard basis 
  $\{\vec{e_1},\vec{e_2},\vec{e_3}\}$ where 
  $\vec{e_1}=(1,0,0)\tran$,
  $\vec{e_2}=(0,1,0)\tran$ and 
  $\vec{e_3}=(0,0,1)\tran$.
  \begin{enumerate}
    \item Let $T$ be this transformation. This means 
      $T(x,y,z)\tran=(x,y,0)\tran$. We get
      \[
        T\vec{e_1}= \begin{pmatrix} 1\\0\\0 \end{pmatrix},\quad
        T\vec{e_2}= \begin{pmatrix} 0\\1\\0 \end{pmatrix},\quad
        T\vec{e_3}= \begin{pmatrix} 0\\0\\0 \end{pmatrix}.
      \]
      Therefore is matrix is 
      $ \begin{pmatrix} 1&0&0\\0&1&0\\0&0&0 \end{pmatrix}. $
    \item Let $R$ be this transformation. Since $R$ project every vector
      through $x$-$y$ plane, hence $R(x,y,z)\tran=(x,y,-z)\tran$. We get
      \[
        R\vec{e_1}= \begin{pmatrix} 1\\0\\0 \end{pmatrix},\quad
        R\vec{e_2}= \begin{pmatrix} 0\\1\\0 \end{pmatrix},\quad
        R\vec{e_3}= \begin{pmatrix*}[r] 0\\0\\-1 \end{pmatrix*}.
      \]
      Thus the matrix of $R$ is 
      $
      \begin{pmatrix*}[r]
        1&0&0\\
        0&1&0\\
        0&0&-1
      \end{pmatrix*}
      $
    \item Let $S$ be this transformation. $S$ moves the vectors
      $\vec{e_1},\vec{e_2}$ to the point $x',y'$ respectively.
      %Then we have 
      %$T(x,y,z)\tran = T(x',y',z)$ where $x'=\cos 30^\circ=\frac{\sqrt{3}}{2}$
      %and $y=\sin 30^\circ=\frac{1}{2}$.
      \begin{center}
        \begin{tikzpicture}[scale=2.5, rotate=-10]
          \coordinate (O) at (0,0);
          \coordinate (x) at (0,-1);
          \coordinate (y) at (1,0);
          \coordinate (xp) at (-60:1);
          \coordinate (yp) at (30:1);


          \draw[-open triangle 60, thick] (0,0)--(0,-1) node[below]{$\vec{e_1}$};
          \draw[-open triangle 60, thick] (0,0)--(1,0) node[right]{$\vec{e_2}$};

          \draw[-open triangle 60, gray] (0,0)--(-60:1);
          \draw[-open triangle 60, gray] (0,0)--(30:1);

          \draw[dashed] ($(0,-0.86)$)--(xp) (0.5,0)--(xp);
          %\draw

          \node at ($(-60:1)+(0.1,-0.05)$) {$S\vec{e_1}$};
          \node at ($(30:1)+(0.1,0.05)$) {$S\vec{e_2}$};

          \draw pic[draw, angle eccentricity=2] {angle=x--O--xp};
          \draw pic[draw, angle eccentricity=2,"$30^\circ$"] {angle=y--O--yp};
        \end{tikzpicture}
      \end{center}
      Since $\cos30^\circ=\frac{\sqrt{3}}{2}$ and $\sin30^\circ=\frac{1}{2}$,
      we conclude that
      \[
        S\vec{e_1}= \begin{pmatrix} \frac{\sqrt{3}}{2}\\[0.2cm]\frac{1}{2}\\[0.2cm]0 \end{pmatrix},\quad
        S\vec{e_2}= \begin{pmatrix} -\frac{1}{2}\\[0.2cm]\frac{\sqrt{3}}{2}\\[0.2cm]0 \end{pmatrix},\quad
        S\vec{e_3}= \begin{pmatrix*}[r] 0\\0\\1 \end{pmatrix*}.
      \]
      Therefore the matrix is
      $ \begin{pmatrix}
        \frac{\sqrt{3}}{2} & -\frac{1}{2} & 0\\[0.2cm]
        \frac{1}{2} & \frac{\sqrt{3}}{2} & 0\\[0.2cm]
        0 & 0 &1
      \end{pmatrix}. $
  \end{enumerate}
\end{proof}
% <==
% ==> ex5
\begin{exercise}
  Let $A$ be a linear transformation. If $\vec{z}$ is the center of the
  staight interval $[\vec{x},\vec{y}]$, show that $A\vec{z}$ is the
  center of the interval $[A\vec{x}, A\vec{y}]$.
\end{exercise}
\begin{proof}
  $\vec{z}$ is the center of $[\vec{x}, \vec{y}]$ iff 
  $\vec{z}=\frac{1}{2}\vec{x}+\frac{1}{2}\vec{y}$. Therefore,
  \[
    A\vec{z}=A\left(\frac{1}{2}\vec{x}+\frac{1}{2}\vec{y}\right)
    =\frac{1}{2}A\vec{x}+\frac{1}{2}A\vec{y}
  \]
  Thus, $A\vec{z}$ is the center of the interval $[A\vec{x}, A\vec{y}]$.
\end{proof}
% <==
% ==> ex6
\begin{exercise}
  The set $\C$ of complex numbers can be canonically identified 
  with the space $\R^2$ by treating each $z=x+iy\in\C$ as a column
  $(x,y)\tran\in\R^2$.
  \begin{enumerate}
    \item Treating $\C$ as a complex vector space, show that the multiplication
      by $\alpha=a+ib\in\C$ is a linear transformation in $\C$.
      What is its matrix.
    \item Treating $\C$ as the real vector space $\R^2$ show that the multiplication 
      by $\alpha=a+ib\in\C$ is a linear transformation there.
    \item Define $T(x+iy)=2x-y+i(x-3y)$. Show that this tran is not a 
      linear transformation in the complex vector space $\C$, but
      if we treat $\C$ as the real vector space $\R^2$ then it is a linear
      transformation there, then find its matrix.
  \end{enumerate}
\end{exercise}
\begin{proof}
  \text{}
  \begin{enumerate}
    \item Let $T$ be this transformation. For any $\vec{x}\in\C$, we have
      $T\vec{x}=\alpha \vec{x}\in\C$.
      Thus $T:\C\to\C$, and we'll prove that $T$ is a linear 
      transformation. Let $\vec{x},\vec{y}\in\C$ be two vectors, 
      and $z\in\C$ be a scalar (complex). Observe that
      \begin{itemize}
        \item $T(\vec{x}+\vec{y})=\alpha(\vec{x}+\vec{y})
          =\alpha\vec{x}+\alpha\vec{y}=T\vec{x}+T\vec{y}$
          \quad (distributivity of complex numbers)
        \item $T(z\vec{x})=\alpha(z\vec{x})=z(\alpha\vec{x})=zT\vec{x}$
      \end{itemize}
      This shows that this transformation $T$ is a linear one.
      To find its matrix, we only need to know the basis of $\C$.
      Since any vector $\vec{x}\in\C$ we be written as 
      \[\vec{x}=1\cdot \underbrace{\vec{x}}_{\text{scalar}}\]
      and because this representation is unique, we obtain that 
      $\{1\}\subset\C$ is a basis of $\C$. Thus the matrix is
      \[
        [T]=[T(1)]=[\alpha\cdot 1]=[\alpha].
      \]
    \item Because we treat $\C$ as $\R^2$, then any complex number 
      $\vec{x}=x+iy$ can be represented as 
      $\begin{pmatrix} x\\y \end{pmatrix}\in\R^2$.
      Let $T$ be this transformation. Thus $T$ would look like
      \begin{align*}
        T\begin{pmatrix} x\\y \end{pmatrix}
        &=T(\vec{x})=\alpha\vec{x}\\
        &=(a+ib)(x+iy)\\
        &=(ax-by)+i(ay+bx)\\
        &=\begin{pmatrix} ax-by\\ay+bx \end{pmatrix}\in\R^2
      \end{align*}
      Thus $T:\R^2\to\R^2$.
      We need to show that $T$ is in fact linear. Let 
      $\vec{x_1}= \begin{pmatrix} x_1\\y_1 \end{pmatrix} $ and 
      $\vec{x_2}= \begin{pmatrix} x_2\\y_2 \end{pmatrix} $ and 
      be two arbitrary vectors. We have
      \[
        T\vec{x_1}+T\vec{x_2}=
        \begin{pmatrix} ax_1-by_1\\ay_1+bx_1 \end{pmatrix}+
        \begin{pmatrix} ax_2-by_2\\ay_2+bx_2 \end{pmatrix}=
        \begin{pmatrix} a(x_1+x_2)-b(y_1+y_2)\\a(y_1+y_2)+b(x_1+x_2) \end{pmatrix}=
        T(\vec{x_1}+\vec{x_2}),
      \]
      and for any scalar $r\in\R$,
      \[
        rT\vec{x}=r\begin{pmatrix} ax-by\\ay+bx \end{pmatrix}=
        \begin{pmatrix} rax-rby\\ray+rbx \end{pmatrix}=
        T(r\vec{x}).
      \]
      This shows that $T$ is a linear transformation.
      To find the matrix, we first need to find a bisis in $\R^2$.
      Luckily, as we've proved earlier we could choose $\{\vec{e_1},\vec{e_2}\}$
      to be a basis where
      \[
        \vec{e_1} \begin{pmatrix} 1\\0 \end{pmatrix}
        \quad\text{and}\quad
        \vec{e_2} \begin{pmatrix} 0\\1 \end{pmatrix}
      \]
      Therefore
      \[
        T\vec{e_1}= \begin{pmatrix} a\\b \end{pmatrix}
        \quad\text{and}\quad
        T\vec{e_2}= \begin{pmatrix} -b\\a \end{pmatrix}
      \]
      Thus the matrix of this transformation is 
      $ \begin{pmatrix} a&-b\\b&a \end{pmatrix} $.
    \item Define $T(x+iy)=2x-y+i(x-3y)$
      \begin{itemize}
        \item We'll prove that $T$ is not linear in complex vector space.
          Observe that
          \[
            T(i)=T(0+i)=-1-3i
            \quad\text{and}\quad
            T(1)=T(1+0i)=2+i
          \]
          cleary $T(i)\neq iT(1)$, this implies that $T$ is 
          not a linear transformation in $\C$.
        \item 
          In $\R^2$ the transformation would look like
          \[
            T\begin{pmatrix}x\\y\end{pmatrix}=
            \begin{pmatrix}2x-y\\x-3y\end{pmatrix}.
          \]
          For any vectors
          $\vec{x_1}=\begin{pmatrix}x_1\\y_1\end{pmatrix}$ and
          $\vec{x_2}=\begin{pmatrix}x_2\\y_2\end{pmatrix}$, we have
          \[
            T\vec{x_1}+T\vec{x_2}=
            \begin{pmatrix}2x_1-y_1\\x_1-3y_1\end{pmatrix}+
            \begin{pmatrix}2x_2-y_2\\x_2-3y_2\end{pmatrix}=
            \begin{pmatrix}2(x_1+x_2)-(y_1+y_2)\\(x_1+x_2)-3(y_1+y_2)\end{pmatrix}=
            T(\vec{x_1}+\vec{x_2})
          \]
          and for any scalar $r\in\R$,
          \[
            rT\vec{x}=r\begin{pmatrix}2x-y\\x-3y\end{pmatrix}=
            \begin{pmatrix}2rx-ry\\rx-3ry\end{pmatrix}=
            T(r\vec{x})
          \]
          this shows that $T:\R^2\to\R^2$ is linear.
          Because $\begin{pmatrix} 1\\0 \end{pmatrix}$ and 
          $\begin{pmatrix} 0\\1 \end{pmatrix}$ is a basis in $\R^2$ and
          \[
            T\begin{pmatrix} 1\\0 \end{pmatrix}=\begin{pmatrix} 2\\1 \end{pmatrix}
            \quad\text{and}\quad
            T\begin{pmatrix} 0\\1 \end{pmatrix}=\begin{pmatrix} -1\\-3 \end{pmatrix},
          \]
          thus the matrix of this transformation is 
          $ \begin{pmatrix*}[r] 2 & -1\\1&-3 \end{pmatrix*} $.
      \end{itemize}
  \end{enumerate}
\end{proof}
% <==
% ==> ex7
\begin{exercise}
  Show that any linear transformation in $\C$ 
  (treated as a complex vector space) is a multiplication by 
  $\alpha\in\C$.
\end{exercise}
\begin{proof}
  Let $T:\C\to\C$ be this transformation. For any $\vec{x}\in\C$
  \[
    T\vec{x}=T(\vec{x}\cdot \vec{1})=\vec{x}\cdot \underbrace{T(\vec{1})}_{\text{scalar}}
  \]
  and the proof is completed.
\end{proof}
% <==
% <==
% ==> linear transformation as a vector
\section{Linear transformation as a Vector}
Let set $\mathcal{L}(V,W)$ is a vector space with addition and scalar 
multiplication (as proved above).
% <==
% ==> composition
\section{Composition}

% ==> hw1
\begin{homework}\label{_trace}
  Let $A$ and $B$ be matrices of size $m\by n$ and $n\by m$ 
  respectively. Then \[\tr(AB)=\tr(BA).\]
\end{homework}
\begin{proof}
  % ==> intro
  We'd like to prove this theorem \emph{less} computationally.
  Let $X\in M_{n\by m}$. Consider the mapping
  $T,~T_1: M_{n\by m}\to\F$ defined by
  \[
    T(X)=\tr(AX)
    \quad\text{and}\quad
    T_1(X)=\tr(XA).
  \]
  To prove the theorem it is sufficient to show that 
  $T,T_1$ are linear and they are the same.
  so by substituting $X=B$ gives the theorem.
  % <==
  % ==> claim: linearity
  \begin{claim}
    The transformations $T,T_1$ defined above are linear.
  \end{claim}
  \begin{proof}
    For $X,Y\in M_{n\by m}$,
    \begin{itemize}
      \item From the properties of matrix, $A(X+Y)=AX+AY$. 
        Because $AX$ and $BX$ are both square matrices with 
        size $m\by m$, and since we add the matrices $AX+AY$
        entrywise, it follows that
        \begin{align*}
          T(X+Y)&=\tr(A(X+Y))=\tr(AX+AY)\\
                &=\tr(AX)+\tr(AY)\\
                &=T(X)+T(Y)
        \end{align*}
      \item Similarly for any scalar $\alpha\in\F$,
        \[
          T(\alpha X)=\tr(A\cdot\alpha X)
          =\tr(\alpha AX)=\alpha\tr(AX)=\alpha T(X)
        \]
    \end{itemize}
    This implies that $T$ is a linear transformation. With simply
    proof, we conclude that $T_1$ is also a linear transformation.
  \end{proof}
  % <==
  % ==> claim: T=T_1
  We choose $\vec{e_{11}},\vec{e_{21}},\dots,\vec{e_{nm}}$ to be 
  the standard basis of $M_{n\by m}$, meaning the vector
  \[
    \vec{e_{ij}}=
    \begin{pmatrix}
      0 & 0 & \cdots & 0 & \cdots & 0\\
      \vdots  & & \cdots & & \cdots & \vdots\\
      0 & 0 & \cdots & 1 & \cdots & 0\\
      0 & 0 & \cdots & 0 & \cdots & 0
    \end{pmatrix}
  \]
  is a matrix whose entries are zero, except at
  the entry at row $i$ and column $j$, which is $1$.
  Then we only need to show that $T\vec{e_{ij}}=T_1\vec{e_{ij}}$
  for all $i,j$. Let 
  \[
    A=
    \begin{pmatrix}
      a_{11} & a_{12} & \cdots & a_{1j} & \cdots  &a_{1m}\\
      \vdots &        & \vdots &        & \vdots  &\vdots\\
      a_{i1} & a_{i2} & \cdots & a_{ij} & \cdots  &a_{im}\\
      a_{nn} & a_{n2} & \cdots & a_{nj} & \cdots  &a_{nm}
    \end{pmatrix}
  \]
  Hence 
  \begin{align*}
    A\vec{e_{ij}}&=
    \begin{pmatrix}
      \cdot & \cdot & \cdot  & \cdot \\
      \cdot & \cdot & a_{ij} & \cdot \\
      \cdot & \cdot & \cdot  & \cdot
    \end{pmatrix}
    \begin{pmatrix}
      0 & 0 & 0 \\
      0 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 0 & 0
    \end{pmatrix}=
    \begin{pmatrix}
      0     &\cdot   &\cdot \\
      \cdot &a_{ij}  &\cdot \\
      \cdot &\cdot   &0
    \end{pmatrix}
  \intertext{and}
    \vec{e_{ij}}A&=
    \begin{pmatrix}
      0 & 0 & 0 \\
      0 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 0 & 0
    \end{pmatrix}
    \begin{pmatrix}
      \cdot & \cdot & \cdot  & \cdot \\
      \cdot & \cdot & a_{ij} & \cdot \\
      \cdot & \cdot & \cdot  & \cdot
    \end{pmatrix}=
    \begin{pmatrix}
      0      &  \cdot  &  \cdot  & \cdot \\
      \cdot  &  0      &  \cdot  & \cdot \\
      \cdot  &  \cdot  &  a_{ij} & \cdot \\
      \cdot  &  \cdot  &  \cdot  & 0
    \end{pmatrix}
  \end{align*}
  This implies that $T\vec{e_{ij}}=T_1\vec{e_{ij}}$ for all $i,j$,
  and hence $T=T_1$.
  % <==
\end{proof}
% <==

% ==> ex1
\begin{exercise}
  Working on it.
\end{exercise}
% <==
% ==> ex2
\begin{exercise}
  Let $T_{\gamma}$ be the rotation matrix by $\gamma$ in $\R^2$.
  Check by matrix multiplication that 
  $T_{\gamma}T_{-\gamma}=T_{-\gamma}T_{\gamma}=I$.
\end{exercise}
\begin{proof}
  Working on it.
\end{proof}
% <==
% ==> ex3
\begin{exercise}
  Multiply two rotation matrices $T_{\alpha}$ and $T_{\beta}$. Deduce
  formulas for $\sin(\alpha+\beta)$ and $\cos(\alpha+\beta)$
  from here.
\end{exercise}
\begin{proof}
  Working on it.
\end{proof}
% <==
% ==> ex4
\begin{exercise}
  Find the matrix of the orthogonal projection in $\R^2$ on to the line
  $x_1=-2x_2$.
\end{exercise}
\begin{proof}
  Let $T$ be this transformation. Let $R_{\gamma}$ and $P_x$ be the
  transformations of rotation by $\gamma$ and projection to $x$-axis,
  respectively. Therefore $T=R_{\gamma}P_xR_{-\gamma}$. Note that
  \[
    R_{\gamma}=
    \begin{pmatrix*}[r]
      \cos\gamma &-\sin\gamma\\
      \sin\gamma &\cos\gamma
    \end{pmatrix*},
    \quad
    R_{-\gamma}=
    \begin{pmatrix*}[r]
      \cos\gamma &\sin\gamma\\
      -\sin\gamma &\cos\gamma
    \end{pmatrix*},
    \quad\text{and}\quad
    P_x=
    \begin{pmatrix}
      1  &0\\
      0  &0
    \end{pmatrix}
  \]
  Thus, 
  \[
    \cos\gamma=\frac{\overline{OB}}{\overline{OA}}=\frac{2}{\sqrt{5}}
    \quad\text{and}\quad
    \sin\gamma=\frac{\overline{AB}}{\overline{OA}}=\frac{-1}{\sqrt{5}}.
  \]
  We get
  \marginpar{
  \begin{center}
    \begin{tikzpicture}[scale=0.9]
      \coordinate [label=-135:$O$] (O) at (0,0);
      \coordinate [label=-90:$A$]  (X) at (2,-1);
      \coordinate [label=90:$B$]   (Y) at (2,0) ;
      \draw[-open triangle 60] (-1,0) -- (3.5,0);
      \draw[-open triangle 60] (0,-2) -- (0,2);

      % draw the tick
      \foreach \x in {1,2,3}{
        \draw (\x,0.1)--(\x,-0.1);
      }
      \draw (-0.1,-1)--(0.1,-1);

      % draw the dashes
      \draw[dashed] (X)--(0,-1) (X)--(Y);

      \draw (-1,0.5)--(3.5,-1.75);
      %\draw (-1,0.5)--(3.5,-1.75)node[right]{$y=-\frac{1}{2}x$};
      \draw pic[draw, angle eccentricity=2, "$\gamma$"]
        {angle=X--O--Y};
    \end{tikzpicture}
  \end{center}
  }
  \begin{align*}
    T&=
    \begin{pmatrix*}[r]
      \frac{2}{\sqrt{5}}  &\frac{1}{\sqrt{5}}\\[0.3cm]
      -\frac{1}{\sqrt{5}} &\frac{2}{\sqrt{5}}
    \end{pmatrix*}
    \begin{pmatrix} 1&0\\0&0 \end{pmatrix}
    \begin{pmatrix*}[r]
      \frac{2}{\sqrt{5}}  &-\frac{1}{\sqrt{5}}\\[0.3cm]
      \frac{1}{\sqrt{5}}  &\frac{2}{\sqrt{5}}
    \end{pmatrix*}\\[0.3cm]
    &=
    \frac{1}{5}
    \begin{pmatrix*}[r] 2 &1\\-1&2 \end{pmatrix*}
    \begin{pmatrix} 1&0\\0&0 \end{pmatrix}
    \begin{pmatrix*}[r] 2 &-1\\1&2 \end{pmatrix*}\\[0.3cm]
    &=
    \frac{1}{5}
    \begin{pmatrix*}[r]
      4&-2\\-2&-2
    \end{pmatrix*}
  \end{align*}
\end{proof}
% <==
% ==> ex5
\begin{exercise}
  Find linear transformations $A,B:\R^2\to\R^2$ such that 
  $AB=\vec{0}$ but $BA\neq \vec{0}$.
\end{exercise}
\begin{proof}[Solution]
  Consider the following
  \[
    \begin{pmatrix}
      1&1\\2&2
    \end{pmatrix}
    \begin{pmatrix*}[r]
      -1 &1\\ 1&-1
    \end{pmatrix*}=
    \begin{pmatrix}
      0&0\\0&0
    \end{pmatrix}
  \]
  however
  \[
    \begin{pmatrix*}[r]
      -1 &1\\ 1&-1
    \end{pmatrix*}
    \begin{pmatrix}
      1&1\\2&2
    \end{pmatrix}=
    \begin{pmatrix*}[r]
      1&1\\-1&-1
    \end{pmatrix*}\neq \vec{0}
  \]
  Therefore, these two matrices are the ones we wish to find.
\end{proof}
% <==
% ==> ex6
\begin{exercise}
  Prove that $\tr(AB)=\tr(BA)$.
\end{exercise}
\begin{proof}
  See on page \pageref{_trace}.
\end{proof}
% <==
% ==> ex7
\begin{exercise}
  Construct a non-zero matrix $A$ such that $A^2=\vec{0}$
\end{exercise}
\begin{proof}
  Let $A=\begin{pmatrix} a&c\\b&d \end{pmatrix}$. Thus
  perform the multiplication, we get
  \[
    \begin{cases}
      a^2 + bc=0\\
      ac+cd=0\\
      ab+bd=0\\
      bc+d^2=0
    \end{cases}
  \]
  for simplicity, we'll choose $a=1$. Hence $bc=-1$ and
  \[
    \begin{cases}
      c(d+1)=0\\
      b(d+1)=1\\
      d^2=1
    \end{cases}
  \]
  this suggests that $d=-1$, and $bc=-1$. Here, we'll choose $b=1$
  and $c=-1$. Therefore, the matrix
  \[ A=\begin{pmatrix}1&-1\\1&-1\end{pmatrix}. \]
\end{proof}
% <==
% ==> ex8
\begin{exercise}
  Find the matrix of the reflection through the line $y=-2x/3$.
\end{exercise}
  \marginpar{
  \begin{center}
    \begin{tikzpicture}[scale=0.9]
      \coordinate [label=-135:$O$] (O) at (0,0);
      \coordinate [label=-95:$A$]  (X) at (3,-2);
      \coordinate [label=90:$B$]   (Y) at (3,0) ;
      \draw[-open triangle 60] (-1,0) -- (3.8,0);
      \draw[-open triangle 60] (0,-2.5) -- (0,1);

      % draw the tick
      \foreach \x in {1,2,3}{
        \draw (\x,0.1)--(\x,-0.1);
      }
      \draw (-0.1,-1)--(0.1,-1);
      \draw (-0.1,-2)--(0.1,-2);

      % draw the dashes
      \draw[dashed] (X)--(0,-2) (X)--(Y);

      \draw (-1,2/3)--(3.5,-2.4);
      %\draw (-1,0.5)--(3.5,-1.75)node[right]{$y=-\frac{1}{2}x$};
      \draw pic[draw, angle eccentricity=2, "$\gamma$"]
        {angle=X--O--Y};
    \end{tikzpicture}
  \end{center}
  }
\begin{proof}
  Let $T$ be this transformation and $\gamma$ be the angle between
  the $x$-axis and the line $y=-2x/3$. Hence $T=R_{\gamma}T_0R_{\gamma}$.
  We then have $\cos\gamma=OB/OA=3/\sqrt{13}$ and 
  $\sin\gamma=-AB/OA=-2/\sqrt{13}$. Thus
  \begin{align*}
    T&=
    \begin{pmatrix*}[r]
      \cos\gamma &-\sin\gamma\\
      \sin\gamma &\cos\gamma
    \end{pmatrix*}
    \begin{pmatrix*}[r]
      1&0\\0&-1
    \end{pmatrix*}
    \begin{pmatrix*}[r]
      \cos\gamma &\sin\gamma\\
      -\sin\gamma &\cos\gamma
    \end{pmatrix*}\\
     &=
     \frac{1}{13}
     \begin{pmatrix*}[r]
       3&2\\-2&3
     \end{pmatrix*}
    \begin{pmatrix*}[r]
      1&0\\0&-1
    \end{pmatrix*}
     \begin{pmatrix*}[r]
       3&-2\\2&3
     \end{pmatrix*}\\
     &=
     \frac{1}{13}
     \begin{pmatrix*}[r]
       3&2\\-2&3
     \end{pmatrix*}
     \begin{pmatrix*}[r]
       3&-2\\-2&-3
     \end{pmatrix*}\\
     &=
     \frac{1}{15}
     \begin{pmatrix*}[r]
       5&-12\\-12&-5
     \end{pmatrix*}.
  \end{align*}
\end{proof}
% <==
% <==
% ==> isomorphism
\section{Isomophism}
% ==> ex1
\begin{exercise}
  Prove that if $A:V\to W$ is an isomorphism and
  $\vec{v_1},\vec{v_2},\dots,\vec{v_n}$ is a basis in $V$,
  then $A\vec{v_1},A\vec{v_2},\dots,A\vec{v_n}$ is a basis in
  $W$.
\end{exercise}
\begin{proof}
  Since $A:V\to W$ is an isomorphism, hence it's invertable i.e. 
  there is a linear transformation $A\inv:W\to V$ such that
  $AA\inv=A\inv A=I$. Thus for any $\vec{w}\in W$, there is a
  $\vec{v}\in V$ such that $A\inv\vec{w}=\vec{v}$. Recall that
  $\{\vec{v_1},\vec{v_2},\dots,\vec{v_n}\}$ is a basis in $V$, then
  there are unique scalars $\alpha_1,\alpha_2,\dots,\alpha_n$ such that
  \[\vec{v}=\alpha_1\vec{v_1}+\alpha_2\vec{v_2}+\cdots+\alpha_n\vec{v_n}.\]
  This implies that
  \begin{align*}
    A\inv\vec{w}&=\alpha_1\vec{v_1}+\alpha_2\vec{v_2}+\cdots+\alpha_n\vec{v_n}\\
    AA\inv\vec{w}&=A(\alpha_1\vec{v_1}+\alpha_2\vec{v_2}+\cdots+\alpha_n\vec{v_n})\\
    \vec{w}&=\alpha_1A\vec{v_1}+\alpha_2A\vec{v_2}+\cdots+\alpha_nA\vec{v_n}\\
  \end{align*}
  Because $\alpha_1,\alpha_2,\cdots,\alpha_n$ are unique, we conclude
  that any $\vec{w}\in W$ can be represented as a unique linear combination
  of $A\vec{v_1},A\vec{v_2},\dots,A\vec{v_2}$. Thus the proof is completed.
\end{proof}
% <==
% ==> ex2
\begin{exercise}
  Find all right inverses of the $1\by 2$ matrix (row) $A=(1,1)$.
  Conclude from here thaat the row $A$ is not left invertable.
\end{exercise}
% <==
% ==> ex3
\begin{exercise}
  Find all the left inverses of the column $(1,2,3)\tran$.
\end{exercise}
\begin{proof}
  Let $A=(1,2,3)\tran$. Because $A$ is a $3\by 1$ matrix, then its
  inverse, say $B$ is a $1\by 3$ matrix. Let 
  $B= \begin{pmatrix} x&y&z \end{pmatrix} $. Hence
  \[AB=
    \begin{pmatrix}
      x&y&z
    \end{pmatrix}
    \begin{pmatrix}
      1\\2\\3
    \end{pmatrix}=
    \begin{pmatrix}
      1
    \end{pmatrix}
  \]
  This implies that $x+2y+3z=1$ or $x=1-2y-3z$. Thus all the left
  inverses of $A$ is in the form
  \[B=
    \begin{pmatrix}
      1-2y-3z & y&z
    \end{pmatrix}
  \]
  where $y,z$ are arbitrary real numbers.
\end{proof}
% <==
% ==> ex4
\begin{exercise}
  Is the column $(1,2,3)\tran$ right invertable?
\end{exercise}
\begin{proof}[Solution]
  The column $(1,2,3)\tran$ is not right invertable, because
  as proved in previous exercise the column $(1,2,3)\tran$ has
  more than one left inverses.
\end{proof}
% <==
% ==> ex5
\begin{exercise}
  Find two matrices $A$ and $B$ that $AB$ is invertable, but
  $A$ and $B$ are not.
\end{exercise}
\begin{proof}[Solution]
  Consider:
  $ A=
    \begin{pmatrix}
      2&1&-1
    \end{pmatrix}
    \quad\text{and}\quad
    B=
    \begin{pmatrix}
      1&2&3
    \end{pmatrix}\tran $
    Note that
    \[
      AB=
    \begin{pmatrix}
      2&1&-1
    \end{pmatrix}
    \begin{pmatrix}
      1\\2\\3
    \end{pmatrix}=
    \begin{pmatrix}
      2+2-3
    \end{pmatrix}=
    \begin{pmatrix}
      1
    \end{pmatrix}
    \]
    However, as proved in previous exercise, we know that
    the matrix $A$ is not invertable. And we wish to prove that
    $B$ is not invertable either. To achieved this, we have to
    find two matrices that are right invertable to $B$. Observe that
    \[
      \begin{pmatrix}
        2&1&-1
      \end{pmatrix}
      \begin{pmatrix}
        1\\0\\1
      \end{pmatrix}=
      \begin{pmatrix}
        1
      \end{pmatrix}
      \quad\text{and}\quad
      \begin{pmatrix}
        2&1&-1
      \end{pmatrix}
      \begin{pmatrix}
        2\\1\\6
      \end{pmatrix}=
      \begin{pmatrix}
        1
      \end{pmatrix}
    \]
    This suggests that $B$ is not invertable. Therefore, we've fond
    matrices $A$ and $B$ such that $AB$ is invertable, yet 
    $A$ and $B$ are not.
\end{proof}
% <==
% ==> ex6
\begin{exercise}
  Suppose the product $AB$ is invertable. Show that $A$ is right 
  invertable, and $B$ is left invertable.
\end{exercise}
\begin{proof}
  Because $AB$ is invertable, then matrix $(AB)\inv$ is defined.
  Observe that
  \[ A\cdot B(AB)\inv=AB\cdot(AB)\inv=I \] and 
  \[ (AB)\inv A\cdot B=(AB)\inv\cdot AB=I\]
  This shows that $A$ is right invertable, and $B$ is left invertable,
  as expected.
\end{proof}
% <==
% ==> ex7
\begin{exercise}
  Let $A$ and $AB$ be invertable. Prove that $B$ is also in invertable.
\end{exercise}
\begin{proof}
  We claim that $(AB)\inv A$ is the inverse of $B$. To prove this,
  observe that 
  \[
    (AB)\inv A\cdot B=(AB)\inv\cdot AB=I
  \] and
  \[
    B\cdot (AB)\inv A=A\inv AB\cdot (AB)\inv A=A\inv IA=I
  \]
  This shows that $(AB)\inv A$ is both the left and the right inverses
  of $B$. Thus, $B$ is invertable.
\end{proof}
% <==
% ==> ex8
\begin{exercise}
  Let $A$ be $n\by n$ matrix. Prove that if $A^2=\vec{0}$ then 
  $A$ is not invertable.
\end{exercise}
\begin{proof}
  Assume by contradiction that $A$ is invertable, meaning there's a
  matrix $A\inv$ such that $AA\inv=A\inv A=I$. Thus
  \[
    A=A^2A\inv=\vec{0}A\inv=\vec{0}
  \]
  Then $I=AA\inv=\vec{0}A\inv=\vec{0}$, a contradiction. Therefore,
  $A$ is not invertable.
\end{proof}
% <==
% ==> ex9
\begin{exercise}
  Suppose $AB=\vec{0}$ for some non-zero matrix $B$. Can $A$ invertable?
\end{exercise}
\begin{proof}
  We claim that $A$ is not invertable. To prove this, we assume by
  contradiction that $A$ is invertable. Hence $A\inv$ exists, and
  \[
    B=A\inv A\cdot B=A\inv\cdot AB=A\inv\vec{0}=\vec{0}
  \]
  which is a contradiction that $B$ is non-zero.
\end{proof}
% <==
% ==> ex10
\begin{exercise}
  Write matrices of the linear transformations $T_1$ and $T_2$ in $\F^5$,
  defined as follows: $T_1$ interchanges the coordinates $X_2$ and $x_2$
  of the vector $\vec{x}$, and $T_2$ just adds to the coordinates 
  $x_2\to$
  $a$ times the coordinate $x_4$. and does not change other coordinates, i.e.
  \[
    T_1
    \begin{pmatrix}
      x_1\\x_2\\x_3\\x_4\\x_5
    \end{pmatrix}=
    \begin{pmatrix}
      x_1\\x_4\\x_3\\x_2\\x_5
    \end{pmatrix}
    \quad\text{and}\quad
    T_2
    \begin{pmatrix}
      x_1\\x_2\\x_3\\x_4\\x_5
    \end{pmatrix}=
    \begin{pmatrix}
      x_1\\x_2+ax_4\\x_3\\x_4\\x_5
    \end{pmatrix}.
  \]
  where $a$ is a fixed number.
  Show that $T_1$ and $T_2$ are invertable transformations, and write
  the matrices of the inverses.
\end{exercise}
\begin{proof}
  The matrix of this 
\end{proof}
% <==



% <==
% ==> subspace
\section{Subspaces}


% ==> ex1
\begin{exercise}
  Let $X$ and $Y$ be subspaces of a vector space $V$. Prove that
  $X\cap Y$ is a subspace of $V$.
\end{exercise}
\begin{proof}
  Let $\vec{a}$ and $\vec{b}$ be arbitrary vectors of $X\cap Y$.
  Because $X$ and $Y$ are themselves  subspaces of $V$, hence 
  \[
    \begin{cases}
      \vec{a}\in X\\
      \vec{b}\in X
    \end{cases}\implies
    \begin{cases}
      \alpha\vec{a}\in X\\
      \beta\vec{b}\in X
    \end{cases}
  \]
  this implies that $\alpha\vec{a}+\beta\vec{b}\in X$ for
  any scalars $\alpha,\beta$. Similarly, $\alpha\vec{a}+\beta\vec{b}\in Y$.
  Thus $\alpha\vec{a}+\beta\vec{b}\in X\cap Y$. Therefore, 
  $X\cap Y$ is also a subspace of $V$.
\end{proof}
% <==
% ==> ex2
\begin{exercise}
  Let $V$ be a vector space. For $X,Y\subset V$ the sum $X+Y$
  is the collection of all vectors $\vec{v}$ which can be represented as 
  $\vec{v}=\vec{x}+\vec{y}$ where $\vec{x}\in X$ and $\vec{y}\in Y$.
  Show that if $X$ and $Y$ are subspaces of $V$, then $X+Y$ is also
  a subspace of $V$.
\end{exercise}
\begin{proof}
  Let $\vec{v_1},\vec{v_2}\in X+Y$ then there are $\vec{x_1},\vec{x_2}\in X$
  and $\vec{y_1},\vec{y_2}\in Y$ such that
  \[
    \vec{v_1}=\vec{x_1}+\vec{y_1}
    \quad\text{and}\quad
    \vec{v_2}=\vec{x_2}+\vec{y_2}
  \]
  Because $X,Y$ are subspaces of $V$, then
  \[
    \alpha\vec{v_1}+\beta\vec{v_2}
    =\underbrace{(\alpha\vec{x_1}+\beta\vec{x_2})}_{\text{vector of $X$}} + 
    \underbrace{(\alpha\vec{y_1}+\beta\vec{y_2})}_{\text{vector of $Y$}}
  \]
  Hence $\alpha\vec{v_1}+\beta\vec{v_2}$ is also a vector of $X+Y$.
  Thus $X+Y$ is a subspace of $V$.
\end{proof}
% <==
% ==> ex3
\begin{exercise}
  Let $X$ be a subspace of a vector space $V$, and let $\vec{v}\in V$
  and $\vec{v}\notin X$. Prove that if $\vec{x}\in X$ then
  $\vec{x}+\vec{v}\notin X$.
\end{exercise}
\begin{proof}
  We'll prove this by contradiction by assuming that 
  $\vec{x}+\vec{v}\in X$. Because $X$ is a subspace of $V$ and 
  $\vec{x}\in X$, hence $\vec{-x}\in X$. Thus
  \[
    (\vec{x}+\vec{v})+(\vec{-x})\in X
  \]
  and we conclude that $\vec{v}\in X$, which is a contradiction.
\end{proof}
% <==
% ==> ex4
\begin{exercise}
  Let $X$ and $Y$ be subspaces of a vector space $V$. Using the previous
  exercise, show that $X\cup Y$ is a subspace iff $X\subset Y$ or 
  $Y\subset X$.
\end{exercise}
\begin{proof}
  We need to prove this in two directions.
  \begin{itemize}
    \item If $X\subset Y$ or $Y\subset X$: Without loss of generality, 
      we may assume that $Y\subset X$. Therefore $X\cup Y=X$ is a 
      subspace of $V$.
    \item If $X\not\subset Y$ and $Y\not\subset Y$, we now wish to show that
      $X\cup Y$ is not a subspace. Observe that if
      $X\not\subset Y$ and $Y\not\subset X$ that means there are 
      $\vec{x_0}\in X$ and $\vec{y_0}\in Y$ such that $\vec{x_0}\notin Y$
      and $\vec{y_0}\notin X$. Hence $\vec{x_0},\vec{y_0}\in X\cup Y$ and
      follow from the previous exercise, we conclude that
      \[
        \vec{x_0}+\vec{y_0}\notin X
        \quad\text{and}\quad
        \vec{x_0}+\vec{y_0}\notin Y
      \]
      Therefore, $\vec{x_0}+\vec{y_0}\notin X\cup Y$. This suggests that
      $X\cup Y$ is not a vector space. Hence the proof is completed.
  \end{itemize}
\end{proof}
% <==
% <==

\chapter{Systems of linear equations}

% ==> intro
\section{Different faces of linear transformation}
% <==
% ==> Echelon form
\section{Solution of a linear system. Echelon forms}

% ==> ex1
\begin{exercise}
  Write the systems of equations below in matrix form.
\end{exercise}
% <==
% ==> ex2
\begin{exercise}
  Find all solutions of the vector equation
  \[x_1\vec{v_1}+x_2\vec{v_2}+x_3\vec{v_3}=\vec{0}\]
  where $\vec{v_1}=(1,1,0)\tran,~\vec{x_2}=(0,1,1)\tran$ and 
  $\vec{v_3}=(1,0,1)\tran.$ What conclusion can you make about linear
  independence (dependence) of the system of vectors 
  $\vec{v_1},\vec{v_2},\vec{v_3}$.
\end{exercise}
\begin{proof}
  The echelon form of the system is
  \[
    \begin{pmatrix}
      1 &0 &1\\
      1 &1 &0\\
      0 &1 &1
    \end{pmatrix}
    \sim
    \begin{pmatrix}
      1 &0 &1\\
      0 &1 &-1\\
      0 &0 &2
    \end{pmatrix}
  \]
  so, clearly the solution to this equation is
  $x_1=x_2=x_3=0$.

  \textbf{Conclusion.} If the vectors $\vec{v_1},\vec{v_2},\vec{v_3}$
  are linearly independence, then the above equation has unique 
  solution, namely $x_1=x_2=x_3=0$.

  If the vectors $\vec{v_1},\vec{v_2},\vec{v_3}$ are linearly 
  dependence, then there exists $\alpha, \beta,\gamma$ 
  (some of them are non-zero) such that
  \[\alpha\vec{v_1}+\beta\vec{v_2}+\gamma\vec{v_3}=\vec{0}\]
  Therefore, the solution to the above equation is
  \[
    \begin{cases}
      x_1=\alpha t\\
      x_2=\beta t\\
      x_3=\gamma t
    \end{cases}
  \]
  for some $t\in\R$.
\end{proof}
% <==

% <==
% ==> Analyzing pivots
\section{Analyzing pivots}


\setcounter{exercise}{5}
% ==> ex6
\begin{exercise}
  Prove or disprove. If the columns of a square ($n\by n$)
  matrix are linearly independence, so are the columns of $A^2$.
\end{exercise}
\begin{proof}
  Since $A$ is a square matrix, and columns vectors are linearly 
  independence, so
  \begin{align*}
    &\text{has pivot in every column}\\\implies\quad
    &\text{has pivot in evry row}\\\implies\quad
    &A\text{ is invertable}\\\implies\quad
    &A^2\text{ also invertable}\\\implies\quad
    &A^2\text{ has pivot every column and row}
  \end{align*}
  Therefore, the column vectors of $A^2$ also independence.
\end{proof}
% <==
% ==> ex7
\begin{exercise}
  Prove or disprove. If the columns of a square ($n\by n$)
  matrix are linearly independence, so are the columns of $A^3$.
\end{exercise}
% <==
% ==> ex8
\begin{exercise}
  Show that if the equation $A\vec{x}=\vec{0}$ has unique solution,
  then $A$ is left invertable.
\end{exercise}
\begin{proof}
  Because $A$ has unique solution, then it has pivot in every
  column. Therfore, \#col $\leq$ \#row, so we let $m\by n$ be the size
  of $A$ where $n\leq m$
  Let $R$ be the reduced 
  echelon form of $A$, hence there exists $E=E_k\cdots E_2E_1$
  such that $R=EA$.
  Observe that, $R$ would look like
  \[
    R=
    \begin{pmatrix}
      \fbox{1} &0  &\cdots &0\\
      0 &\fbox{1}  &\cdots &0\\
      \vdots&\vdots&\ddots & \\
      0 &0  &\cdots &\fbox{1}\\
      0 &0  &\cdots &0\\
      0 &0  &\cdots &0\\
    \end{pmatrix}
  \]
  Using matrix muliplication gives us $R\tran R=I_n$. We obtain that
  \[
    R\tran EA=R\tran T=I_n
  \]
  Therfore, 
  \fbox{$A$ is left invertable, and $R\tran E$ is its left inverse}.



\end{proof}
% <==
% <==
% ==> find A-inverse
\section{Find $A\inv$ by row reduction}
% <==
% ==> dimension
\section{Dimension}

% ==> ex1
\begin{exercise}
  True or false.
  \begin{enumerate}
    \item Every vector space that is generated by a finite set has a basis;
    \item Every vector space has a (finite) basis;
    \item A vector space cannot have more that one basis;
    \item A vector space has a finite basis, then the number of vectors
      in every basis is the same;
    \item The dimension of $\P_n$ is $n$;
    \item The dimension of $M_{m\by n}$ is $m+n$;
    \item If vectors $\vec{v_1},\vec{v_2},\dots,\vec{v_n}$ generate (span)
      the vector space $V$, then every vector in $V$ can be written as
      a linear combination of vectors $\vec{v_1},\vec{v_2},\dots,\vec{v_n}$
      in only one way;
    \item Every subspace of a finite-dimensional space is finite-dimensional.
    \item If $V$ is a vector space having dimension $n$, then $V$ has exactly
      one subspace of dimension $0$, and exactly one subspace of dimension
      $n$.
  \end{enumerate}
\end{exercise}
\begin{proof}
  \begin{enumerate}
    \item \textbf{True.} That finite set which generated a vector space
      is the spanning set itself. Since it's finite, it contains a
      basis.
    \item \textbf{False.} Take $\R[x]$ for example.
    \item \textbf{False.} In $\R^2$, one can choose 
      \[
        \left\{\binom{1}{0}, \binom{0}{1}\right\}
        \quad\text{or}\quad
        \left\{\binom{1}{1}, \binom{0}{1}\right\}
      \]
      as a basis.
    \item \textbf{True.} As proved in the above theorems,
      \begin{align*}
        &\text{\# independence vectors}\leq \dim V\\
        &\text{\# generating vectors}\geq \dim V
      \end{align*}
      hence the number of any basis in $V$ must be exactly $\dim V$ vectors.
    \item \textbf{False.} In $\P_n$, the standard basis is
      \[
        1,~t,~t^2,\dots,t^n
      \]
      which has $n+1$ vectors. Hence $\boxed{\dim\P_n=n+1}$.
    \item \textbf{False.} The standard basis in $M_{m\by n}$ is
      \[
        \{\vec{e}_{11},\vec{e}_{12},\dots,\vec{e}_{mn}\}
      \]
      has $m\times n$ vectors. Hence $\boxed{\dim M_{m\by n}=mn}$.
    \item \textbf{False.} span doesn't guarantee uniqueness.
    \item \textbf{True.} Let $W$ be a subspace of $V$. Because $\dim V$
      finite, we can find 
      $$\mathcal{A}=\{\vec{v_1},\vec{v_2},\dots,\vec{v_n}\}\subset V$$
      that spans $V$. WLOG, we assume that none of vectors in $\mathcal{A}$
      belongs to $W$ (the unluckiest case.) We can choose 
      $\vec{w_1}\in W$ such that $\vec{w_1}\neq\vec{0}$. Then
      \[
        \vec{w_1}=\alpha_1\vec{v_1}+\cdots+\alpha_n\vec{v_n}
      \]
      Because $\vec{w_1}\neq 0$, we're sure some of the $\alpha_i$'s are
      non-zero, say $\alpha_1$. Then the new system
      \[
        \mathcal{A}_1=\{\vec{w_1},\vec{v_2},\dots,\vec{v_n}\}
      \]
      still spans the space $V$. Now, if $\mathcal{B}_1:=\{\vec{w_1}\}$
      doesn't span $W$, we can repeat the above procedure and find 
      $\vec{w_2}$. We can do this at most $n$ times, because once
      we reach the $n$th step, we have the new system 
      $\mathcal{A}_n\subset W$ that spans the whole space $V$.

      Therfore, after some finite $k\le n$ step, we have
      \[
        \mathcal{B}_k=\{\vec{w_1},\vec{w_2},\dots,\vec{w_k}\}\subset W
      \]
      spans $W$. Hence, $W$ is finite dimensional.
    \item \textbf{Not sure.}
  \end{enumerate}
\end{proof}
% <==
% ==> ex2
\begin{exercise}
  Prove that if $V$ is a vector space having dimension $n$, then
  a system of vectors $\vec{v_1},\vec{v_2},\dots,\vec{v_n}$ in $V$
  is linearly independent iff it spans $V$.
\end{exercise}
\begin{proof}
  \text{}
  \begin{itemize}
    \item[$(\Longrightarrow)$] Suppose that $v_1,\dots,v_n$ linearly
      independent in $V$. If it's not span, then we can complete
      this system to a basis, but then this new system will have 
      the number of vectors more than $n$, but this is a contradiction
      because a basis (linearly independent) cannot have vectors more 
      than $n$.
    \item[$(\Longleftarrow)$] Now suppose that $v_1,\dots,v_n$ spans
      $V$. If it's not linearly independence, we can throw away some
      vectors, and become the basis. However, this new basis system
      has less that $n$ vectors in it, which is a contradiction because
      a basis (span) must have more than $n$ vectors in it.
  \end{itemize}
\end{proof}
% <==
% ==> ex3
\begin{exercise}
  Prove that a linearly independent system of vectors
  $\vec{v_1},\vec{v_2},\dots,\vec{v_n}$ in a vector
  space $V$ is a basis iff $n=\dim V$.
\end{exercise}
\begin{proof}
  We have $\vec{v_1},\dots,\vec{v_n}$ linearly independent.
  If it's a basis then $\dim V=n$. Now suppose that $\dim V=n$.
  We need to show that the system is a basis. From the above
  exercise, this system has to span $V$. Thus a basis in $V$.
\end{proof}
% <==
% ==> ex4
\begin{exercise}
  (An old problem revisited.) Is it possible that 
  vectors $\vec{v_1}, \vec{v_2}, \vec{v_3}$ are linearly
  dependent, but the vectors 
  $\vec{w_1}=\vec{v_1}+\vec{v_2},~\vec{w_2}=\vec{v_2}+\vec{v_3}$
  and $\vec{w_3}=\vec{v_3}+\vec{v_1}$ are linearly independent.
\end{exercise}
\begin{proof}
  Because $\vec{w_1},\vec{w_2},\vec{w_3}$ linearly independent, then
  \[\dim\spans\{\vec{w_1},\vec{w_2},\vec{w_3}\}=3.\]
  Because $\vec{w_1}=\vec{v_1}+\vec{v_2},~\vec{w_2}=\vec{v_2}+\vec{v_3}$,
  it's easy to see that
  \[
    \spans\{\vec{w_1},\vec{w_2},\vec{w_3}\}\subseteq
    \spans\{\vec{v_1},\vec{v_2},\vec{v_3}\}
  \]
  %
  But since the vectors $\vec{v_1},\vec{v_2},\vec{v_3}$ linearly dependent, 
  then one vector, say $\vec{v_3}$, is the linear combination of 
  $\vec{v_1},\vec{v_2}$. This suggests that 
  $\dim\spans\{\vec{v_1},\vec{v_2},\vec{v_3}\}=
  \dim\spans\{\vec{v_1},\vec{v_2}\}\leq 2$, 
  thus
  \[
    \dim\spans\{\vec{w_1},\vec{w_2},\vec{w_3}\}\leq
    \dim\spans\{\vec{v_1},\vec{v_2},\vec{v_3}\}
    \leq 2
  \]
  But this contradicts to the result we found above. Hence it's 
  impossible that such $\vec{v_1},\vec{v_2},\vec{v_3}$ exists.
\end{proof}
% <==
% ==> ex5
\begin{exercise}
  Let vectors $\vec{u},\vec{v},\vec{w}$ be a basis in $V$. Show that
  $\vec{u}+\vec{v}+\vec{w},~\vec{v}+\vec{w},~\vec{w}$ is also
  a basis in $V$.
\end{exercise}
\begin{proof}
  It can be shown by using the fact from the exercise above
  i.e. show that the new system is either spans or LI 
  (because it already has 3 vectors in it).
  But here I'm gonna use matrix and pivot stuff. 
  Since $V$ has three vectors as a basis, then
  $\dim V=3$. Choose an isomorphism $A:V\to\R^3$ such that
  $A\vec{v}=(1,0,0),~A\vec{u}=(0,1,0)$ and $A\vec{w}=(0,0,1)$. 
  Letting
  \begin{align*}
    &\vec{a_1}:=A(\vec{u}+\vec{v}+\vec{w})=(1,1,1)\\
    &\vec{a_2}:=A(\vec{v}+\vec{w})=(0,1,1)\\
    &\vec{a_3}:=A(\vec{w})=(0,0,1)
  \end{align*}
  Observe that the Echelon form of the matrix formed by 
  $\vec{a_1},\vec{a_2},\vec{a_3}$
  \begin{align*}
    \begin{bmatrix} 1&0&0\\ 1&1&0\\ 1&1&1 \end{bmatrix}\sim
    \begin{bmatrix} 1&0&0\\ 0&1&0\\ 0&1&1 \end{bmatrix}\sim
    \begin{bmatrix} 1&0&0\\ 0&1&0\\ 0&0&1 \end{bmatrix}
  \end{align*}
  has pivots in every row and every column, hence the system
  $\{\vec{a_1},\vec{a_2},\vec{a_3}\}$ is basis in $\R^3$. Using
  isomorphism theorem, we conclude that
  \[\vec{u}+\vec{v}+\vec{w},~\vec{v}+\vec{w},~\vec{w}\]
  also a basis in $V$.
\end{proof}
% <==



% <==
% ==> rank
\newpage
\section{Rank}

% ==> ex4
\setcounter{exercise}{3}
\begin{exercise}
  Prove that if $A:X\to Y$ and $V$ is a subspace of $X$, then
  $\dim AV\leq\rank A$. Then deduce that $\rank AB\leq\rank A$.
\end{exercise}
\begin{proof}
  Observe that $AV\subseteq AX=\col A$, hence
  \[\dim AV\leq \dim\col A=\rank A.\]
  To prove the next statement, we may assume that $B:Z\to X$.
  This implies that $\col B=BZ\subseteq X$.
  Using the previous result we obtain that
  \begin{align*}
    &\dim A(\col B)\leq\rank A\\\implies\quad
    &\dim\col AB\leq\rank A\\\implies\quad
    &\rank AB\leq\rank A
  \end{align*}
\end{proof}
% <==

% <==
% ==> change of basis
\newpage
\section{Change of basis}
\setcounter{exercise}{2}

% ==> ex3
\begin{exercise}
  Find the change of coordinates matrix that changes the 
  coordinates in basis $\{1,1+t\}$ in $\P_1$ to the 
  coordinates in the basis $\{1-t, 2t\}$.
\end{exercise}
\begin{proof}
  Let's denote $\mathcal{A}=\{1,1+t\}$ and 
  $\mathcal{B}=\{1-t, 2t\}$. Let $\mathcal{S}$
  be the standard basis in $\P_1$. Therefore, 
  the matrix that transforms from vector in basis
  $\mathcal{A}$ to basis $\mathcal{B}$ is 
  $[\mathcal{BA}]= [\mathcal{BS}][\mathcal{SA}]$.
  We have
  \[ [\mathcal{SA}] = \begin{pmatrix} 1 &1 \\ 0 &1 \end{pmatrix} \]
  and 
  \[
    [\mathcal{BS}]=[\mathcal{SB}]\inv=
    \begin{pmatrix} 1  & 0\\ -1 & 2 \end{pmatrix}\inv=\frac{1}{2}
    \begin{pmatrix} 2 &0\\ 1 &1 \end{pmatrix}
  \]
  Therefore, 
  \[
    [\mathcal{BA}]= \frac{1}{2}
    \begin{pmatrix} 2 &0\\ 1 &1 \end{pmatrix}
    \begin{pmatrix} 1 &1 \\ 0 &1 \end{pmatrix}.
  \]
\end{proof}
% <==
% ==> ex4
\begin{exercise}
  Let $T$ be the ...
\end{exercise}
\begin{proof}
  In standard basis, $T$ looks like
  \[ [T]= \begin{pmatrix} 3 &1\\ 1 &-2 \end{pmatrix}.\]
  Let $\mathcal{B}=\{(1,1)\tran, (1,2)\tran\}$. In basis $\mathcal{B}$,
  the transformation would look like
  \[
    [T]_{\mathcal{BB}}= [\mathcal{BS}] [T] [\mathcal{SB}]
  \]
  And we have
  \[ [\mathcal{SB}]= \begin{pmatrix} 1 &1\\ 1 &2 \end{pmatrix} \]
  so
  \[
    [\mathcal{BS}]= [\mathcal{SB}]\inv=
    \begin{pmatrix} 1 &1\\ 1 &2 \end{pmatrix}\inv=
    \begin{pmatrix} 2 &-1\\ -1&1 \end{pmatrix}
  \]
  Therefore, the transformation $[T]_{\mathcal{BB}}$ in basis
  $\mathcal{B}$ is
  \[
    [T]_{\mathcal{BB}}=
    \begin{pmatrix} 2 &-1\\ -1&1 \end{pmatrix}
    \begin{pmatrix} 3 &1\\ 1 &-2 \end{pmatrix}
    \begin{pmatrix} 1 &1\\ 1 &2 \end{pmatrix} 
  \]
\end{proof}
% <==
% ==> ex5
\begin{exercise}
  Prove that if $A$ and $B$ are similar matrices, then
  $\tr A=\tr B$.
\end{exercise}
\begin{proof}
  Because $A$ and $B$ are similar, then there exists 
  an invertable matrix $Q$ such that $A=Q\inv BQ$. Observe that
  \[
    A=Q\inv\cdot BQ
    \quad\text{and}\quad
    B=BQ\cdot Q\inv
  \]
  This implies that $\tr A=\tr B$.
\end{proof}
% <==

% <==






\end{document}
